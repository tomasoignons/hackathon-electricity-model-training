{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5f1333f-fec7-4052-a75a-f0fa7c12b2fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU pip sagemaker boto3 jsonlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffec39f0-fe87-4bb2-8196-c57498f2459d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/pydantic/_internal/_fields.py:192: UserWarning: Field name \"json\" in \"MonitoringDatasetFormat\" shadows an attribute in parent \"Base\"\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[03/15/25 16:03:55] </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Found credentials from IAM Role:                                   <a href=\"file:///home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/botocore/credentials.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">credentials.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/botocore/credentials.py#1132\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1132</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         BaseNotebookInstanceEc2InstanceRole                                <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                   </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[03/15/25 16:03:55]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Found credentials from IAM Role:                                   \u001b]8;id=244478;file:///home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/botocore/credentials.py\u001b\\\u001b[2mcredentials.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=24468;file:///home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/botocore/credentials.py#1132\u001b\\\u001b[2m1132\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         BaseNotebookInstanceEc2InstanceRole                                \u001b[2m                   \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "import zipfile\n",
    "from dateutil.parser import parse\n",
    "import json\n",
    "from random import shuffle\n",
    "import random\n",
    "import datetime\n",
    "import os\n",
    "import jsonlines\n",
    "import boto3\n",
    "import s3fs\n",
    "import sagemaker\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import timedelta\n",
    "from time import gmtime, strftime, sleep\n",
    "import tqdm\n",
    "from sagemaker.s3 import S3Downloader, S3Uploader\n",
    "\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import IntSlider, FloatSlider, Checkbox, Select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d40a259b-209b-4109-80d6-0824d8f638f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e0a5e68-6cc2-4c65-9789-40e882a9a09f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[03/15/25 16:03:59] </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Found credentials from IAM Role:                                   <a href=\"file:///home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/botocore/credentials.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">credentials.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/botocore/credentials.py#1132\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1132</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         BaseNotebookInstanceEc2InstanceRole                                <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                   </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[03/15/25 16:03:59]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Found credentials from IAM Role:                                   \u001b]8;id=256787;file:///home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/botocore/credentials.py\u001b\\\u001b[2mcredentials.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=234053;file:///home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/botocore/credentials.py#1132\u001b\\\u001b[2m1132\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         BaseNotebookInstanceEc2InstanceRole                                \u001b[2m                   \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sagemaker_session = sagemaker.Session()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b8860836-a445-4c51-900d-131606b99f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_bucket = sagemaker_session.default_bucket()  # replace with an existing bucket if needed\n",
    "s3_prefix = \"train-test-hack\"  # prefix used for all data stored within the bucket\n",
    "experiment_prefix = \"deepar\"\n",
    "\n",
    "sm_role = sagemaker.get_execution_role()  # IAM role to use by SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "19bab01e-5885-416d-bc91-f8124162215f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-west-2-827154710549/train-test-hack/data\n",
      "s3://sagemaker-us-west-2-827154710549/train-test-hack/output\n"
     ]
    }
   ],
   "source": [
    "region = sagemaker_session.boto_region_name\n",
    "\n",
    "s3_data_path = f\"s3://{s3_bucket}/{s3_prefix}/data\"\n",
    "s3_output_path = f\"s3://{s3_bucket}/{s3_prefix}/output\"\n",
    "\n",
    "print(s3_data_path)\n",
    "print(s3_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11f5a3fd-4e09-4869-82f9-b7bf2623064d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[03/15/25 16:04:01] </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Same images used for training and inference. Defaulting to image     <a href=\"file:///home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/image_uris.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">image_uris.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/image_uris.py#393\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">393</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         scope: inference.                                                    <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                 </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[03/15/25 16:04:01]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Same images used for training and inference. Defaulting to image     \u001b]8;id=229258;file:///home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/image_uris.py\u001b\\\u001b[2mimage_uris.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=243962;file:///home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/image_uris.py#393\u001b\\\u001b[2m393\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         scope: inference.                                                    \u001b[2m                 \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Ignoring unnecessary instance type: <span style=\"color: #e100e1; text-decoration-color: #e100e1; font-style: italic\">None</span>.                            <a href=\"file:///home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/image_uris.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">image_uris.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/image_uris.py#530\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">530</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Ignoring unnecessary instance type: \u001b[3;38;2;225;0;225mNone\u001b[0m.                            \u001b]8;id=750800;file:///home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/image_uris.py\u001b\\\u001b[2mimage_uris.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=681453;file:///home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/image_uris.py#530\u001b\\\u001b[2m530\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_name = sagemaker.image_uris.retrieve(\"forecasting-deepar\", region)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2318917a-c575-41dc-80b7-786a5e78d534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# maps of quota codes to instance types\n",
    "ep_instance_quota_code_map = {\n",
    "    \"ml.m5.4xlarge\":\"L-E2649D46\",\n",
    "    \"ml.m5.xlarge\":\"L-2F737F8D\",\n",
    "}\n",
    "\n",
    "training_instance_quota_code_map = {\n",
    "    \"ml.c5.4xlarge\":\"L-E7898792\",\n",
    "    \"ml.p2.xlarge\":\"L-5585E645\",\n",
    "    \"ml.m5.4xlarge\":\"L-AFB011B4\",\n",
    "    \"ml.c5.2xlarge\":\"L-49679826\",\n",
    "    \"ml.m5.2xlarge\":\"L-AD0A282D\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2c04b20-094c-4378-ac56-9a120d063872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper to check resource quota in the account\n",
    "def check_quota(q_map, instance, min_n=1):\n",
    "    quotas_client = boto3.client(\"service-quotas\")\n",
    "\n",
    "\n",
    "    r = quotas_client.get_service_quota(\n",
    "        ServiceCode=\"sagemaker\",\n",
    "        QuotaCode=q_map[instance],\n",
    "    )\n",
    "\n",
    "    q = r[\"Quota\"][\"Value\"]\n",
    "    n = r[\"Quota\"][\"QuotaName\"]\n",
    "    min_n = min_n\n",
    "\n",
    "    b = q >= min_n\n",
    "\n",
    "    print(f\"\\033[92mSUCCESS: Quota {q} for {n} >= required {min_n}\\033[0m\" if b else f\"\\033[91mWARNING: Quota {q} for {n} < required {min_n}\\033[0m\")\n",
    "\n",
    "    return b\n",
    "\n",
    "# helper to get the first available instance in the quota map\n",
    "def get_best_instance(q_map):\n",
    "    l = [i for i in\n",
    "            [i if check_quota(q_map, i) else None \n",
    "             for i in q_map.keys()] if i is not None]\n",
    "    return l[0] if len(l) > 0 else ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "80a8b66a-8141-410b-b223-5804c09ee5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"./data\", exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a62016e7-58aa-45c6-82e9-b83e597b1339",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_zip_file_name = 'LD2011_2014.txt.zip'\n",
    "dataset_path = './data/LD2011_2014.txt'\n",
    "\n",
    "s3_dataset_path = f\"s3://sustainability-hackathon-dataset/final_data.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "90e900d5-6ecd-42a6-9d0b-baed94e0ea54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset ./data/LD2011_2014.txt exists, skipping download and unzip!\n"
     ]
    }
   ],
   "source": [
    "if not os.path.isfile(dataset_path):\n",
    "    print(f'Downloading and unzipping the dataset to {dataset_path}')\n",
    "    s3_client = boto3.client(\"s3\")\n",
    "    s3_client.download_file(\n",
    "        f\"sagemaker-example-files-prod-{region}\", s3_dataset_path, f\"./data/{dataset_zip_file_name}\"\n",
    "    )\n",
    "\n",
    "    zip_ref = zipfile.ZipFile(f\"./data/{dataset_zip_file_name}\", \"r\")\n",
    "    zip_ref.extractall(\"./data\")\n",
    "    zip_ref.close()\n",
    "    dataset_path = '.'.join(zip_ref.filename.split('.')[:-1])\n",
    "else:\n",
    "    print(f'The dataset {dataset_path} exists, skipping download and unzip!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cb955fd0-40d3-41df-a6a7-1d2cd0963a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = pd.read_csv(\n",
    "    \"./data/data.csv\",\n",
    "    sep=',',\n",
    "    index_col=0,\n",
    "    decimal='.',\n",
    "    parse_dates=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5879efa8-5e2d-413f-8963-0a68e19fb511",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DE_solar_generation_actual</th>\n",
       "      <th>DE_temperature</th>\n",
       "      <th>DE_radiation_direct_horizontal</th>\n",
       "      <th>DE_radiation_diffuse_horizontal</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>utc_timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-01-01 08:00:00</th>\n",
       "      <td>340.85</td>\n",
       "      <td>-1.046</td>\n",
       "      <td>8.8773</td>\n",
       "      <td>51.9464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-01 09:00:00</th>\n",
       "      <td>1623.00</td>\n",
       "      <td>0.072</td>\n",
       "      <td>34.1583</td>\n",
       "      <td>97.0929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-01 10:00:00</th>\n",
       "      <td>3017.46</td>\n",
       "      <td>0.866</td>\n",
       "      <td>56.5458</td>\n",
       "      <td>120.2907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-01 11:00:00</th>\n",
       "      <td>4123.22</td>\n",
       "      <td>1.493</td>\n",
       "      <td>62.8690</td>\n",
       "      <td>126.2112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-01 12:00:00</th>\n",
       "      <td>3903.66</td>\n",
       "      <td>1.818</td>\n",
       "      <td>54.2607</td>\n",
       "      <td>115.7275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-31 19:00:00</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.767</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-31 20:00:00</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.656</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-31 21:00:00</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.476</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-31 22:00:00</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.226</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-31 23:00:00</th>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>43720 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     DE_solar_generation_actual  DE_temperature  \\\n",
       "utc_timestamp                                                     \n",
       "2015-01-01 08:00:00                      340.85          -1.046   \n",
       "2015-01-01 09:00:00                     1623.00           0.072   \n",
       "2015-01-01 10:00:00                     3017.46           0.866   \n",
       "2015-01-01 11:00:00                     4123.22           1.493   \n",
       "2015-01-01 12:00:00                     3903.66           1.818   \n",
       "...                                         ...             ...   \n",
       "2019-12-31 19:00:00                        0.00           0.767   \n",
       "2019-12-31 20:00:00                        0.00           0.656   \n",
       "2019-12-31 21:00:00                        0.00           0.476   \n",
       "2019-12-31 22:00:00                        0.00           0.226   \n",
       "2019-12-31 23:00:00                        0.00          -0.044   \n",
       "\n",
       "                     DE_radiation_direct_horizontal  \\\n",
       "utc_timestamp                                         \n",
       "2015-01-01 08:00:00                          8.8773   \n",
       "2015-01-01 09:00:00                         34.1583   \n",
       "2015-01-01 10:00:00                         56.5458   \n",
       "2015-01-01 11:00:00                         62.8690   \n",
       "2015-01-01 12:00:00                         54.2607   \n",
       "...                                             ...   \n",
       "2019-12-31 19:00:00                          0.0000   \n",
       "2019-12-31 20:00:00                          0.0000   \n",
       "2019-12-31 21:00:00                          0.0000   \n",
       "2019-12-31 22:00:00                          0.0000   \n",
       "2019-12-31 23:00:00                          0.0000   \n",
       "\n",
       "                     DE_radiation_diffuse_horizontal  \n",
       "utc_timestamp                                         \n",
       "2015-01-01 08:00:00                          51.9464  \n",
       "2015-01-01 09:00:00                          97.0929  \n",
       "2015-01-01 10:00:00                         120.2907  \n",
       "2015-01-01 11:00:00                         126.2112  \n",
       "2015-01-01 12:00:00                         115.7275  \n",
       "...                                              ...  \n",
       "2019-12-31 19:00:00                           0.0000  \n",
       "2019-12-31 20:00:00                           0.0000  \n",
       "2019-12-31 21:00:00                           0.0000  \n",
       "2019-12-31 22:00:00                           0.0000  \n",
       "2019-12-31 23:00:00                           0.0000  \n",
       "\n",
       "[43720 rows x 4 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "36376c57-db48-4ca8-809e-9a642edc3a28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DE_solar_generation_actual</th>\n",
       "      <th>DE_temperature</th>\n",
       "      <th>DE_radiation_direct_horizontal</th>\n",
       "      <th>DE_radiation_diffuse_horizontal</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>utc_timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-01-01 08:00:00</th>\n",
       "      <td>42.60625</td>\n",
       "      <td>-0.130750</td>\n",
       "      <td>1.109662</td>\n",
       "      <td>6.493300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-01 09:00:00</th>\n",
       "      <td>202.87500</td>\n",
       "      <td>0.009000</td>\n",
       "      <td>4.269787</td>\n",
       "      <td>12.136613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-01 10:00:00</th>\n",
       "      <td>377.18250</td>\n",
       "      <td>0.108250</td>\n",
       "      <td>7.068225</td>\n",
       "      <td>15.036338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-01 11:00:00</th>\n",
       "      <td>515.40250</td>\n",
       "      <td>0.186625</td>\n",
       "      <td>7.858625</td>\n",
       "      <td>15.776400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-01 12:00:00</th>\n",
       "      <td>487.95750</td>\n",
       "      <td>0.227250</td>\n",
       "      <td>6.782587</td>\n",
       "      <td>14.465938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-31 19:00:00</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.095875</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-31 20:00:00</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.082000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-31 21:00:00</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.059500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-31 22:00:00</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.028250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-31 23:00:00</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.005500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>43816 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     DE_solar_generation_actual  DE_temperature  \\\n",
       "utc_timestamp                                                     \n",
       "2015-01-01 08:00:00                    42.60625       -0.130750   \n",
       "2015-01-01 09:00:00                   202.87500        0.009000   \n",
       "2015-01-01 10:00:00                   377.18250        0.108250   \n",
       "2015-01-01 11:00:00                   515.40250        0.186625   \n",
       "2015-01-01 12:00:00                   487.95750        0.227250   \n",
       "...                                         ...             ...   \n",
       "2019-12-31 19:00:00                     0.00000        0.095875   \n",
       "2019-12-31 20:00:00                     0.00000        0.082000   \n",
       "2019-12-31 21:00:00                     0.00000        0.059500   \n",
       "2019-12-31 22:00:00                     0.00000        0.028250   \n",
       "2019-12-31 23:00:00                     0.00000       -0.005500   \n",
       "\n",
       "                     DE_radiation_direct_horizontal  \\\n",
       "utc_timestamp                                         \n",
       "2015-01-01 08:00:00                        1.109662   \n",
       "2015-01-01 09:00:00                        4.269787   \n",
       "2015-01-01 10:00:00                        7.068225   \n",
       "2015-01-01 11:00:00                        7.858625   \n",
       "2015-01-01 12:00:00                        6.782587   \n",
       "...                                             ...   \n",
       "2019-12-31 19:00:00                        0.000000   \n",
       "2019-12-31 20:00:00                        0.000000   \n",
       "2019-12-31 21:00:00                        0.000000   \n",
       "2019-12-31 22:00:00                        0.000000   \n",
       "2019-12-31 23:00:00                        0.000000   \n",
       "\n",
       "                     DE_radiation_diffuse_horizontal  \n",
       "utc_timestamp                                         \n",
       "2015-01-01 08:00:00                         6.493300  \n",
       "2015-01-01 09:00:00                        12.136613  \n",
       "2015-01-01 10:00:00                        15.036338  \n",
       "2015-01-01 11:00:00                        15.776400  \n",
       "2015-01-01 12:00:00                        14.465938  \n",
       "...                                              ...  \n",
       "2019-12-31 19:00:00                         0.000000  \n",
       "2019-12-31 20:00:00                         0.000000  \n",
       "2019-12-31 21:00:00                         0.000000  \n",
       "2019-12-31 22:00:00                         0.000000  \n",
       "2019-12-31 23:00:00                         0.000000  \n",
       "\n",
       "[43816 rows x 4 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# resample to 2h intervals\n",
    "freq = \"1h\"\n",
    "div = 8 # 2 hours contain 8x 15 min intervals, you need to  delete the resampled value by 8\n",
    "\n",
    "data_kw = df_raw.resample(freq).sum() / div\n",
    "data_kw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "aab649a2-4250-492c-8bb8-00f36c86bc5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['DE_radiation_diffuse_horizontal',\n",
       "       'DE_radiation_direct_horizontal'], dtype='<U31')"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select two random time series to include in a small dataset\n",
    "sample_size = 2\n",
    "columns_to_keep = np.random.choice(data_kw.columns.to_list(), size=sample_size, replace=False)\n",
    "columns_to_keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "aa9137a3-7b14-4af3-9521-8aab48091d8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DE_radiation_diffuse_horizontal</th>\n",
       "      <th>DE_radiation_direct_horizontal</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>utc_timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-01-01 08:00:00</th>\n",
       "      <td>6.493300</td>\n",
       "      <td>1.109662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-01 09:00:00</th>\n",
       "      <td>12.136613</td>\n",
       "      <td>4.269787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-01 10:00:00</th>\n",
       "      <td>15.036338</td>\n",
       "      <td>7.068225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-01 11:00:00</th>\n",
       "      <td>15.776400</td>\n",
       "      <td>7.858625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-01 12:00:00</th>\n",
       "      <td>14.465938</td>\n",
       "      <td>6.782587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-31 19:00:00</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-31 20:00:00</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-31 21:00:00</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-31 22:00:00</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-31 23:00:00</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>43816 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     DE_radiation_diffuse_horizontal  \\\n",
       "utc_timestamp                                          \n",
       "2015-01-01 08:00:00                         6.493300   \n",
       "2015-01-01 09:00:00                        12.136613   \n",
       "2015-01-01 10:00:00                        15.036338   \n",
       "2015-01-01 11:00:00                        15.776400   \n",
       "2015-01-01 12:00:00                        14.465938   \n",
       "...                                              ...   \n",
       "2019-12-31 19:00:00                         0.000000   \n",
       "2019-12-31 20:00:00                         0.000000   \n",
       "2019-12-31 21:00:00                         0.000000   \n",
       "2019-12-31 22:00:00                         0.000000   \n",
       "2019-12-31 23:00:00                         0.000000   \n",
       "\n",
       "                     DE_radiation_direct_horizontal  \n",
       "utc_timestamp                                        \n",
       "2015-01-01 08:00:00                        1.109662  \n",
       "2015-01-01 09:00:00                        4.269787  \n",
       "2015-01-01 10:00:00                        7.068225  \n",
       "2015-01-01 11:00:00                        7.858625  \n",
       "2015-01-01 12:00:00                        6.782587  \n",
       "...                                             ...  \n",
       "2019-12-31 19:00:00                        0.000000  \n",
       "2019-12-31 20:00:00                        0.000000  \n",
       "2019-12-31 21:00:00                        0.000000  \n",
       "2019-12-31 22:00:00                        0.000000  \n",
       "2019-12-31 23:00:00                        0.000000  \n",
       "\n",
       "[43816 rows x 2 columns]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_kw_small = data_kw[columns_to_keep]\n",
    "data_kw_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "3efceb9a-46d2-4027-9c85-255604806aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_FULL_DATASET = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "9d90458f-59d9-4c20-a621-b792abdd3bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_FULL_DATASET:\n",
    "    ts_df = data_kw\n",
    "else:\n",
    "    ts_df = data_kw_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "4f85c9b6-a04d-4aa7-ad32-8f6c831f3ef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(43816, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 6340.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     DE_solar_generation_actual  DE_temperature  \\\n",
      "utc_timestamp                                                     \n",
      "2015-01-01 08:00:00                    42.60625       -0.130750   \n",
      "2015-01-01 09:00:00                   202.87500        0.009000   \n",
      "2015-01-01 10:00:00                   377.18250        0.108250   \n",
      "2015-01-01 11:00:00                   515.40250        0.186625   \n",
      "2015-01-01 12:00:00                   487.95750        0.227250   \n",
      "\n",
      "                     DE_radiation_direct_horizontal  \\\n",
      "utc_timestamp                                         \n",
      "2015-01-01 08:00:00                        1.109662   \n",
      "2015-01-01 09:00:00                        4.269787   \n",
      "2015-01-01 10:00:00                        7.068225   \n",
      "2015-01-01 11:00:00                        7.858625   \n",
      "2015-01-01 12:00:00                        6.782587   \n",
      "\n",
      "                     DE_radiation_diffuse_horizontal  \n",
      "utc_timestamp                                         \n",
      "2015-01-01 08:00:00                         6.493300  \n",
      "2015-01-01 09:00:00                        12.136613  \n",
      "2015-01-01 10:00:00                        15.036338  \n",
      "2015-01-01 11:00:00                        15.776400  \n",
      "2015-01-01 12:00:00                        14.465938  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# convert the DataFrame to a list of pandas.Series objects\n",
    "timeseries = []\n",
    "print(ts_df.shape)\n",
    "for i in tqdm.trange(ts_df.shape[1]):\n",
    "    timeseries.append(np.trim_zeros(ts_df.iloc[:, i], trim=\"f\"))\n",
    "print(ts_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "899fc08d-d6cb-48f6-b85e-4eb69596b78a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x1000 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# show random timeseries for 28 day period\n",
    "sample_size = min(ts_df.shape[1], 10)\n",
    "\n",
    "n_rows = (sample_size+1)//2\n",
    "n_cols = 2\n",
    "\n",
    "fig, axs = plt.subplots(n_rows, n_cols, figsize=(20, 5*n_rows), sharex=True)\n",
    "axx = axs.ravel()\n",
    "for i, ts in enumerate(random.sample(timeseries, sample_size)):\n",
    "    series = ts.loc[\"2014-01-01\":\"2014-01-28\"]\n",
    "    if len(series): series.plot(ax=axx[i])\n",
    "    axx[i].set_xlabel(\"date\")\n",
    "    axx[i].set_ylabel(f\"kW consumption - {ts.name}\")\n",
    "    axx[i].grid(which=\"minor\", axis=\"x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "ffd1a0c3-50ea-4285-9c30-597e4cc96b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict for 7 days\n",
    "prediction_days = 7\n",
    "intervals_per_day = 12\n",
    "prediction_length = prediction_days * intervals_per_day\n",
    "\n",
    "# use same period as the context length, this is the number of state updates accomplished before making predictions\n",
    "context_length = prediction_days * intervals_per_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "c3890b0e-204c-48c9-995a-35a0cee2890b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the model sees data from 2015-01-01 00:00:00 to 2019-09-01 00:00:00 for training.\n"
     ]
    }
   ],
   "source": [
    "start_dataset = pd.Timestamp(\"2015-01-01 00:00:00\", unit=freq)\n",
    "end_training = pd.Timestamp(\"2019-09-01 00:00:00\", unit=freq)\n",
    "print(f\"the model sees data from {start_dataset} to {end_training} for training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "e5103edf-5ae4-4014-9e7a-0960acd059e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 4 time series\n"
     ]
    }
   ],
   "source": [
    "training_data = [\n",
    "    {\n",
    "        \"start\": str(start_dataset),\n",
    "        \"target\": ts[\n",
    "            start_dataset : end_training - timedelta(days=1)\n",
    "        ].tolist(),  # use -1, because pandas indexing includes the upper bound\n",
    "    }\n",
    "    for ts in timeseries\n",
    "]\n",
    "print(f\"Training for {len(training_data)} time series\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "bbecf80f-6292-4f9c-bb7f-5dbc960ba036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "num_test_windows = 4\n",
    "\n",
    "test_data = [\n",
    "    {\n",
    "        \"start\": str(start_dataset),\n",
    "        \"target\": ts[start_dataset : end_training + timedelta(days=k * prediction_length)].tolist(),\n",
    "    }\n",
    "    for k in range(1, num_test_windows + 1)\n",
    "    for ts in timeseries\n",
    "]\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "379f3c5f-672d-45ee-bb4a-ad8e56597e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_dicts_to_file(path, data):\n",
    "    with open(path, \"wb\") as fp:\n",
    "        for d in data:\n",
    "            fp.write(json.dumps(d).encode(\"utf-8\"))\n",
    "            fp.write(\"\\n\".encode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "0f774362-14a5-498b-a514-2a376dcb8fa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 239 ms, sys: 4.01 ms, total: 243 ms\n",
      "Wall time: 246 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "write_dicts_to_file(\"./data/train.json\", training_data)\n",
    "write_dicts_to_file(\"./data/test.json\", test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "93d10a1b-dd0b-4d3b-ac88-3166294683ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content of the DeepAR training/test dataset file:\n",
      "{\n",
      "  \"start\": \"2015-01-01 00:00:00\",\n",
      "  \"target\": [\n",
      "    42.60625,\n",
      "    202.875,\n",
      "    377.1825,\n",
      "    515.4025,\n",
      "    487.9575,\n",
      "    351.76,\n",
      "    162.4475,\n",
      "    15.01875,\n",
      "    0.0,\n",
      "    0.0,\n",
      "    0.0,\n",
      "    0.0,\n",
      "    0...\n",
      ".94625,\n",
      "    3025.18875,\n",
      "    3153.92125,\n",
      "    3072.26625,\n",
      "    2721.52625,\n",
      "    2219.7325,\n",
      "    1579.825,\n",
      "    865.095,\n",
      "    256.98875,\n",
      "    5.40125,\n",
      "    0.0,\n",
      "    0.0,\n",
      "    0.0,\n",
      "    0.0,\n",
      "    0.0,\n",
      "    0.0\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"Content of the DeepAR training/test dataset file:\")\n",
    "\n",
    "# Read the jsonl file and show the head an the tail of the first object\n",
    "with jsonlines.open(f\"./data/train.json\") as reader:      \n",
    "    sample = json.dumps(reader.read(), indent=2)\n",
    "    print(f\"{sample[:200]}...\\n{sample[len(sample)-200:]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "9d083d46-ab33-4652-a79c-71b035e6a58a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delete: s3://sagemaker-us-west-2-827154710549/train-test-hack/data/train/train.json\n",
      "delete: s3://sagemaker-us-west-2-827154710549/train-test-hack/data/test/test.json\n"
     ]
    }
   ],
   "source": [
    "!aws s3 rm {s3_data_path}/train/ --recursive\n",
    "!aws s3 rm {s3_data_path}/test/ --recursive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "1bdf5ae0-1c91-4f86-a5d4-3096507b6624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: data/train.json to s3://sagemaker-us-west-2-827154710549/train-test-hack/data/train/train.json\n",
      "upload: data/train.json to s3://sagemaker-us-west-2-827154710549/train-test-hack/data/test/test.json\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp ./data/train.json {s3_data_path}/train/train.json\n",
    "!aws s3 cp ./data/train.json {s3_data_path}/test/test.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01cf087-41a1-4211-a437-6a778e4c899e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "2400ec08-0fe5-49b5-8b00-f42a152841e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TRAIN A DeepAR Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "4918debc-dd75-476b-a464-a2d80c416210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the instance based on the quota availability\n",
    "instance_type = \"ml.m4.xlarge\"\n",
    "\n",
    "assert instance_type, \"You don't have quotas for required processing instance\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "57736b28-8b0a-4b02-a928-1961a5d89524",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = sagemaker.estimator.Estimator(\n",
    "    image_uri=image_name,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    role=sm_role,\n",
    "    instance_count=1,\n",
    "    instance_type=instance_type,\n",
    "    base_job_name=\"deepar-demo-notebook\",\n",
    "    output_path=s3_output_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "425068df-344a-4a64-866e-66eb428285c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"time_freq\": freq.upper(),\n",
    "    \"epochs\": \"200\", # you can reduce number of epochs for faster training: 100 epochs takes about 14 min to train\n",
    "    \"early_stopping_patience\": \"40\",\n",
    "    \"mini_batch_size\": \"64\",\n",
    "    \"learning_rate\": \"5E-4\",\n",
    "    \"context_length\": \"24\",\n",
    "    \"prediction_length\": \"6\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "6482dd29-c143-4a36-b184-f08d224d4c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.set_hyperparameters(**hyperparameters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "c122a663-6206-4816-a834-20d9944a5d04",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[03/15/25 17:19:04] </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> SageMaker Python SDK will collect telemetry to help us better  <a href=\"file:///home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/telemetry/telemetry_logging.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">telemetry_logging.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/telemetry/telemetry_logging.py#91\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">91</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         understand our user's needs, diagnose issues, and deliver      <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                       </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         additional features.                                           <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                       </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         To opt out of telemetry, please disable via TelemetryOptOut    <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                       </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         parameter in SDK defaults config. For more information, refer  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                       </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         to                                                             <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                       </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #0069ff; text-decoration-color: #0069ff; text-decoration: underline\">https://sagemaker.readthedocs.io/en/stable/overview.html#confi</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                       </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #0069ff; text-decoration-color: #0069ff; text-decoration: underline\">guring-and-using-defaults-with-the-sagemaker-python-sdk.</span>       <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                       </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[03/15/25 17:19:04]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m SageMaker Python SDK will collect telemetry to help us better  \u001b]8;id=764544;file:///home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/telemetry/telemetry_logging.py\u001b\\\u001b[2mtelemetry_logging.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=256702;file:///home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/telemetry/telemetry_logging.py#91\u001b\\\u001b[2m91\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         understand our user's needs, diagnose issues, and deliver      \u001b[2m                       \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         additional features.                                           \u001b[2m                       \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         To opt out of telemetry, please disable via TelemetryOptOut    \u001b[2m                       \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         parameter in SDK defaults config. For more information, refer  \u001b[2m                       \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         to                                                             \u001b[2m                       \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[4;38;2;0;105;255mhttps://sagemaker.readthedocs.io/en/stable/overview.html#confi\u001b[0m \u001b[2m                       \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[4;38;2;0;105;255mguring-and-using-defaults-with-the-sagemaker-python-sdk.\u001b[0m       \u001b[2m                       \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Creating training-job with name:                                       <a href=\"file:///home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/session.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">session.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/session.py#1042\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1042</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         deepar-demo-notebook-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-03-15-17-19-04-336                           <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Creating training-job with name:                                       \u001b]8;id=970342;file:///home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/session.py\u001b\\\u001b[2msession.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=671088;file:///home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/session.py#1042\u001b\\\u001b[2m1042\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         deepar-demo-notebook-\u001b[1;36m2025\u001b[0m-03-15-17-19-04-336                           \u001b[2m               \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-15 17:19:05 Starting - Starting the training job...\n",
      "..25-03-15 17:19:18 Starting - Preparing the instances for training.\n",
      "...........17:20:06 Downloading - Downloading the training image.\n",
      "...\u001b[34mDocker entrypoint called with argument(s): train\u001b[0meted. Training in progress..\n",
      "\u001b[34mRunning default environment configuration script\u001b[0m\n",
      "\u001b[34mRunning custom environment configuration script\u001b[0m\n",
      "\u001b[34m/opt/amazon/lib/python3.8/site-packages/mxnet/model.py:97: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if num_device is 1 and 'dist' not in kvstore:\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:22:38 INFO 140512987567936] Reading default configuration from /opt/amazon/lib/python3.8/site-packages/algorithm/resources/default-input.json: {'_kvstore': 'auto', '_num_gpus': 'auto', '_num_kv_servers': 'auto', '_tuning_objective_metric': '', 'cardinality': 'auto', 'dropout_rate': '0.10', 'early_stopping_patience': '', 'embedding_dimension': '10', 'learning_rate': '0.001', 'likelihood': 'student-t', 'mini_batch_size': '128', 'num_cells': '40', 'num_dynamic_feat': 'auto', 'num_eval_samples': '100', 'num_layers': '2', 'test_quantiles': '[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]'}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:22:38 INFO 140512987567936] Merging with provided configuration from /opt/ml/input/config/hyperparameters.json: {'context_length': '24', 'early_stopping_patience': '40', 'epochs': '200', 'learning_rate': '5E-4', 'mini_batch_size': '64', 'prediction_length': '6', 'time_freq': '1H'}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:22:38 INFO 140512987567936] Final configuration: {'_kvstore': 'auto', '_num_gpus': 'auto', '_num_kv_servers': 'auto', '_tuning_objective_metric': '', 'cardinality': 'auto', 'dropout_rate': '0.10', 'early_stopping_patience': '40', 'embedding_dimension': '10', 'learning_rate': '5E-4', 'likelihood': 'student-t', 'mini_batch_size': '64', 'num_cells': '40', 'num_dynamic_feat': 'auto', 'num_eval_samples': '100', 'num_layers': '2', 'test_quantiles': '[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]', 'context_length': '24', 'epochs': '200', 'prediction_length': '6', 'time_freq': '1H'}\u001b[0m\n",
      "\u001b[34mProcess 7 is a worker.\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:22:38 INFO 140512987567936] Detected entry point for worker worker\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:22:38 INFO 140512987567936] Using early stopping with patience 40\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:22:38 INFO 140512987567936] random_seed is None\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:22:38 INFO 140512987567936] [cardinality=auto] `cat` field was NOT found in the file `/opt/ml/input/data/train/train.json` and will NOT be used for training.\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:22:38 INFO 140512987567936] [num_dynamic_feat=auto] `dynamic_feat` field was NOT found in the file `/opt/ml/input/data/train/train.json` and will NOT be used for training.\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:22:38 INFO 140512987567936] Training set statistics:\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:22:38 INFO 140512987567936] Real time series\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:22:38 INFO 140512987567936] number of time series: 4\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:22:38 INFO 140512987567936] number of observations: 163460\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:22:38 INFO 140512987567936] mean target length: 40865.0\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:22:38 INFO 140512987567936] min/mean/max target: -1.5857499837875366/142.56887526956137/3777.4013671875\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:22:38 INFO 140512987567936] mean abs(target): 142.58608145723724\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:22:38 INFO 140512987567936] contains missing values: no\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:22:38 INFO 140512987567936] Small number of time series. Doing 160 passes over dataset with prob 1.0 per epoch.\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:22:38 INFO 140512987567936] Test set statistics:\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:22:38 INFO 140512987567936] Real time series\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:22:38 INFO 140512987567936] number of time series: 4\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:22:38 INFO 140512987567936] number of observations: 163460\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:22:38 INFO 140512987567936] mean target length: 40865.0\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:22:38 INFO 140512987567936] min/mean/max target: -1.5857499837875366/142.56887526956137/3777.4013671875\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:22:38 INFO 140512987567936] mean abs(target): 142.58608145723724\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:22:38 INFO 140512987567936] contains missing values: no\u001b[0m\n",
      "\u001b[34m/opt/amazon/lib/python3.8/site-packages/algorithm/core/date_feature_set.py:44: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)\n",
      "  return index.weekofyear / 51.0 - 0.5\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:22:38 INFO 140512987567936] #memory_usage::<batchbuffer> = 576.5283203125 mb\u001b[0m\n",
      "\u001b[34m/opt/amazon/python3.8/lib/python3.8/subprocess.py:848: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used\n",
      "  self.stdout = io.open(c2pread, 'rb', bufsize)\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:22:38 INFO 140512987567936] nvidia-smi: took 0.034 seconds to run.\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:22:38 INFO 140512987567936] nvidia-smi identified 0 GPUs.\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:22:38 INFO 140512987567936] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:22:38 INFO 140512987567936] Create Store: local\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742059358.830994, \"EndTime\": 1742059359.4348676, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"get_graph.time\": {\"sum\": 600.1536846160889, \"count\": 1, \"min\": 600.1536846160889, \"max\": 600.1536846160889}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:22:39 INFO 140512987567936] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:22:39 INFO 140512987567936] #memory_usage::<model> = 57 mb\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742059359.4349463, \"EndTime\": 1742059359.8091743, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"initialize.time\": {\"sum\": 977.2963523864746, \"count\": 1, \"min\": 977.2963523864746, \"max\": 977.2963523864746}}}\u001b[0m\n",
      "\u001b[34m[17:22:44] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.3.x_Cuda_11.1.x.406.0/AL2_x86_64/generic-flavor/src/src/operator/nn/mkldnn/mkldnn_base.cc:74: Allocate 10240 bytes with malloc directly\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:22:44 INFO 140512987567936] Epoch[0] Batch[0] avg_epoch_loss=4.077769\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:22:44 INFO 140512987567936] #quality_metric: host=algo-1, epoch=0, batch=0 train loss <loss>=4.077768802642822\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:22:45 INFO 140512987567936] Epoch[0] Batch[5] avg_epoch_loss=4.049348\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:22:45 INFO 140512987567936] #quality_metric: host=algo-1, epoch=0, batch=5 train loss <loss>=4.049347837766011\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:22:45 INFO 140512987567936] Epoch[0] Batch [5]#011Speed: 297.14 samples/sec#011loss=4.049348\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:22:46 INFO 140512987567936] processed a total of 616 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742059359.8092504, \"EndTime\": 1742059366.961086, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"epochs\": {\"sum\": 200.0, \"count\": 1, \"min\": 200, \"max\": 200}, \"update.time\": {\"sum\": 7151.767253875732, \"count\": 1, \"min\": 7151.767253875732, \"max\": 7151.767253875732}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:22:46 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=86.13131935312317 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:22:46 INFO 140512987567936] #progress_metric: host=algo-1, completed 0.5 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:22:46 INFO 140512987567936] #quality_metric: host=algo-1, epoch=0, train loss <loss>=4.1284548282623295\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:22:46 INFO 140512987567936] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:22:46 INFO 140512987567936] Saved checkpoint to \"/opt/ml/model/state_00c527e6-d37e-4230-8614-389a1a0be143-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742059366.961156, \"EndTime\": 1742059366.9749546, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 13.31949234008789, \"count\": 1, \"min\": 13.31949234008789, \"max\": 13.31949234008789}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:22:51 INFO 140512987567936] Epoch[1] Batch[0] avg_epoch_loss=3.365747\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:22:51 INFO 140512987567936] #quality_metric: host=algo-1, epoch=1, batch=0 train loss <loss>=3.3657472133636475\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:22:52 INFO 140512987567936] Epoch[1] Batch[5] avg_epoch_loss=3.606391\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:22:52 INFO 140512987567936] #quality_metric: host=algo-1, epoch=1, batch=5 train loss <loss>=3.6063907941182456\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:22:52 INFO 140512987567936] Epoch[1] Batch [5]#011Speed: 308.00 samples/sec#011loss=3.606391\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:22:53 INFO 140512987567936] processed a total of 595 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742059366.9750092, \"EndTime\": 1742059373.577789, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6602.725982666016, \"count\": 1, \"min\": 6602.725982666016, \"max\": 6602.725982666016}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:22:53 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=90.11278564718579 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:22:53 INFO 140512987567936] #progress_metric: host=algo-1, completed 1.0 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:22:53 INFO 140512987567936] #quality_metric: host=algo-1, epoch=1, train loss <loss>=3.578761672973633\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:22:53 INFO 140512987567936] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:22:53 INFO 140512987567936] Saved checkpoint to \"/opt/ml/model/state_f39f735e-91a0-4ea0-a0f1-1c60c1e9dc25-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742059373.5778682, \"EndTime\": 1742059373.592931, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 14.359235763549805, \"count\": 1, \"min\": 14.359235763549805, \"max\": 14.359235763549805}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:22:58 INFO 140512987567936] Epoch[2] Batch[0] avg_epoch_loss=3.849241\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:22:58 INFO 140512987567936] #quality_metric: host=algo-1, epoch=2, batch=0 train loss <loss>=3.8492414951324463\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:22:59 INFO 140512987567936] Epoch[2] Batch[5] avg_epoch_loss=3.585778\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:22:59 INFO 140512987567936] #quality_metric: host=algo-1, epoch=2, batch=5 train loss <loss>=3.585777997970581\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:22:59 INFO 140512987567936] Epoch[2] Batch [5]#011Speed: 291.07 samples/sec#011loss=3.585778\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:23:00 INFO 140512987567936] processed a total of 638 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742059373.593004, \"EndTime\": 1742059380.1165233, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6523.459672927856, \"count\": 1, \"min\": 6523.459672927856, \"max\": 6523.459672927856}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:23:00 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=97.79934965528763 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:23:00 INFO 140512987567936] #progress_metric: host=algo-1, completed 1.5 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:23:00 INFO 140512987567936] #quality_metric: host=algo-1, epoch=2, train loss <loss>=3.455914258956909\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:23:00 INFO 140512987567936] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:23:00 INFO 140512987567936] Saved checkpoint to \"/opt/ml/model/state_9c330610-ba71-4a7e-81f4-272dc12ad515-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742059380.116593, \"EndTime\": 1742059380.1314852, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 14.565706253051758, \"count\": 1, \"min\": 14.565706253051758, \"max\": 14.565706253051758}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:23:04 INFO 140512987567936] Epoch[3] Batch[0] avg_epoch_loss=3.528039\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:23:04 INFO 140512987567936] #quality_metric: host=algo-1, epoch=3, batch=0 train loss <loss>=3.52803897857666\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:23:05 INFO 140512987567936] Epoch[3] Batch[5] avg_epoch_loss=3.233716\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:23:05 INFO 140512987567936] #quality_metric: host=algo-1, epoch=3, batch=5 train loss <loss>=3.2337156534194946\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:23:05 INFO 140512987567936] Epoch[3] Batch [5]#011Speed: 321.21 samples/sec#011loss=3.233716\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:23:06 INFO 140512987567936] processed a total of 625 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742059380.1315424, \"EndTime\": 1742059386.5229154, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6391.324520111084, \"count\": 1, \"min\": 6391.324520111084, \"max\": 6391.324520111084}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:23:06 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=97.78730046075417 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:23:06 INFO 140512987567936] #progress_metric: host=algo-1, completed 2.0 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:23:06 INFO 140512987567936] #quality_metric: host=algo-1, epoch=3, train loss <loss>=3.1279724836349487\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:23:06 INFO 140512987567936] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:23:06 INFO 140512987567936] Saved checkpoint to \"/opt/ml/model/state_1a64a3dd-cfdd-431f-af52-9808878f3587-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742059386.5229828, \"EndTime\": 1742059386.5364132, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 13.070344924926758, \"count\": 1, \"min\": 13.070344924926758, \"max\": 13.070344924926758}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:23:10 INFO 140512987567936] Epoch[4] Batch[0] avg_epoch_loss=2.799439\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:23:10 INFO 140512987567936] #quality_metric: host=algo-1, epoch=4, batch=0 train loss <loss>=2.7994391918182373\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:23:11 INFO 140512987567936] Epoch[4] Batch[5] avg_epoch_loss=2.743828\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:23:11 INFO 140512987567936] #quality_metric: host=algo-1, epoch=4, batch=5 train loss <loss>=2.7438284953435264\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:23:11 INFO 140512987567936] Epoch[4] Batch [5]#011Speed: 298.05 samples/sec#011loss=2.743828\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:23:13 INFO 140512987567936] Epoch[4] Batch[10] avg_epoch_loss=2.651459\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:23:13 INFO 140512987567936] #quality_metric: host=algo-1, epoch=4, batch=10 train loss <loss>=2.5406151175498963\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:23:13 INFO 140512987567936] Epoch[4] Batch [10]#011Speed: 233.90 samples/sec#011loss=2.540615\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:23:13 INFO 140512987567936] processed a total of 646 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742059386.5364666, \"EndTime\": 1742059393.2478473, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6711.29035949707, \"count\": 1, \"min\": 6711.29035949707, \"max\": 6711.29035949707}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:23:13 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=96.25318713947733 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:23:13 INFO 140512987567936] #progress_metric: host=algo-1, completed 2.5 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:23:13 INFO 140512987567936] #quality_metric: host=algo-1, epoch=4, train loss <loss>=2.6514587781646033\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:23:13 INFO 140512987567936] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:23:13 INFO 140512987567936] Saved checkpoint to \"/opt/ml/model/state_81bc30a6-7c1c-423c-8ed3-43880d0edb41-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742059393.2479522, \"EndTime\": 1742059393.260789, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 12.311458587646484, \"count\": 1, \"min\": 12.311458587646484, \"max\": 12.311458587646484}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:23:17 INFO 140512987567936] Epoch[5] Batch[0] avg_epoch_loss=3.148053\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:23:17 INFO 140512987567936] #quality_metric: host=algo-1, epoch=5, batch=0 train loss <loss>=3.148052930831909\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:23:18 INFO 140512987567936] Epoch[5] Batch[5] avg_epoch_loss=2.770586\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:23:18 INFO 140512987567936] #quality_metric: host=algo-1, epoch=5, batch=5 train loss <loss>=2.7705860137939453\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:23:18 INFO 140512987567936] Epoch[5] Batch [5]#011Speed: 309.80 samples/sec#011loss=2.770586\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:23:19 INFO 140512987567936] processed a total of 600 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742059393.2608473, \"EndTime\": 1742059399.5794892, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6318.58491897583, \"count\": 1, \"min\": 6318.58491897583, \"max\": 6318.58491897583}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:23:19 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=94.9562971961963 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:23:19 INFO 140512987567936] #progress_metric: host=algo-1, completed 3.0 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:23:19 INFO 140512987567936] #quality_metric: host=algo-1, epoch=5, train loss <loss>=2.949498677253723\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:23:19 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:23:23 INFO 140512987567936] Epoch[6] Batch[0] avg_epoch_loss=2.653179\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:23:23 INFO 140512987567936] #quality_metric: host=algo-1, epoch=6, batch=0 train loss <loss>=2.653179168701172\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:23:24 INFO 140512987567936] Epoch[6] Batch[5] avg_epoch_loss=2.741984\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:23:24 INFO 140512987567936] #quality_metric: host=algo-1, epoch=6, batch=5 train loss <loss>=2.741983691851298\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:23:24 INFO 140512987567936] Epoch[6] Batch [5]#011Speed: 310.50 samples/sec#011loss=2.741984\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:23:26 INFO 140512987567936] Epoch[6] Batch[10] avg_epoch_loss=2.423258\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:23:26 INFO 140512987567936] #quality_metric: host=algo-1, epoch=6, batch=10 train loss <loss>=2.040786325931549\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:23:26 INFO 140512987567936] Epoch[6] Batch [10]#011Speed: 234.59 samples/sec#011loss=2.040786\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:23:26 INFO 140512987567936] processed a total of 646 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742059399.5795655, \"EndTime\": 1742059406.2633548, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6683.413028717041, \"count\": 1, \"min\": 6683.413028717041, \"max\": 6683.413028717041}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:23:26 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=96.65513617676966 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:23:26 INFO 140512987567936] #progress_metric: host=algo-1, completed 3.5 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:23:26 INFO 140512987567936] #quality_metric: host=algo-1, epoch=6, train loss <loss>=2.4232576164332302\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:23:26 INFO 140512987567936] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:23:26 INFO 140512987567936] Saved checkpoint to \"/opt/ml/model/state_ed9f547d-0daf-490e-81af-1966ea9dda62-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742059406.2634442, \"EndTime\": 1742059406.2777185, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 13.835668563842773, \"count\": 1, \"min\": 13.835668563842773, \"max\": 13.835668563842773}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:23:30 INFO 140512987567936] Epoch[7] Batch[0] avg_epoch_loss=2.517694\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:23:30 INFO 140512987567936] #quality_metric: host=algo-1, epoch=7, batch=0 train loss <loss>=2.5176944732666016\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:23:31 INFO 140512987567936] Epoch[7] Batch[5] avg_epoch_loss=2.685582\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:23:31 INFO 140512987567936] #quality_metric: host=algo-1, epoch=7, batch=5 train loss <loss>=2.685581684112549\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:23:31 INFO 140512987567936] Epoch[7] Batch [5]#011Speed: 306.10 samples/sec#011loss=2.685582\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:23:33 INFO 140512987567936] Epoch[7] Batch[10] avg_epoch_loss=2.757432\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:23:33 INFO 140512987567936] #quality_metric: host=algo-1, epoch=7, batch=10 train loss <loss>=2.8436515808105467\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:23:33 INFO 140512987567936] Epoch[7] Batch [10]#011Speed: 217.18 samples/sec#011loss=2.843652\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:23:33 INFO 140512987567936] processed a total of 668 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742059406.2777777, \"EndTime\": 1742059413.026954, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6749.119997024536, \"count\": 1, \"min\": 6749.119997024536, \"max\": 6749.119997024536}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:23:33 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=98.9744085457971 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:23:33 INFO 140512987567936] #progress_metric: host=algo-1, completed 4.0 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:23:33 INFO 140512987567936] #quality_metric: host=algo-1, epoch=7, train loss <loss>=2.7574316371570933\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:23:33 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:23:37 INFO 140512987567936] Epoch[8] Batch[0] avg_epoch_loss=2.807564\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:23:37 INFO 140512987567936] #quality_metric: host=algo-1, epoch=8, batch=0 train loss <loss>=2.8075637817382812\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:23:38 INFO 140512987567936] Epoch[8] Batch[5] avg_epoch_loss=2.414290\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:23:38 INFO 140512987567936] #quality_metric: host=algo-1, epoch=8, batch=5 train loss <loss>=2.414290448029836\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:23:38 INFO 140512987567936] Epoch[8] Batch [5]#011Speed: 299.88 samples/sec#011loss=2.414290\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:23:40 INFO 140512987567936] Epoch[8] Batch[10] avg_epoch_loss=2.411225\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:23:40 INFO 140512987567936] #quality_metric: host=algo-1, epoch=8, batch=10 train loss <loss>=2.407545733451843\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:23:40 INFO 140512987567936] Epoch[8] Batch [10]#011Speed: 215.23 samples/sec#011loss=2.407546\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:23:40 INFO 140512987567936] processed a total of 680 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742059413.0270193, \"EndTime\": 1742059420.1888769, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 7161.494970321655, \"count\": 1, \"min\": 7161.494970321655, \"max\": 7161.494970321655}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:23:40 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=94.95100236604588 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:23:40 INFO 140512987567936] #progress_metric: host=algo-1, completed 4.5 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:23:40 INFO 140512987567936] #quality_metric: host=algo-1, epoch=8, train loss <loss>=2.411224668676203\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:23:40 INFO 140512987567936] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:23:40 INFO 140512987567936] Saved checkpoint to \"/opt/ml/model/state_d27bf41c-3b61-419e-9201-d0a8d2844857-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742059420.1889389, \"EndTime\": 1742059420.2026055, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 13.22627067565918, \"count\": 1, \"min\": 13.22627067565918, \"max\": 13.22627067565918}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:23:44 INFO 140512987567936] Epoch[9] Batch[0] avg_epoch_loss=2.580682\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:23:44 INFO 140512987567936] #quality_metric: host=algo-1, epoch=9, batch=0 train loss <loss>=2.580681562423706\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:23:45 INFO 140512987567936] Epoch[9] Batch[5] avg_epoch_loss=2.471107\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:23:45 INFO 140512987567936] #quality_metric: host=algo-1, epoch=9, batch=5 train loss <loss>=2.4711073637008667\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:23:45 INFO 140512987567936] Epoch[9] Batch [5]#011Speed: 308.16 samples/sec#011loss=2.471107\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:23:46 INFO 140512987567936] processed a total of 635 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742059420.2026615, \"EndTime\": 1742059426.6357145, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6432.999849319458, \"count\": 1, \"min\": 6432.999849319458, \"max\": 6432.999849319458}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:23:46 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=98.70818134224858 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:23:46 INFO 140512987567936] #progress_metric: host=algo-1, completed 5.0 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:23:46 INFO 140512987567936] #quality_metric: host=algo-1, epoch=9, train loss <loss>=2.439538836479187\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:23:46 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:23:50 INFO 140512987567936] Epoch[10] Batch[0] avg_epoch_loss=2.875153\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:23:50 INFO 140512987567936] #quality_metric: host=algo-1, epoch=10, batch=0 train loss <loss>=2.8751533031463623\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:23:51 INFO 140512987567936] Epoch[10] Batch[5] avg_epoch_loss=2.626883\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:23:51 INFO 140512987567936] #quality_metric: host=algo-1, epoch=10, batch=5 train loss <loss>=2.6268831491470337\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:23:51 INFO 140512987567936] Epoch[10] Batch [5]#011Speed: 286.64 samples/sec#011loss=2.626883\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:23:53 INFO 140512987567936] Epoch[10] Batch[10] avg_epoch_loss=2.833154\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:23:53 INFO 140512987567936] #quality_metric: host=algo-1, epoch=10, batch=10 train loss <loss>=3.0806795597076415\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:23:53 INFO 140512987567936] Epoch[10] Batch [10]#011Speed: 218.81 samples/sec#011loss=3.080680\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:23:53 INFO 140512987567936] processed a total of 658 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742059426.6357825, \"EndTime\": 1742059433.4279997, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6791.78261756897, \"count\": 1, \"min\": 6791.78261756897, \"max\": 6791.78261756897}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:23:53 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=96.88031918581346 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:23:53 INFO 140512987567936] #progress_metric: host=algo-1, completed 5.5 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:23:53 INFO 140512987567936] #quality_metric: host=algo-1, epoch=10, train loss <loss>=2.833154244856401\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:23:53 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:23:57 INFO 140512987567936] Epoch[11] Batch[0] avg_epoch_loss=2.478245\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:23:57 INFO 140512987567936] #quality_metric: host=algo-1, epoch=11, batch=0 train loss <loss>=2.4782445430755615\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:23:58 INFO 140512987567936] Epoch[11] Batch[5] avg_epoch_loss=2.184677\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:23:58 INFO 140512987567936] #quality_metric: host=algo-1, epoch=11, batch=5 train loss <loss>=2.1846765279769897\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:23:58 INFO 140512987567936] Epoch[11] Batch [5]#011Speed: 305.24 samples/sec#011loss=2.184677\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:24:00 INFO 140512987567936] Epoch[11] Batch[10] avg_epoch_loss=2.399033\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:24:00 INFO 140512987567936] #quality_metric: host=algo-1, epoch=11, batch=10 train loss <loss>=2.6562612056732178\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:24:00 INFO 140512987567936] Epoch[11] Batch [10]#011Speed: 208.67 samples/sec#011loss=2.656261\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:24:00 INFO 140512987567936] processed a total of 672 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742059433.4280705, \"EndTime\": 1742059440.2395513, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6811.203241348267, \"count\": 1, \"min\": 6811.203241348267, \"max\": 6811.203241348267}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:24:00 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=98.65963854219763 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:24:00 INFO 140512987567936] #progress_metric: host=algo-1, completed 6.0 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:24:00 INFO 140512987567936] #quality_metric: host=algo-1, epoch=11, train loss <loss>=2.3990331996570933\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:24:00 INFO 140512987567936] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:24:00 INFO 140512987567936] Saved checkpoint to \"/opt/ml/model/state_5b3e2dfc-840a-4554-bda6-33f1f4450ded-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742059440.2396145, \"EndTime\": 1742059440.2553508, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 15.42043685913086, \"count\": 1, \"min\": 15.42043685913086, \"max\": 15.42043685913086}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:24:04 INFO 140512987567936] Epoch[12] Batch[0] avg_epoch_loss=2.513985\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:24:04 INFO 140512987567936] #quality_metric: host=algo-1, epoch=12, batch=0 train loss <loss>=2.5139853954315186\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:24:05 INFO 140512987567936] Epoch[12] Batch[5] avg_epoch_loss=2.386559\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:24:05 INFO 140512987567936] #quality_metric: host=algo-1, epoch=12, batch=5 train loss <loss>=2.386559247970581\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:24:05 INFO 140512987567936] Epoch[12] Batch [5]#011Speed: 291.48 samples/sec#011loss=2.386559\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:24:06 INFO 140512987567936] processed a total of 635 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742059440.2554085, \"EndTime\": 1742059446.8138502, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6558.389186859131, \"count\": 1, \"min\": 6558.389186859131, \"max\": 6558.389186859131}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:24:06 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=96.8211333590371 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:24:06 INFO 140512987567936] #progress_metric: host=algo-1, completed 6.5 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:24:06 INFO 140512987567936] #quality_metric: host=algo-1, epoch=12, train loss <loss>=2.257334840297699\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:24:06 INFO 140512987567936] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:24:06 INFO 140512987567936] Saved checkpoint to \"/opt/ml/model/state_232f22ad-4f23-4ec1-8762-cddab478a303-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742059446.813916, \"EndTime\": 1742059446.8281837, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 13.884544372558594, \"count\": 1, \"min\": 13.884544372558594, \"max\": 13.884544372558594}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:24:11 INFO 140512987567936] Epoch[13] Batch[0] avg_epoch_loss=2.495199\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:24:11 INFO 140512987567936] #quality_metric: host=algo-1, epoch=13, batch=0 train loss <loss>=2.495198965072632\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:24:12 INFO 140512987567936] Epoch[13] Batch[5] avg_epoch_loss=2.247320\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:24:12 INFO 140512987567936] #quality_metric: host=algo-1, epoch=13, batch=5 train loss <loss>=2.247319976488749\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:24:12 INFO 140512987567936] Epoch[13] Batch [5]#011Speed: 303.23 samples/sec#011loss=2.247320\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:24:13 INFO 140512987567936] Epoch[13] Batch[10] avg_epoch_loss=2.178466\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:24:13 INFO 140512987567936] #quality_metric: host=algo-1, epoch=13, batch=10 train loss <loss>=2.0958417654037476\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:24:13 INFO 140512987567936] Epoch[13] Batch [10]#011Speed: 211.21 samples/sec#011loss=2.095842\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:24:13 INFO 140512987567936] processed a total of 667 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742059446.8282278, \"EndTime\": 1742059453.6018074, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6773.531198501587, \"count\": 1, \"min\": 6773.531198501587, \"max\": 6773.531198501587}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:24:13 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=98.47022964119938 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:24:13 INFO 140512987567936] #progress_metric: host=algo-1, completed 7.0 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:24:13 INFO 140512987567936] #quality_metric: host=algo-1, epoch=13, train loss <loss>=2.1784662441773848\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:24:13 INFO 140512987567936] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:24:13 INFO 140512987567936] Saved checkpoint to \"/opt/ml/model/state_f9e888a1-5fdc-4ba2-9ca2-3f0f7106a16d-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742059453.6018674, \"EndTime\": 1742059453.6174228, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 15.179157257080078, \"count\": 1, \"min\": 15.179157257080078, \"max\": 15.179157257080078}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:24:17 INFO 140512987567936] Epoch[14] Batch[0] avg_epoch_loss=2.287013\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:24:17 INFO 140512987567936] #quality_metric: host=algo-1, epoch=14, batch=0 train loss <loss>=2.287012815475464\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:24:18 INFO 140512987567936] Epoch[14] Batch[5] avg_epoch_loss=2.190624\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:24:18 INFO 140512987567936] #quality_metric: host=algo-1, epoch=14, batch=5 train loss <loss>=2.1906239986419678\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:24:18 INFO 140512987567936] Epoch[14] Batch [5]#011Speed: 300.38 samples/sec#011loss=2.190624\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:24:20 INFO 140512987567936] processed a total of 639 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742059453.617478, \"EndTime\": 1742059460.009339, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6391.812324523926, \"count\": 1, \"min\": 6391.812324523926, \"max\": 6391.812324523926}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:24:20 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=99.97036712332364 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:24:20 INFO 140512987567936] #progress_metric: host=algo-1, completed 7.5 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:24:20 INFO 140512987567936] #quality_metric: host=algo-1, epoch=14, train loss <loss>=2.2813477993011473\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:24:20 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:24:24 INFO 140512987567936] Epoch[15] Batch[0] avg_epoch_loss=2.091317\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:24:24 INFO 140512987567936] #quality_metric: host=algo-1, epoch=15, batch=0 train loss <loss>=2.0913169384002686\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:24:25 INFO 140512987567936] Epoch[15] Batch[5] avg_epoch_loss=2.174133\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:24:25 INFO 140512987567936] #quality_metric: host=algo-1, epoch=15, batch=5 train loss <loss>=2.174132744471232\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:24:25 INFO 140512987567936] Epoch[15] Batch [5]#011Speed: 308.81 samples/sec#011loss=2.174133\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:24:26 INFO 140512987567936] Epoch[15] Batch[10] avg_epoch_loss=1.924420\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:24:26 INFO 140512987567936] #quality_metric: host=algo-1, epoch=15, batch=10 train loss <loss>=1.6247638940811158\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:24:26 INFO 140512987567936] Epoch[15] Batch [10]#011Speed: 230.02 samples/sec#011loss=1.624764\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:24:26 INFO 140512987567936] processed a total of 657 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742059460.0093935, \"EndTime\": 1742059466.6334012, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6623.715400695801, \"count\": 1, \"min\": 6623.715400695801, \"max\": 6623.715400695801}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:24:26 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=99.18679859861449 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:24:26 INFO 140512987567936] #progress_metric: host=algo-1, completed 8.0 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:24:26 INFO 140512987567936] #quality_metric: host=algo-1, epoch=15, train loss <loss>=1.9244196306575427\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:24:26 INFO 140512987567936] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:24:26 INFO 140512987567936] Saved checkpoint to \"/opt/ml/model/state_e576bc44-0d53-477f-b653-1cb414e10670-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742059466.6335166, \"EndTime\": 1742059466.649313, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 15.363216400146484, \"count\": 1, \"min\": 15.363216400146484, \"max\": 15.363216400146484}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:24:30 INFO 140512987567936] Epoch[16] Batch[0] avg_epoch_loss=1.825794\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:24:30 INFO 140512987567936] #quality_metric: host=algo-1, epoch=16, batch=0 train loss <loss>=1.8257943391799927\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:24:32 INFO 140512987567936] Epoch[16] Batch[5] avg_epoch_loss=2.161853\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:24:32 INFO 140512987567936] #quality_metric: host=algo-1, epoch=16, batch=5 train loss <loss>=2.161853472391764\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:24:32 INFO 140512987567936] Epoch[16] Batch [5]#011Speed: 286.27 samples/sec#011loss=2.161853\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:24:33 INFO 140512987567936] Epoch[16] Batch[10] avg_epoch_loss=2.053753\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:24:33 INFO 140512987567936] #quality_metric: host=algo-1, epoch=16, batch=10 train loss <loss>=1.9240320920944214\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:24:33 INFO 140512987567936] Epoch[16] Batch [10]#011Speed: 240.26 samples/sec#011loss=1.924032\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:24:33 INFO 140512987567936] processed a total of 655 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742059466.649378, \"EndTime\": 1742059473.3589234, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6709.490776062012, \"count\": 1, \"min\": 6709.490776062012, \"max\": 6709.490776062012}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:24:33 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=97.62148562432579 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:24:33 INFO 140512987567936] #progress_metric: host=algo-1, completed 8.5 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:24:33 INFO 140512987567936] #quality_metric: host=algo-1, epoch=16, train loss <loss>=2.0537528449838813\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:24:33 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:24:37 INFO 140512987567936] Epoch[17] Batch[0] avg_epoch_loss=1.630750\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:24:37 INFO 140512987567936] #quality_metric: host=algo-1, epoch=17, batch=0 train loss <loss>=1.6307495832443237\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:24:38 INFO 140512987567936] Epoch[17] Batch[5] avg_epoch_loss=1.953544\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:24:38 INFO 140512987567936] #quality_metric: host=algo-1, epoch=17, batch=5 train loss <loss>=1.953543762365977\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:24:38 INFO 140512987567936] Epoch[17] Batch [5]#011Speed: 315.02 samples/sec#011loss=1.953544\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:24:39 INFO 140512987567936] Epoch[17] Batch[10] avg_epoch_loss=2.331243\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:24:39 INFO 140512987567936] #quality_metric: host=algo-1, epoch=17, batch=10 train loss <loss>=2.784482717514038\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:24:39 INFO 140512987567936] Epoch[17] Batch [10]#011Speed: 229.88 samples/sec#011loss=2.784483\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:24:39 INFO 140512987567936] processed a total of 647 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742059473.3589911, \"EndTime\": 1742059479.9876838, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6628.287792205811, \"count\": 1, \"min\": 6628.287792205811, \"max\": 6628.287792205811}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:24:39 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=97.6107873202414 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:24:39 INFO 140512987567936] #progress_metric: host=algo-1, completed 9.0 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:24:39 INFO 140512987567936] #quality_metric: host=algo-1, epoch=17, train loss <loss>=2.3312432874332774\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:24:39 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:24:44 INFO 140512987567936] Epoch[18] Batch[0] avg_epoch_loss=2.094096\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:24:44 INFO 140512987567936] #quality_metric: host=algo-1, epoch=18, batch=0 train loss <loss>=2.0940964221954346\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:24:45 INFO 140512987567936] Epoch[18] Batch[5] avg_epoch_loss=2.171751\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:24:45 INFO 140512987567936] #quality_metric: host=algo-1, epoch=18, batch=5 train loss <loss>=2.1717509031295776\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:24:45 INFO 140512987567936] Epoch[18] Batch [5]#011Speed: 318.06 samples/sec#011loss=2.171751\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:24:46 INFO 140512987567936] processed a total of 611 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742059479.9877348, \"EndTime\": 1742059486.2890947, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6301.040172576904, \"count\": 1, \"min\": 6301.040172576904, \"max\": 6301.040172576904}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:24:46 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=96.96663827726948 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:24:46 INFO 140512987567936] #progress_metric: host=algo-1, completed 9.5 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:24:46 INFO 140512987567936] #quality_metric: host=algo-1, epoch=18, train loss <loss>=2.0886642932891846\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:24:46 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:24:50 INFO 140512987567936] Epoch[19] Batch[0] avg_epoch_loss=2.347783\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:24:50 INFO 140512987567936] #quality_metric: host=algo-1, epoch=19, batch=0 train loss <loss>=2.347783327102661\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:24:51 INFO 140512987567936] Epoch[19] Batch[5] avg_epoch_loss=1.935648\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:24:51 INFO 140512987567936] #quality_metric: host=algo-1, epoch=19, batch=5 train loss <loss>=1.9356479446093242\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:24:51 INFO 140512987567936] Epoch[19] Batch [5]#011Speed: 309.75 samples/sec#011loss=1.935648\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:24:52 INFO 140512987567936] Epoch[19] Batch[10] avg_epoch_loss=1.971655\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:24:52 INFO 140512987567936] #quality_metric: host=algo-1, epoch=19, batch=10 train loss <loss>=2.0148634195327757\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:24:52 INFO 140512987567936] Epoch[19] Batch [10]#011Speed: 217.83 samples/sec#011loss=2.014863\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:24:52 INFO 140512987567936] processed a total of 656 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742059486.28916, \"EndTime\": 1742059492.9665444, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6677.070379257202, \"count\": 1, \"min\": 6677.070379257202, \"max\": 6677.070379257202}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:24:52 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=98.2453193418808 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:24:52 INFO 140512987567936] #progress_metric: host=algo-1, completed 10.0 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:24:52 INFO 140512987567936] #quality_metric: host=algo-1, epoch=19, train loss <loss>=1.9716549786654385\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:24:52 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:24:57 INFO 140512987567936] Epoch[20] Batch[0] avg_epoch_loss=2.403560\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:24:57 INFO 140512987567936] #quality_metric: host=algo-1, epoch=20, batch=0 train loss <loss>=2.403560161590576\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:24:58 INFO 140512987567936] Epoch[20] Batch[5] avg_epoch_loss=1.943998\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:24:58 INFO 140512987567936] #quality_metric: host=algo-1, epoch=20, batch=5 train loss <loss>=1.9439981778462727\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:24:58 INFO 140512987567936] Epoch[20] Batch [5]#011Speed: 302.11 samples/sec#011loss=1.943998\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:24:59 INFO 140512987567936] Epoch[20] Batch[10] avg_epoch_loss=1.919900\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:24:59 INFO 140512987567936] #quality_metric: host=algo-1, epoch=20, batch=10 train loss <loss>=1.8909816503524781\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:24:59 INFO 140512987567936] Epoch[20] Batch [10]#011Speed: 189.22 samples/sec#011loss=1.890982\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:24:59 INFO 140512987567936] processed a total of 697 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742059492.9666061, \"EndTime\": 1742059499.7760825, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6809.181451797485, \"count\": 1, \"min\": 6809.181451797485, \"max\": 6809.181451797485}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:24:59 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=102.36043136459058 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:24:59 INFO 140512987567936] #progress_metric: host=algo-1, completed 10.5 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:24:59 INFO 140512987567936] #quality_metric: host=algo-1, epoch=20, train loss <loss>=1.9198997562581843\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:24:59 INFO 140512987567936] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:24:59 INFO 140512987567936] Saved checkpoint to \"/opt/ml/model/state_4780868d-40f9-496a-ab2f-c792d69cdd81-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742059499.776144, \"EndTime\": 1742059499.7899082, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 13.444662094116211, \"count\": 1, \"min\": 13.444662094116211, \"max\": 13.444662094116211}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:25:04 INFO 140512987567936] Epoch[21] Batch[0] avg_epoch_loss=2.091316\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:25:04 INFO 140512987567936] #quality_metric: host=algo-1, epoch=21, batch=0 train loss <loss>=2.091315746307373\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:25:05 INFO 140512987567936] Epoch[21] Batch[5] avg_epoch_loss=2.144894\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:25:05 INFO 140512987567936] #quality_metric: host=algo-1, epoch=21, batch=5 train loss <loss>=2.1448938250541687\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:25:05 INFO 140512987567936] Epoch[21] Batch [5]#011Speed: 313.88 samples/sec#011loss=2.144894\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:25:06 INFO 140512987567936] Epoch[21] Batch[10] avg_epoch_loss=2.306015\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:25:06 INFO 140512987567936] #quality_metric: host=algo-1, epoch=21, batch=10 train loss <loss>=2.499360466003418\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:25:06 INFO 140512987567936] Epoch[21] Batch [10]#011Speed: 229.50 samples/sec#011loss=2.499360\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:25:06 INFO 140512987567936] processed a total of 665 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742059499.789967, \"EndTime\": 1742059506.4834168, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6693.396329879761, \"count\": 1, \"min\": 6693.396329879761, \"max\": 6693.396329879761}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:25:06 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=99.35007269564011 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:25:06 INFO 140512987567936] #progress_metric: host=algo-1, completed 11.0 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:25:06 INFO 140512987567936] #quality_metric: host=algo-1, epoch=21, train loss <loss>=2.3060150254856455\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:25:06 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:25:10 INFO 140512987567936] Epoch[22] Batch[0] avg_epoch_loss=1.551474\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:25:10 INFO 140512987567936] #quality_metric: host=algo-1, epoch=22, batch=0 train loss <loss>=1.551473617553711\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:25:11 INFO 140512987567936] Epoch[22] Batch[5] avg_epoch_loss=1.698386\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:25:11 INFO 140512987567936] #quality_metric: host=algo-1, epoch=22, batch=5 train loss <loss>=1.6983856558799744\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:25:11 INFO 140512987567936] Epoch[22] Batch [5]#011Speed: 300.76 samples/sec#011loss=1.698386\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:25:12 INFO 140512987567936] processed a total of 605 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742059506.4834898, \"EndTime\": 1742059512.8487597, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6364.859819412231, \"count\": 1, \"min\": 6364.859819412231, \"max\": 6364.859819412231}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:25:12 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=95.05158372613619 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:25:12 INFO 140512987567936] #progress_metric: host=algo-1, completed 11.5 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:25:12 INFO 140512987567936] #quality_metric: host=algo-1, epoch=22, train loss <loss>=1.9706133246421813\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:25:12 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:25:17 INFO 140512987567936] Epoch[23] Batch[0] avg_epoch_loss=2.143272\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:25:17 INFO 140512987567936] #quality_metric: host=algo-1, epoch=23, batch=0 train loss <loss>=2.1432723999023438\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:25:18 INFO 140512987567936] Epoch[23] Batch[5] avg_epoch_loss=2.111782\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:25:18 INFO 140512987567936] #quality_metric: host=algo-1, epoch=23, batch=5 train loss <loss>=2.1117820143699646\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:25:18 INFO 140512987567936] Epoch[23] Batch [5]#011Speed: 308.02 samples/sec#011loss=2.111782\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:25:19 INFO 140512987567936] processed a total of 609 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742059512.84883, \"EndTime\": 1742059519.1305594, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6281.412601470947, \"count\": 1, \"min\": 6281.412601470947, \"max\": 6281.412601470947}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:25:19 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=96.95117588732226 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:25:19 INFO 140512987567936] #progress_metric: host=algo-1, completed 12.0 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:25:19 INFO 140512987567936] #quality_metric: host=algo-1, epoch=23, train loss <loss>=1.9963807821273805\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:25:19 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:25:23 INFO 140512987567936] Epoch[24] Batch[0] avg_epoch_loss=2.324568\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:25:23 INFO 140512987567936] #quality_metric: host=algo-1, epoch=24, batch=0 train loss <loss>=2.3245677947998047\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:25:24 INFO 140512987567936] Epoch[24] Batch[5] avg_epoch_loss=1.820274\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:25:24 INFO 140512987567936] #quality_metric: host=algo-1, epoch=24, batch=5 train loss <loss>=1.820273518562317\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:25:24 INFO 140512987567936] Epoch[24] Batch [5]#011Speed: 326.90 samples/sec#011loss=1.820274\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:25:25 INFO 140512987567936] Epoch[24] Batch[10] avg_epoch_loss=1.964396\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:25:25 INFO 140512987567936] #quality_metric: host=algo-1, epoch=24, batch=10 train loss <loss>=2.1373422622680662\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:25:25 INFO 140512987567936] Epoch[24] Batch [10]#011Speed: 240.50 samples/sec#011loss=2.137342\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:25:25 INFO 140512987567936] processed a total of 661 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742059519.1306295, \"EndTime\": 1742059525.603777, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6472.817420959473, \"count\": 1, \"min\": 6472.817420959473, \"max\": 6472.817420959473}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:25:25 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=102.11795373728988 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:25:25 INFO 140512987567936] #progress_metric: host=algo-1, completed 12.5 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:25:25 INFO 140512987567936] #quality_metric: host=algo-1, epoch=24, train loss <loss>=1.9643956747922031\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:25:25 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:25:29 INFO 140512987567936] Epoch[25] Batch[0] avg_epoch_loss=2.281297\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:25:29 INFO 140512987567936] #quality_metric: host=algo-1, epoch=25, batch=0 train loss <loss>=2.281296730041504\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:25:30 INFO 140512987567936] Epoch[25] Batch[5] avg_epoch_loss=1.944328\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:25:30 INFO 140512987567936] #quality_metric: host=algo-1, epoch=25, batch=5 train loss <loss>=1.9443280299504597\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:25:30 INFO 140512987567936] Epoch[25] Batch [5]#011Speed: 304.20 samples/sec#011loss=1.944328\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:25:31 INFO 140512987567936] processed a total of 635 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742059525.603838, \"EndTime\": 1742059531.962003, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6357.893943786621, \"count\": 1, \"min\": 6357.893943786621, \"max\": 6357.893943786621}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:25:31 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=99.87422232681945 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:25:31 INFO 140512987567936] #progress_metric: host=algo-1, completed 13.0 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:25:31 INFO 140512987567936] #quality_metric: host=algo-1, epoch=25, train loss <loss>=1.9184866547584534\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:25:31 INFO 140512987567936] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:25:31 INFO 140512987567936] Saved checkpoint to \"/opt/ml/model/state_85b76b9f-e18d-436c-b1b7-9e68cc8d4896-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742059531.9620743, \"EndTime\": 1742059531.9765384, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 13.144254684448242, \"count\": 1, \"min\": 13.144254684448242, \"max\": 13.144254684448242}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:25:36 INFO 140512987567936] Epoch[26] Batch[0] avg_epoch_loss=1.812793\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:25:36 INFO 140512987567936] #quality_metric: host=algo-1, epoch=26, batch=0 train loss <loss>=1.8127933740615845\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:25:37 INFO 140512987567936] Epoch[26] Batch[5] avg_epoch_loss=1.754882\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:25:37 INFO 140512987567936] #quality_metric: host=algo-1, epoch=26, batch=5 train loss <loss>=1.7548822959264119\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:25:37 INFO 140512987567936] Epoch[26] Batch [5]#011Speed: 305.22 samples/sec#011loss=1.754882\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:25:38 INFO 140512987567936] Epoch[26] Batch[10] avg_epoch_loss=1.632434\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:25:38 INFO 140512987567936] #quality_metric: host=algo-1, epoch=26, batch=10 train loss <loss>=1.4854961335659027\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:25:38 INFO 140512987567936] Epoch[26] Batch [10]#011Speed: 235.21 samples/sec#011loss=1.485496\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:25:38 INFO 140512987567936] processed a total of 653 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742059531.9766016, \"EndTime\": 1742059538.6536462, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6676.984071731567, \"count\": 1, \"min\": 6676.984071731567, \"max\": 6676.984071731567}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:25:38 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=97.79651559986748 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:25:38 INFO 140512987567936] #progress_metric: host=algo-1, completed 13.5 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:25:38 INFO 140512987567936] #quality_metric: host=algo-1, epoch=26, train loss <loss>=1.6324340403079987\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:25:38 INFO 140512987567936] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:25:38 INFO 140512987567936] Saved checkpoint to \"/opt/ml/model/state_534cea23-1ecf-48b5-bdf4-7719474fa019-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742059538.6537554, \"EndTime\": 1742059538.6667213, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 12.517452239990234, \"count\": 1, \"min\": 12.517452239990234, \"max\": 12.517452239990234}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:25:42 INFO 140512987567936] Epoch[27] Batch[0] avg_epoch_loss=1.822447\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:25:42 INFO 140512987567936] #quality_metric: host=algo-1, epoch=27, batch=0 train loss <loss>=1.8224472999572754\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:25:43 INFO 140512987567936] Epoch[27] Batch[5] avg_epoch_loss=1.762116\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:25:43 INFO 140512987567936] #quality_metric: host=algo-1, epoch=27, batch=5 train loss <loss>=1.7621163527170818\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:25:43 INFO 140512987567936] Epoch[27] Batch [5]#011Speed: 308.87 samples/sec#011loss=1.762116\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:25:45 INFO 140512987567936] Epoch[27] Batch[10] avg_epoch_loss=1.880314\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:25:45 INFO 140512987567936] #quality_metric: host=algo-1, epoch=27, batch=10 train loss <loss>=2.022150444984436\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:25:45 INFO 140512987567936] Epoch[27] Batch [10]#011Speed: 223.98 samples/sec#011loss=2.022150\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:25:45 INFO 140512987567936] processed a total of 680 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742059538.6667724, \"EndTime\": 1742059545.2333622, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6566.537618637085, \"count\": 1, \"min\": 6566.537618637085, \"max\": 6566.537618637085}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:25:45 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=103.55361631502602 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:25:45 INFO 140512987567936] #progress_metric: host=algo-1, completed 14.0 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:25:45 INFO 140512987567936] #quality_metric: host=algo-1, epoch=27, train loss <loss>=1.880313667384061\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:25:45 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:25:49 INFO 140512987567936] Epoch[28] Batch[0] avg_epoch_loss=1.865195\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:25:49 INFO 140512987567936] #quality_metric: host=algo-1, epoch=28, batch=0 train loss <loss>=1.8651951551437378\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:25:50 INFO 140512987567936] Epoch[28] Batch[5] avg_epoch_loss=1.891062\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:25:50 INFO 140512987567936] #quality_metric: host=algo-1, epoch=28, batch=5 train loss <loss>=1.8910621802012126\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:25:50 INFO 140512987567936] Epoch[28] Batch [5]#011Speed: 295.78 samples/sec#011loss=1.891062\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:25:51 INFO 140512987567936] Epoch[28] Batch[10] avg_epoch_loss=1.855711\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:25:51 INFO 140512987567936] #quality_metric: host=algo-1, epoch=28, batch=10 train loss <loss>=1.8132904529571534\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:25:51 INFO 140512987567936] Epoch[28] Batch [10]#011Speed: 225.08 samples/sec#011loss=1.813290\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:25:51 INFO 140512987567936] processed a total of 653 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742059545.233436, \"EndTime\": 1742059551.9706707, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6736.807584762573, \"count\": 1, \"min\": 6736.807584762573, \"max\": 6736.807584762573}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:25:51 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=96.92845529186098 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:25:51 INFO 140512987567936] #progress_metric: host=algo-1, completed 14.5 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:25:51 INFO 140512987567936] #quality_metric: host=algo-1, epoch=28, train loss <loss>=1.8557113950902766\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:25:51 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:25:56 INFO 140512987567936] Epoch[29] Batch[0] avg_epoch_loss=2.015482\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:25:56 INFO 140512987567936] #quality_metric: host=algo-1, epoch=29, batch=0 train loss <loss>=2.0154824256896973\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:25:57 INFO 140512987567936] Epoch[29] Batch[5] avg_epoch_loss=1.749650\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:25:57 INFO 140512987567936] #quality_metric: host=algo-1, epoch=29, batch=5 train loss <loss>=1.7496498425801594\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:25:57 INFO 140512987567936] Epoch[29] Batch [5]#011Speed: 284.70 samples/sec#011loss=1.749650\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:25:58 INFO 140512987567936] processed a total of 640 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742059551.9707582, \"EndTime\": 1742059558.389154, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6418.061256408691, \"count\": 1, \"min\": 6418.061256408691, \"max\": 6418.061256408691}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:25:58 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=99.71701271093966 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:25:58 INFO 140512987567936] #progress_metric: host=algo-1, completed 15.0 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:25:58 INFO 140512987567936] #quality_metric: host=algo-1, epoch=29, train loss <loss>=1.7900934219360352\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:25:58 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:26:02 INFO 140512987567936] Epoch[30] Batch[0] avg_epoch_loss=1.622548\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:26:02 INFO 140512987567936] #quality_metric: host=algo-1, epoch=30, batch=0 train loss <loss>=1.6225484609603882\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:26:03 INFO 140512987567936] Epoch[30] Batch[5] avg_epoch_loss=1.894731\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:26:03 INFO 140512987567936] #quality_metric: host=algo-1, epoch=30, batch=5 train loss <loss>=1.8947306672732036\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:26:03 INFO 140512987567936] Epoch[30] Batch [5]#011Speed: 294.80 samples/sec#011loss=1.894731\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:26:04 INFO 140512987567936] processed a total of 636 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742059558.3892221, \"EndTime\": 1742059564.7308598, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6341.191053390503, \"count\": 1, \"min\": 6341.191053390503, \"max\": 6341.191053390503}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:26:04 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=100.29492975422721 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:26:04 INFO 140512987567936] #progress_metric: host=algo-1, completed 15.5 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:26:04 INFO 140512987567936] #quality_metric: host=algo-1, epoch=30, train loss <loss>=1.8841474890708922\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:26:04 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:26:08 INFO 140512987567936] Epoch[31] Batch[0] avg_epoch_loss=2.378874\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:26:08 INFO 140512987567936] #quality_metric: host=algo-1, epoch=31, batch=0 train loss <loss>=2.3788743019104004\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:26:09 INFO 140512987567936] Epoch[31] Batch[5] avg_epoch_loss=1.833516\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:26:09 INFO 140512987567936] #quality_metric: host=algo-1, epoch=31, batch=5 train loss <loss>=1.8335163593292236\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:26:09 INFO 140512987567936] Epoch[31] Batch [5]#011Speed: 305.15 samples/sec#011loss=1.833516\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:26:10 INFO 140512987567936] processed a total of 626 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742059564.730933, \"EndTime\": 1742059570.976699, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6244.328260421753, \"count\": 1, \"min\": 6244.328260421753, \"max\": 6244.328260421753}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:26:10 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=100.24800917370314 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:26:10 INFO 140512987567936] #progress_metric: host=algo-1, completed 16.0 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:26:10 INFO 140512987567936] #quality_metric: host=algo-1, epoch=31, train loss <loss>=1.7998852372169494\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:26:10 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:26:15 INFO 140512987567936] Epoch[32] Batch[0] avg_epoch_loss=1.854937\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:26:15 INFO 140512987567936] #quality_metric: host=algo-1, epoch=32, batch=0 train loss <loss>=1.8549368381500244\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:26:16 INFO 140512987567936] Epoch[32] Batch[5] avg_epoch_loss=1.594010\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:26:16 INFO 140512987567936] #quality_metric: host=algo-1, epoch=32, batch=5 train loss <loss>=1.5940099557240803\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:26:16 INFO 140512987567936] Epoch[32] Batch [5]#011Speed: 304.52 samples/sec#011loss=1.594010\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:26:17 INFO 140512987567936] processed a total of 624 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742059570.9767783, \"EndTime\": 1742059577.2007625, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6223.488807678223, \"count\": 1, \"min\": 6223.488807678223, \"max\": 6223.488807678223}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:26:17 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=100.26362974665602 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:26:17 INFO 140512987567936] #progress_metric: host=algo-1, completed 16.5 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:26:17 INFO 140512987567936] #quality_metric: host=algo-1, epoch=32, train loss <loss>=1.6185405850410461\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:26:17 INFO 140512987567936] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:26:17 INFO 140512987567936] Saved checkpoint to \"/opt/ml/model/state_f3e96bb8-59f2-49cf-9a12-e6dd37171c57-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742059577.200833, \"EndTime\": 1742059577.2140634, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 12.675762176513672, \"count\": 1, \"min\": 12.675762176513672, \"max\": 12.675762176513672}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:26:21 INFO 140512987567936] Epoch[33] Batch[0] avg_epoch_loss=1.711502\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:26:21 INFO 140512987567936] #quality_metric: host=algo-1, epoch=33, batch=0 train loss <loss>=1.7115018367767334\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:26:22 INFO 140512987567936] Epoch[33] Batch[5] avg_epoch_loss=1.804690\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:26:22 INFO 140512987567936] #quality_metric: host=algo-1, epoch=33, batch=5 train loss <loss>=1.8046898047129314\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:26:22 INFO 140512987567936] Epoch[33] Batch [5]#011Speed: 303.84 samples/sec#011loss=1.804690\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:26:23 INFO 140512987567936] Epoch[33] Batch[10] avg_epoch_loss=1.951880\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:26:23 INFO 140512987567936] #quality_metric: host=algo-1, epoch=33, batch=10 train loss <loss>=2.1285073280334474\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:26:23 INFO 140512987567936] Epoch[33] Batch [10]#011Speed: 250.85 samples/sec#011loss=2.128507\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:26:23 INFO 140512987567936] processed a total of 651 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742059577.2141285, \"EndTime\": 1742059583.6769967, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6462.81361579895, \"count\": 1, \"min\": 6462.81361579895, \"max\": 6462.81361579895}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:26:23 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=100.72846406694602 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:26:23 INFO 140512987567936] #progress_metric: host=algo-1, completed 17.0 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:26:23 INFO 140512987567936] #quality_metric: host=algo-1, epoch=33, train loss <loss>=1.9518795880404385\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:26:23 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:26:27 INFO 140512987567936] Epoch[34] Batch[0] avg_epoch_loss=1.654058\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:26:27 INFO 140512987567936] #quality_metric: host=algo-1, epoch=34, batch=0 train loss <loss>=1.6540577411651611\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:26:28 INFO 140512987567936] Epoch[34] Batch[5] avg_epoch_loss=1.564719\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:26:28 INFO 140512987567936] #quality_metric: host=algo-1, epoch=34, batch=5 train loss <loss>=1.5647189418474834\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:26:28 INFO 140512987567936] Epoch[34] Batch [5]#011Speed: 306.16 samples/sec#011loss=1.564719\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:26:30 INFO 140512987567936] Epoch[34] Batch[10] avg_epoch_loss=1.435500\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:26:30 INFO 140512987567936] #quality_metric: host=algo-1, epoch=34, batch=10 train loss <loss>=1.2804375886917114\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:26:30 INFO 140512987567936] Epoch[34] Batch [10]#011Speed: 228.94 samples/sec#011loss=1.280438\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:26:30 INFO 140512987567936] processed a total of 645 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742059583.6770706, \"EndTime\": 1742059590.3281908, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6650.680303573608, \"count\": 1, \"min\": 6650.680303573608, \"max\": 6650.680303573608}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:26:30 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=96.98135336096855 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:26:30 INFO 140512987567936] #progress_metric: host=algo-1, completed 17.5 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:26:30 INFO 140512987567936] #quality_metric: host=algo-1, epoch=34, train loss <loss>=1.435500144958496\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:26:30 INFO 140512987567936] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:26:30 INFO 140512987567936] Saved checkpoint to \"/opt/ml/model/state_fac0660e-4388-4441-b611-17b9ab60228f-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742059590.3282433, \"EndTime\": 1742059590.3419187, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 13.347864151000977, \"count\": 1, \"min\": 13.347864151000977, \"max\": 13.347864151000977}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:26:34 INFO 140512987567936] Epoch[35] Batch[0] avg_epoch_loss=2.193108\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:26:34 INFO 140512987567936] #quality_metric: host=algo-1, epoch=35, batch=0 train loss <loss>=2.193107843399048\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:26:35 INFO 140512987567936] Epoch[35] Batch[5] avg_epoch_loss=2.009323\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:26:35 INFO 140512987567936] #quality_metric: host=algo-1, epoch=35, batch=5 train loss <loss>=2.009323239326477\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:26:35 INFO 140512987567936] Epoch[35] Batch [5]#011Speed: 321.48 samples/sec#011loss=2.009323\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:26:36 INFO 140512987567936] processed a total of 619 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742059590.3419771, \"EndTime\": 1742059596.5818083, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6239.736080169678, \"count\": 1, \"min\": 6239.736080169678, \"max\": 6239.736080169678}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:26:36 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=99.20015430184795 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:26:36 INFO 140512987567936] #progress_metric: host=algo-1, completed 18.0 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:26:36 INFO 140512987567936] #quality_metric: host=algo-1, epoch=35, train loss <loss>=1.9338812232017517\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:26:36 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:26:40 INFO 140512987567936] Epoch[36] Batch[0] avg_epoch_loss=1.656588\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:26:40 INFO 140512987567936] #quality_metric: host=algo-1, epoch=36, batch=0 train loss <loss>=1.656587839126587\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:26:41 INFO 140512987567936] Epoch[36] Batch[5] avg_epoch_loss=1.689794\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:26:41 INFO 140512987567936] #quality_metric: host=algo-1, epoch=36, batch=5 train loss <loss>=1.6897942026456196\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:26:41 INFO 140512987567936] Epoch[36] Batch [5]#011Speed: 295.37 samples/sec#011loss=1.689794\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:26:42 INFO 140512987567936] processed a total of 628 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742059596.581907, \"EndTime\": 1742059602.9020817, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6319.63324546814, \"count\": 1, \"min\": 6319.63324546814, \"max\": 6319.63324546814}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:26:42 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=99.37124699493249 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:26:42 INFO 140512987567936] #progress_metric: host=algo-1, completed 18.5 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:26:42 INFO 140512987567936] #quality_metric: host=algo-1, epoch=36, train loss <loss>=1.6942284464836121\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:26:42 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:26:47 INFO 140512987567936] Epoch[37] Batch[0] avg_epoch_loss=1.685545\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:26:47 INFO 140512987567936] #quality_metric: host=algo-1, epoch=37, batch=0 train loss <loss>=1.6855450868606567\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:26:48 INFO 140512987567936] Epoch[37] Batch[5] avg_epoch_loss=1.938688\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:26:48 INFO 140512987567936] #quality_metric: host=algo-1, epoch=37, batch=5 train loss <loss>=1.9386875828107197\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:26:48 INFO 140512987567936] Epoch[37] Batch [5]#011Speed: 298.31 samples/sec#011loss=1.938688\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:26:49 INFO 140512987567936] Epoch[37] Batch[10] avg_epoch_loss=1.827463\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:26:49 INFO 140512987567936] #quality_metric: host=algo-1, epoch=37, batch=10 train loss <loss>=1.6939926624298096\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:26:49 INFO 140512987567936] Epoch[37] Batch [10]#011Speed: 213.41 samples/sec#011loss=1.693993\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:26:49 INFO 140512987567936] processed a total of 681 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742059602.902148, \"EndTime\": 1742059609.9502027, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 7047.5616455078125, \"count\": 1, \"min\": 7047.5616455078125, \"max\": 7047.5616455078125}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:26:49 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=96.62791668659789 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:26:49 INFO 140512987567936] #progress_metric: host=algo-1, completed 19.0 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:26:49 INFO 140512987567936] #quality_metric: host=algo-1, epoch=37, train loss <loss>=1.827462619001215\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:26:49 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:26:54 INFO 140512987567936] Epoch[38] Batch[0] avg_epoch_loss=1.874834\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:26:54 INFO 140512987567936] #quality_metric: host=algo-1, epoch=38, batch=0 train loss <loss>=1.8748338222503662\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:26:55 INFO 140512987567936] Epoch[38] Batch[5] avg_epoch_loss=1.844925\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:26:55 INFO 140512987567936] #quality_metric: host=algo-1, epoch=38, batch=5 train loss <loss>=1.844924509525299\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:26:55 INFO 140512987567936] Epoch[38] Batch [5]#011Speed: 318.37 samples/sec#011loss=1.844925\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:26:56 INFO 140512987567936] Epoch[38] Batch[10] avg_epoch_loss=1.663327\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:26:56 INFO 140512987567936] #quality_metric: host=algo-1, epoch=38, batch=10 train loss <loss>=1.4454099893569947\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:26:56 INFO 140512987567936] Epoch[38] Batch [10]#011Speed: 221.42 samples/sec#011loss=1.445410\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:26:56 INFO 140512987567936] processed a total of 663 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742059609.950262, \"EndTime\": 1742059616.566067, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6615.499019622803, \"count\": 1, \"min\": 6615.499019622803, \"max\": 6615.499019622803}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:26:56 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=100.21768012595145 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:26:56 INFO 140512987567936] #progress_metric: host=algo-1, completed 19.5 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:26:56 INFO 140512987567936] #quality_metric: host=algo-1, epoch=38, train loss <loss>=1.663327000357888\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:26:56 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:27:00 INFO 140512987567936] Epoch[39] Batch[0] avg_epoch_loss=2.253870\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:27:00 INFO 140512987567936] #quality_metric: host=algo-1, epoch=39, batch=0 train loss <loss>=2.2538704872131348\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:27:01 INFO 140512987567936] Epoch[39] Batch[5] avg_epoch_loss=1.782263\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:27:01 INFO 140512987567936] #quality_metric: host=algo-1, epoch=39, batch=5 train loss <loss>=1.7822630802790325\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:27:01 INFO 140512987567936] Epoch[39] Batch [5]#011Speed: 281.12 samples/sec#011loss=1.782263\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:27:02 INFO 140512987567936] processed a total of 638 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742059616.5661337, \"EndTime\": 1742059622.9823496, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6415.818214416504, \"count\": 1, \"min\": 6415.818214416504, \"max\": 6415.818214416504}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:27:02 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=99.44019408178391 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:27:02 INFO 140512987567936] #progress_metric: host=algo-1, completed 20.0 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:27:02 INFO 140512987567936] #quality_metric: host=algo-1, epoch=39, train loss <loss>=1.7068609356880189\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:27:02 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:27:07 INFO 140512987567936] Epoch[40] Batch[0] avg_epoch_loss=1.422679\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:27:07 INFO 140512987567936] #quality_metric: host=algo-1, epoch=40, batch=0 train loss <loss>=1.422678828239441\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:27:08 INFO 140512987567936] Epoch[40] Batch[5] avg_epoch_loss=1.432754\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:27:08 INFO 140512987567936] #quality_metric: host=algo-1, epoch=40, batch=5 train loss <loss>=1.4327539205551147\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:27:08 INFO 140512987567936] Epoch[40] Batch [5]#011Speed: 315.18 samples/sec#011loss=1.432754\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:27:09 INFO 140512987567936] processed a total of 621 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742059622.9824185, \"EndTime\": 1742059629.173651, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6190.642833709717, \"count\": 1, \"min\": 6190.642833709717, \"max\": 6190.642833709717}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:27:09 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=100.31105783944987 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:27:09 INFO 140512987567936] #progress_metric: host=algo-1, completed 20.5 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:27:09 INFO 140512987567936] #quality_metric: host=algo-1, epoch=40, train loss <loss>=1.4150983214378356\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:27:09 INFO 140512987567936] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:27:09 INFO 140512987567936] Saved checkpoint to \"/opt/ml/model/state_c7536492-6ab0-44cd-82a7-fa54a7b692db-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742059629.1737175, \"EndTime\": 1742059629.1872077, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 12.959957122802734, \"count\": 1, \"min\": 12.959957122802734, \"max\": 12.959957122802734}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:27:13 INFO 140512987567936] Epoch[41] Batch[0] avg_epoch_loss=1.310396\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:27:13 INFO 140512987567936] #quality_metric: host=algo-1, epoch=41, batch=0 train loss <loss>=1.3103957176208496\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:27:14 INFO 140512987567936] Epoch[41] Batch[5] avg_epoch_loss=1.538376\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:27:14 INFO 140512987567936] #quality_metric: host=algo-1, epoch=41, batch=5 train loss <loss>=1.5383757948875427\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:27:14 INFO 140512987567936] Epoch[41] Batch [5]#011Speed: 307.44 samples/sec#011loss=1.538376\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:27:15 INFO 140512987567936] Epoch[41] Batch[10] avg_epoch_loss=1.817554\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:27:15 INFO 140512987567936] #quality_metric: host=algo-1, epoch=41, batch=10 train loss <loss>=2.1525668621063234\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:27:15 INFO 140512987567936] Epoch[41] Batch [10]#011Speed: 224.06 samples/sec#011loss=2.152567\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:27:15 INFO 140512987567936] processed a total of 652 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742059629.1872811, \"EndTime\": 1742059635.7774305, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6590.092182159424, \"count\": 1, \"min\": 6590.092182159424, \"max\": 6590.092182159424}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:27:15 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=98.93500823172218 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:27:15 INFO 140512987567936] #progress_metric: host=algo-1, completed 21.0 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:27:15 INFO 140512987567936] #quality_metric: host=algo-1, epoch=41, train loss <loss>=1.8175535527142612\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:27:15 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:27:19 INFO 140512987567936] Epoch[42] Batch[0] avg_epoch_loss=1.661054\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:27:19 INFO 140512987567936] #quality_metric: host=algo-1, epoch=42, batch=0 train loss <loss>=1.6610543727874756\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:27:20 INFO 140512987567936] Epoch[42] Batch[5] avg_epoch_loss=1.581879\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:27:20 INFO 140512987567936] #quality_metric: host=algo-1, epoch=42, batch=5 train loss <loss>=1.5818793177604675\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:27:20 INFO 140512987567936] Epoch[42] Batch [5]#011Speed: 308.68 samples/sec#011loss=1.581879\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:27:22 INFO 140512987567936] Epoch[42] Batch[10] avg_epoch_loss=1.616730\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:27:22 INFO 140512987567936] #quality_metric: host=algo-1, epoch=42, batch=10 train loss <loss>=1.6585505485534668\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:27:22 INFO 140512987567936] Epoch[42] Batch [10]#011Speed: 239.90 samples/sec#011loss=1.658551\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:27:22 INFO 140512987567936] processed a total of 649 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742059635.777493, \"EndTime\": 1742059642.2739117, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6496.088027954102, \"count\": 1, \"min\": 6496.088027954102, \"max\": 6496.088027954102}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:27:22 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=99.90481495041944 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:27:22 INFO 140512987567936] #progress_metric: host=algo-1, completed 21.5 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:27:22 INFO 140512987567936] #quality_metric: host=algo-1, epoch=42, train loss <loss>=1.6167298772118308\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:27:22 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:27:26 INFO 140512987567936] Epoch[43] Batch[0] avg_epoch_loss=2.289099\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:27:26 INFO 140512987567936] #quality_metric: host=algo-1, epoch=43, batch=0 train loss <loss>=2.2890989780426025\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:27:27 INFO 140512987567936] Epoch[43] Batch[5] avg_epoch_loss=1.804628\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:27:27 INFO 140512987567936] #quality_metric: host=algo-1, epoch=43, batch=5 train loss <loss>=1.8046283324559529\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:27:27 INFO 140512987567936] Epoch[43] Batch [5]#011Speed: 290.07 samples/sec#011loss=1.804628\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:27:28 INFO 140512987567936] Epoch[43] Batch[10] avg_epoch_loss=1.631461\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:27:28 INFO 140512987567936] #quality_metric: host=algo-1, epoch=43, batch=10 train loss <loss>=1.4236611068248748\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:27:28 INFO 140512987567936] Epoch[43] Batch [10]#011Speed: 213.35 samples/sec#011loss=1.423661\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:27:28 INFO 140512987567936] processed a total of 668 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742059642.2739751, \"EndTime\": 1742059648.9163454, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6642.067670822144, \"count\": 1, \"min\": 6642.067670822144, \"max\": 6642.067670822144}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:27:28 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=100.56960487432852 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:27:28 INFO 140512987567936] #progress_metric: host=algo-1, completed 22.0 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:27:28 INFO 140512987567936] #quality_metric: host=algo-1, epoch=43, train loss <loss>=1.6314614117145538\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:27:28 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:27:33 INFO 140512987567936] Epoch[44] Batch[0] avg_epoch_loss=2.224972\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:27:33 INFO 140512987567936] #quality_metric: host=algo-1, epoch=44, batch=0 train loss <loss>=2.2249724864959717\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:27:34 INFO 140512987567936] Epoch[44] Batch[5] avg_epoch_loss=1.859408\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:27:34 INFO 140512987567936] #quality_metric: host=algo-1, epoch=44, batch=5 train loss <loss>=1.8594078818957012\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:27:34 INFO 140512987567936] Epoch[44] Batch [5]#011Speed: 305.40 samples/sec#011loss=1.859408\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:27:35 INFO 140512987567936] processed a total of 630 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742059648.9164124, \"EndTime\": 1742059655.112292, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6195.576190948486, \"count\": 1, \"min\": 6195.576190948486, \"max\": 6195.576190948486}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:27:35 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=101.68384934339664 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:27:35 INFO 140512987567936] #progress_metric: host=algo-1, completed 22.5 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:27:35 INFO 140512987567936] #quality_metric: host=algo-1, epoch=44, train loss <loss>=1.6962124943733214\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:27:35 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:27:39 INFO 140512987567936] Epoch[45] Batch[0] avg_epoch_loss=1.879049\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:27:39 INFO 140512987567936] #quality_metric: host=algo-1, epoch=45, batch=0 train loss <loss>=1.879049301147461\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:27:40 INFO 140512987567936] Epoch[45] Batch[5] avg_epoch_loss=1.499522\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:27:40 INFO 140512987567936] #quality_metric: host=algo-1, epoch=45, batch=5 train loss <loss>=1.49952232837677\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:27:40 INFO 140512987567936] Epoch[45] Batch [5]#011Speed: 317.79 samples/sec#011loss=1.499522\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:27:41 INFO 140512987567936] processed a total of 577 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742059655.112358, \"EndTime\": 1742059661.2959425, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6182.379722595215, \"count\": 1, \"min\": 6182.379722595215, \"max\": 6182.379722595215}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:27:41 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=93.32827101321563 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:27:41 INFO 140512987567936] #progress_metric: host=algo-1, completed 23.0 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:27:41 INFO 140512987567936] #quality_metric: host=algo-1, epoch=45, train loss <loss>=1.4657963514328003\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:27:41 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:27:45 INFO 140512987567936] Epoch[46] Batch[0] avg_epoch_loss=2.000312\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:27:45 INFO 140512987567936] #quality_metric: host=algo-1, epoch=46, batch=0 train loss <loss>=2.000311851501465\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:27:46 INFO 140512987567936] Epoch[46] Batch[5] avg_epoch_loss=1.721032\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:27:46 INFO 140512987567936] #quality_metric: host=algo-1, epoch=46, batch=5 train loss <loss>=1.7210322221120198\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:27:46 INFO 140512987567936] Epoch[46] Batch [5]#011Speed: 290.18 samples/sec#011loss=1.721032\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:27:48 INFO 140512987567936] Epoch[46] Batch[10] avg_epoch_loss=1.872922\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:27:48 INFO 140512987567936] #quality_metric: host=algo-1, epoch=46, batch=10 train loss <loss>=2.05519015789032\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:27:48 INFO 140512987567936] Epoch[46] Batch [10]#011Speed: 236.34 samples/sec#011loss=2.055190\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:27:48 INFO 140512987567936] processed a total of 666 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742059661.296009, \"EndTime\": 1742059668.311642, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 7015.200138092041, \"count\": 1, \"min\": 7015.200138092041, \"max\": 7015.200138092041}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:27:48 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=94.93541640200431 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:27:48 INFO 140512987567936] #progress_metric: host=algo-1, completed 23.5 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:27:48 INFO 140512987567936] #quality_metric: host=algo-1, epoch=46, train loss <loss>=1.8729221929203381\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:27:48 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:27:52 INFO 140512987567936] Epoch[47] Batch[0] avg_epoch_loss=1.932765\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:27:52 INFO 140512987567936] #quality_metric: host=algo-1, epoch=47, batch=0 train loss <loss>=1.9327654838562012\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:27:53 INFO 140512987567936] Epoch[47] Batch[5] avg_epoch_loss=1.439138\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:27:53 INFO 140512987567936] #quality_metric: host=algo-1, epoch=47, batch=5 train loss <loss>=1.4391378164291382\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:27:53 INFO 140512987567936] Epoch[47] Batch [5]#011Speed: 317.59 samples/sec#011loss=1.439138\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:27:54 INFO 140512987567936] processed a total of 629 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742059668.311706, \"EndTime\": 1742059674.485734, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6173.724174499512, \"count\": 1, \"min\": 6173.724174499512, \"max\": 6173.724174499512}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:27:54 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=101.88181284801524 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:27:54 INFO 140512987567936] #progress_metric: host=algo-1, completed 24.0 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:27:54 INFO 140512987567936] #quality_metric: host=algo-1, epoch=47, train loss <loss>=1.4719187021255493\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:27:54 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:27:58 INFO 140512987567936] Epoch[48] Batch[0] avg_epoch_loss=1.219747\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:27:58 INFO 140512987567936] #quality_metric: host=algo-1, epoch=48, batch=0 train loss <loss>=1.2197470664978027\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:27:59 INFO 140512987567936] Epoch[48] Batch[5] avg_epoch_loss=1.550331\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:27:59 INFO 140512987567936] #quality_metric: host=algo-1, epoch=48, batch=5 train loss <loss>=1.5503312548001607\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:27:59 INFO 140512987567936] Epoch[48] Batch [5]#011Speed: 302.23 samples/sec#011loss=1.550331\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:28:01 INFO 140512987567936] Epoch[48] Batch[10] avg_epoch_loss=1.814953\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:28:01 INFO 140512987567936] #quality_metric: host=algo-1, epoch=48, batch=10 train loss <loss>=2.1324992418289184\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:28:01 INFO 140512987567936] Epoch[48] Batch [10]#011Speed: 239.21 samples/sec#011loss=2.132499\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:28:01 INFO 140512987567936] processed a total of 653 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742059674.4858007, \"EndTime\": 1742059681.0487046, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6562.588930130005, \"count\": 1, \"min\": 6562.588930130005, \"max\": 6562.588930130005}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:28:01 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=99.50196754742889 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:28:01 INFO 140512987567936] #progress_metric: host=algo-1, completed 24.5 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:28:01 INFO 140512987567936] #quality_metric: host=algo-1, epoch=48, train loss <loss>=1.8149530670859597\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:28:01 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:28:05 INFO 140512987567936] Epoch[49] Batch[0] avg_epoch_loss=1.707592\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:28:05 INFO 140512987567936] #quality_metric: host=algo-1, epoch=49, batch=0 train loss <loss>=1.7075920104980469\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:28:06 INFO 140512987567936] Epoch[49] Batch[5] avg_epoch_loss=1.588705\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:28:06 INFO 140512987567936] #quality_metric: host=algo-1, epoch=49, batch=5 train loss <loss>=1.5887054204940796\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:28:06 INFO 140512987567936] Epoch[49] Batch [5]#011Speed: 279.53 samples/sec#011loss=1.588705\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:28:07 INFO 140512987567936] processed a total of 639 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742059681.0487707, \"EndTime\": 1742059687.5075014, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6458.304643630981, \"count\": 1, \"min\": 6458.304643630981, \"max\": 6458.304643630981}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:28:07 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=98.94061813663576 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:28:07 INFO 140512987567936] #progress_metric: host=algo-1, completed 25.0 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:28:07 INFO 140512987567936] #quality_metric: host=algo-1, epoch=49, train loss <loss>=1.4675019145011903\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:28:07 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:28:11 INFO 140512987567936] Epoch[50] Batch[0] avg_epoch_loss=1.670671\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:28:11 INFO 140512987567936] #quality_metric: host=algo-1, epoch=50, batch=0 train loss <loss>=1.6706713438034058\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:28:12 INFO 140512987567936] Epoch[50] Batch[5] avg_epoch_loss=1.495311\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:28:12 INFO 140512987567936] #quality_metric: host=algo-1, epoch=50, batch=5 train loss <loss>=1.49531094233195\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:28:12 INFO 140512987567936] Epoch[50] Batch [5]#011Speed: 307.95 samples/sec#011loss=1.495311\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:28:13 INFO 140512987567936] processed a total of 613 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742059687.5075805, \"EndTime\": 1742059693.7839642, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6275.872230529785, \"count\": 1, \"min\": 6275.872230529785, \"max\": 6275.872230529785}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:28:13 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=97.67405933036864 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:28:13 INFO 140512987567936] #progress_metric: host=algo-1, completed 25.5 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:28:13 INFO 140512987567936] #quality_metric: host=algo-1, epoch=50, train loss <loss>=1.4768255829811097\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:28:13 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:28:17 INFO 140512987567936] Epoch[51] Batch[0] avg_epoch_loss=1.353715\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:28:17 INFO 140512987567936] #quality_metric: host=algo-1, epoch=51, batch=0 train loss <loss>=1.3537145853042603\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:28:19 INFO 140512987567936] Epoch[51] Batch[5] avg_epoch_loss=1.454908\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:28:19 INFO 140512987567936] #quality_metric: host=algo-1, epoch=51, batch=5 train loss <loss>=1.4549083908398945\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:28:19 INFO 140512987567936] Epoch[51] Batch [5]#011Speed: 298.21 samples/sec#011loss=1.454908\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:28:20 INFO 140512987567936] Epoch[51] Batch[10] avg_epoch_loss=1.451404\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:28:20 INFO 140512987567936] #quality_metric: host=algo-1, epoch=51, batch=10 train loss <loss>=1.447198748588562\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:28:20 INFO 140512987567936] Epoch[51] Batch [10]#011Speed: 258.29 samples/sec#011loss=1.447199\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:28:20 INFO 140512987567936] processed a total of 660 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742059693.7840333, \"EndTime\": 1742059700.30782, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6522.649765014648, \"count\": 1, \"min\": 6522.649765014648, \"max\": 6522.649765014648}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:28:20 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=101.18419592002158 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:28:20 INFO 140512987567936] #progress_metric: host=algo-1, completed 26.0 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:28:20 INFO 140512987567936] #quality_metric: host=algo-1, epoch=51, train loss <loss>=1.4514040079983799\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:28:20 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:28:24 INFO 140512987567936] Epoch[52] Batch[0] avg_epoch_loss=1.942367\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:28:24 INFO 140512987567936] #quality_metric: host=algo-1, epoch=52, batch=0 train loss <loss>=1.9423669576644897\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:28:25 INFO 140512987567936] Epoch[52] Batch[5] avg_epoch_loss=1.471590\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:28:25 INFO 140512987567936] #quality_metric: host=algo-1, epoch=52, batch=5 train loss <loss>=1.4715903401374817\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:28:25 INFO 140512987567936] Epoch[52] Batch [5]#011Speed: 298.85 samples/sec#011loss=1.471590\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:28:26 INFO 140512987567936] processed a total of 588 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742059700.307892, \"EndTime\": 1742059706.7120478, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6403.63621711731, \"count\": 1, \"min\": 6403.63621711731, \"max\": 6403.63621711731}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:28:26 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=91.82136343020458 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:28:26 INFO 140512987567936] #progress_metric: host=algo-1, completed 26.5 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:28:26 INFO 140512987567936] #quality_metric: host=algo-1, epoch=52, train loss <loss>=1.374343717098236\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:28:26 INFO 140512987567936] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:28:26 INFO 140512987567936] Saved checkpoint to \"/opt/ml/model/state_4840888e-79cc-47a7-b66b-68695e2f6ce7-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742059706.7121146, \"EndTime\": 1742059706.7254674, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 12.907981872558594, \"count\": 1, \"min\": 12.907981872558594, \"max\": 12.907981872558594}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:28:30 INFO 140512987567936] Epoch[53] Batch[0] avg_epoch_loss=1.829528\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:28:30 INFO 140512987567936] #quality_metric: host=algo-1, epoch=53, batch=0 train loss <loss>=1.8295279741287231\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:28:32 INFO 140512987567936] Epoch[53] Batch[5] avg_epoch_loss=1.452149\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:28:32 INFO 140512987567936] #quality_metric: host=algo-1, epoch=53, batch=5 train loss <loss>=1.4521487355232239\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:28:32 INFO 140512987567936] Epoch[53] Batch [5]#011Speed: 302.18 samples/sec#011loss=1.452149\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:28:33 INFO 140512987567936] Epoch[53] Batch[10] avg_epoch_loss=1.703414\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:28:33 INFO 140512987567936] #quality_metric: host=algo-1, epoch=53, batch=10 train loss <loss>=2.0049331188201904\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:28:33 INFO 140512987567936] Epoch[53] Batch [10]#011Speed: 229.23 samples/sec#011loss=2.004933\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:28:33 INFO 140512987567936] processed a total of 641 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742059706.7255309, \"EndTime\": 1742059713.4359853, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6710.403203964233, \"count\": 1, \"min\": 6710.403203964233, \"max\": 6710.403203964233}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:28:33 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=95.5220246478229 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:28:33 INFO 140512987567936] #progress_metric: host=algo-1, completed 27.0 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:28:33 INFO 140512987567936] #quality_metric: host=algo-1, epoch=53, train loss <loss>=1.7034143642945723\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:28:33 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:28:37 INFO 140512987567936] Epoch[54] Batch[0] avg_epoch_loss=1.552851\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:28:37 INFO 140512987567936] #quality_metric: host=algo-1, epoch=54, batch=0 train loss <loss>=1.5528509616851807\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:28:38 INFO 140512987567936] Epoch[54] Batch[5] avg_epoch_loss=1.458234\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:28:38 INFO 140512987567936] #quality_metric: host=algo-1, epoch=54, batch=5 train loss <loss>=1.458233803510666\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:28:38 INFO 140512987567936] Epoch[54] Batch [5]#011Speed: 298.70 samples/sec#011loss=1.458234\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:28:39 INFO 140512987567936] Epoch[54] Batch[10] avg_epoch_loss=1.560745\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:28:39 INFO 140512987567936] #quality_metric: host=algo-1, epoch=54, batch=10 train loss <loss>=1.6837588787078857\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:28:40 INFO 140512987567936] Epoch[54] Batch [10]#011Speed: 250.38 samples/sec#011loss=1.683759\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:28:40 INFO 140512987567936] processed a total of 655 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742059713.436047, \"EndTime\": 1742059720.0022836, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6565.927267074585, \"count\": 1, \"min\": 6565.927267074585, \"max\": 6565.927267074585}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:28:40 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=99.75602726823692 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:28:40 INFO 140512987567936] #progress_metric: host=algo-1, completed 27.5 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:28:40 INFO 140512987567936] #quality_metric: host=algo-1, epoch=54, train loss <loss>=1.560745201327584\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:28:40 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:28:44 INFO 140512987567936] Epoch[55] Batch[0] avg_epoch_loss=0.910572\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:28:44 INFO 140512987567936] #quality_metric: host=algo-1, epoch=55, batch=0 train loss <loss>=0.910572350025177\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:28:45 INFO 140512987567936] Epoch[55] Batch[5] avg_epoch_loss=1.218356\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:28:45 INFO 140512987567936] #quality_metric: host=algo-1, epoch=55, batch=5 train loss <loss>=1.2183555364608765\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:28:45 INFO 140512987567936] Epoch[55] Batch [5]#011Speed: 298.32 samples/sec#011loss=1.218356\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:28:46 INFO 140512987567936] Epoch[55] Batch[10] avg_epoch_loss=1.302692\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:28:46 INFO 140512987567936] #quality_metric: host=algo-1, epoch=55, batch=10 train loss <loss>=1.4038958311080934\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:28:46 INFO 140512987567936] Epoch[55] Batch [10]#011Speed: 237.92 samples/sec#011loss=1.403896\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:28:46 INFO 140512987567936] processed a total of 663 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742059720.0023463, \"EndTime\": 1742059726.5658624, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6563.148975372314, \"count\": 1, \"min\": 6563.148975372314, \"max\": 6563.148975372314}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:28:46 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=101.01697058381524 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:28:46 INFO 140512987567936] #progress_metric: host=algo-1, completed 28.0 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:28:46 INFO 140512987567936] #quality_metric: host=algo-1, epoch=55, train loss <loss>=1.3026920340277932\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:28:46 INFO 140512987567936] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:28:46 INFO 140512987567936] Saved checkpoint to \"/opt/ml/model/state_da922cef-c769-450c-940d-a62f61b545ed-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742059726.56593, \"EndTime\": 1742059726.579549, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 13.176441192626953, \"count\": 1, \"min\": 13.176441192626953, \"max\": 13.176441192626953}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:28:50 INFO 140512987567936] Epoch[56] Batch[0] avg_epoch_loss=1.199077\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:28:50 INFO 140512987567936] #quality_metric: host=algo-1, epoch=56, batch=0 train loss <loss>=1.1990773677825928\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:28:51 INFO 140512987567936] Epoch[56] Batch[5] avg_epoch_loss=1.402287\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:28:51 INFO 140512987567936] #quality_metric: host=algo-1, epoch=56, batch=5 train loss <loss>=1.4022872646649678\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:28:51 INFO 140512987567936] Epoch[56] Batch [5]#011Speed: 309.42 samples/sec#011loss=1.402287\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:28:52 INFO 140512987567936] processed a total of 634 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742059726.579606, \"EndTime\": 1742059732.7512662, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6171.605348587036, \"count\": 1, \"min\": 6171.605348587036, \"max\": 6171.605348587036}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:28:52 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=102.7268800475871 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:28:52 INFO 140512987567936] #progress_metric: host=algo-1, completed 28.5 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:28:52 INFO 140512987567936] #quality_metric: host=algo-1, epoch=56, train loss <loss>=1.3653040409088135\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:28:52 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:28:56 INFO 140512987567936] Epoch[57] Batch[0] avg_epoch_loss=0.996965\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:28:56 INFO 140512987567936] #quality_metric: host=algo-1, epoch=57, batch=0 train loss <loss>=0.9969651699066162\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:28:58 INFO 140512987567936] Epoch[57] Batch[5] avg_epoch_loss=1.437750\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:28:58 INFO 140512987567936] #quality_metric: host=algo-1, epoch=57, batch=5 train loss <loss>=1.4377504388491313\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:28:58 INFO 140512987567936] Epoch[57] Batch [5]#011Speed: 296.12 samples/sec#011loss=1.437750\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:28:59 INFO 140512987567936] processed a total of 605 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742059732.7513344, \"EndTime\": 1742059739.0530953, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6301.444053649902, \"count\": 1, \"min\": 6301.444053649902, \"max\": 6301.444053649902}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:28:59 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=96.00851137567909 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:28:59 INFO 140512987567936] #progress_metric: host=algo-1, completed 29.0 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:28:59 INFO 140512987567936] #quality_metric: host=algo-1, epoch=57, train loss <loss>=1.4165161587297916\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:28:59 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:29:03 INFO 140512987567936] Epoch[58] Batch[0] avg_epoch_loss=1.440515\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:29:03 INFO 140512987567936] #quality_metric: host=algo-1, epoch=58, batch=0 train loss <loss>=1.440515398979187\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:29:04 INFO 140512987567936] Epoch[58] Batch[5] avg_epoch_loss=1.577774\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:29:04 INFO 140512987567936] #quality_metric: host=algo-1, epoch=58, batch=5 train loss <loss>=1.5777737100919087\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:29:04 INFO 140512987567936] Epoch[58] Batch [5]#011Speed: 312.69 samples/sec#011loss=1.577774\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:29:05 INFO 140512987567936] Epoch[58] Batch[10] avg_epoch_loss=1.357110\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:29:05 INFO 140512987567936] #quality_metric: host=algo-1, epoch=58, batch=10 train loss <loss>=1.0923126965761185\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:29:05 INFO 140512987567936] Epoch[58] Batch [10]#011Speed: 233.80 samples/sec#011loss=1.092313\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:29:05 INFO 140512987567936] processed a total of 645 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742059739.0531492, \"EndTime\": 1742059745.7134457, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6660.020112991333, \"count\": 1, \"min\": 6660.020112991333, \"max\": 6660.020112991333}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:29:05 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=96.84498039939926 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:29:05 INFO 140512987567936] #progress_metric: host=algo-1, completed 29.5 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:29:05 INFO 140512987567936] #quality_metric: host=algo-1, epoch=58, train loss <loss>=1.3571096130392768\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:29:05 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:29:09 INFO 140512987567936] Epoch[59] Batch[0] avg_epoch_loss=1.013978\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:29:09 INFO 140512987567936] #quality_metric: host=algo-1, epoch=59, batch=0 train loss <loss>=1.0139778852462769\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:29:10 INFO 140512987567936] Epoch[59] Batch[5] avg_epoch_loss=1.259742\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:29:10 INFO 140512987567936] #quality_metric: host=algo-1, epoch=59, batch=5 train loss <loss>=1.2597422202428181\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:29:10 INFO 140512987567936] Epoch[59] Batch [5]#011Speed: 297.62 samples/sec#011loss=1.259742\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:29:12 INFO 140512987567936] Epoch[59] Batch[10] avg_epoch_loss=1.272150\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:29:12 INFO 140512987567936] #quality_metric: host=algo-1, epoch=59, batch=10 train loss <loss>=1.287039613723755\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:29:12 INFO 140512987567936] Epoch[59] Batch [10]#011Speed: 206.71 samples/sec#011loss=1.287040\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:29:12 INFO 140512987567936] processed a total of 673 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742059745.7135203, \"EndTime\": 1742059752.3986487, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6684.711694717407, \"count\": 1, \"min\": 6684.711694717407, \"max\": 6684.711694717407}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:29:12 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=100.67597907906425 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:29:12 INFO 140512987567936] #progress_metric: host=algo-1, completed 30.0 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:29:12 INFO 140512987567936] #quality_metric: host=algo-1, epoch=59, train loss <loss>=1.2721501263705166\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:29:12 INFO 140512987567936] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:29:12 INFO 140512987567936] Saved checkpoint to \"/opt/ml/model/state_6f125bcd-9225-4edb-b30f-0de6fcc28b11-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742059752.3987143, \"EndTime\": 1742059752.4136283, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 14.471054077148438, \"count\": 1, \"min\": 14.471054077148438, \"max\": 14.471054077148438}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:29:16 INFO 140512987567936] Epoch[60] Batch[0] avg_epoch_loss=1.049366\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:29:16 INFO 140512987567936] #quality_metric: host=algo-1, epoch=60, batch=0 train loss <loss>=1.0493663549423218\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:29:17 INFO 140512987567936] Epoch[60] Batch[5] avg_epoch_loss=1.428220\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:29:17 INFO 140512987567936] #quality_metric: host=algo-1, epoch=60, batch=5 train loss <loss>=1.4282200336456299\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:29:17 INFO 140512987567936] Epoch[60] Batch [5]#011Speed: 300.28 samples/sec#011loss=1.428220\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:29:18 INFO 140512987567936] Epoch[60] Batch[10] avg_epoch_loss=1.447919\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:29:18 INFO 140512987567936] #quality_metric: host=algo-1, epoch=60, batch=10 train loss <loss>=1.4715576887130737\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:29:18 INFO 140512987567936] Epoch[60] Batch [10]#011Speed: 235.21 samples/sec#011loss=1.471558\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:29:18 INFO 140512987567936] processed a total of 659 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742059752.413689, \"EndTime\": 1742059758.8947783, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6481.036424636841, \"count\": 1, \"min\": 6481.036424636841, \"max\": 6481.036424636841}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:29:18 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=101.67972666416739 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:29:18 INFO 140512987567936] #progress_metric: host=algo-1, completed 30.5 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:29:18 INFO 140512987567936] #quality_metric: host=algo-1, epoch=60, train loss <loss>=1.4479189677671953\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:29:18 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:29:23 INFO 140512987567936] Epoch[61] Batch[0] avg_epoch_loss=1.527816\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:29:23 INFO 140512987567936] #quality_metric: host=algo-1, epoch=61, batch=0 train loss <loss>=1.527815818786621\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:29:24 INFO 140512987567936] Epoch[61] Batch[5] avg_epoch_loss=1.532587\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:29:24 INFO 140512987567936] #quality_metric: host=algo-1, epoch=61, batch=5 train loss <loss>=1.5325870315233867\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:29:24 INFO 140512987567936] Epoch[61] Batch [5]#011Speed: 290.20 samples/sec#011loss=1.532587\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:29:25 INFO 140512987567936] Epoch[61] Batch[10] avg_epoch_loss=1.374138\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:29:25 INFO 140512987567936] #quality_metric: host=algo-1, epoch=61, batch=10 train loss <loss>=1.183998280763626\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:29:25 INFO 140512987567936] Epoch[61] Batch [10]#011Speed: 243.37 samples/sec#011loss=1.183998\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:29:25 INFO 140512987567936] processed a total of 661 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742059758.8948462, \"EndTime\": 1742059765.4923182, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6596.995830535889, \"count\": 1, \"min\": 6596.995830535889, \"max\": 6596.995830535889}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:29:25 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=100.19571398033155 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:29:25 INFO 140512987567936] #progress_metric: host=algo-1, completed 31.0 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:29:25 INFO 140512987567936] #quality_metric: host=algo-1, epoch=61, train loss <loss>=1.374137599359859\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:29:25 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:29:29 INFO 140512987567936] Epoch[62] Batch[0] avg_epoch_loss=1.423186\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:29:29 INFO 140512987567936] #quality_metric: host=algo-1, epoch=62, batch=0 train loss <loss>=1.4231857061386108\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:29:30 INFO 140512987567936] Epoch[62] Batch[5] avg_epoch_loss=1.586853\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:29:30 INFO 140512987567936] #quality_metric: host=algo-1, epoch=62, batch=5 train loss <loss>=1.5868525107701619\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:29:30 INFO 140512987567936] Epoch[62] Batch [5]#011Speed: 293.95 samples/sec#011loss=1.586853\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:29:31 INFO 140512987567936] processed a total of 633 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742059765.4923797, \"EndTime\": 1742059771.8931057, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6400.387287139893, \"count\": 1, \"min\": 6400.387287139893, \"max\": 6400.387287139893}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:29:31 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=98.89785584528902 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:29:31 INFO 140512987567936] #progress_metric: host=algo-1, completed 31.5 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:29:31 INFO 140512987567936] #quality_metric: host=algo-1, epoch=62, train loss <loss>=1.4762425303459168\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:29:31 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:29:36 INFO 140512987567936] Epoch[63] Batch[0] avg_epoch_loss=1.562400\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:29:36 INFO 140512987567936] #quality_metric: host=algo-1, epoch=63, batch=0 train loss <loss>=1.5623998641967773\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:29:37 INFO 140512987567936] Epoch[63] Batch[5] avg_epoch_loss=1.484429\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:29:37 INFO 140512987567936] #quality_metric: host=algo-1, epoch=63, batch=5 train loss <loss>=1.4844285051027934\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:29:37 INFO 140512987567936] Epoch[63] Batch [5]#011Speed: 299.34 samples/sec#011loss=1.484429\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:29:38 INFO 140512987567936] processed a total of 640 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742059771.893196, \"EndTime\": 1742059778.2479541, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6353.6248207092285, \"count\": 1, \"min\": 6353.6248207092285, \"max\": 6353.6248207092285}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:29:38 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=100.72834762600122 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:29:38 INFO 140512987567936] #progress_metric: host=algo-1, completed 32.0 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:29:38 INFO 140512987567936] #quality_metric: host=algo-1, epoch=63, train loss <loss>=1.4678633213043213\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:29:38 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:29:42 INFO 140512987567936] Epoch[64] Batch[0] avg_epoch_loss=1.090702\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:29:42 INFO 140512987567936] #quality_metric: host=algo-1, epoch=64, batch=0 train loss <loss>=1.090701699256897\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:29:43 INFO 140512987567936] Epoch[64] Batch[5] avg_epoch_loss=1.236342\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:29:43 INFO 140512987567936] #quality_metric: host=algo-1, epoch=64, batch=5 train loss <loss>=1.2363422811031342\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:29:43 INFO 140512987567936] Epoch[64] Batch [5]#011Speed: 306.81 samples/sec#011loss=1.236342\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:29:44 INFO 140512987567936] processed a total of 603 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742059778.2480185, \"EndTime\": 1742059784.5052533, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6256.80136680603, \"count\": 1, \"min\": 6256.80136680603, \"max\": 6256.80136680603}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:29:44 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=96.37341514389604 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:29:44 INFO 140512987567936] #progress_metric: host=algo-1, completed 32.5 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:29:44 INFO 140512987567936] #quality_metric: host=algo-1, epoch=64, train loss <loss>=1.233860856294632\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:29:44 INFO 140512987567936] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:29:44 INFO 140512987567936] Saved checkpoint to \"/opt/ml/model/state_bb9b32cc-4d8e-409c-a990-912d2c3e5fee-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742059784.5053294, \"EndTime\": 1742059784.5191689, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 13.330936431884766, \"count\": 1, \"min\": 13.330936431884766, \"max\": 13.330936431884766}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:29:48 INFO 140512987567936] Epoch[65] Batch[0] avg_epoch_loss=1.332875\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:29:48 INFO 140512987567936] #quality_metric: host=algo-1, epoch=65, batch=0 train loss <loss>=1.3328746557235718\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:29:49 INFO 140512987567936] Epoch[65] Batch[5] avg_epoch_loss=1.535596\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:29:49 INFO 140512987567936] #quality_metric: host=algo-1, epoch=65, batch=5 train loss <loss>=1.5355955163637798\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:29:49 INFO 140512987567936] Epoch[65] Batch [5]#011Speed: 317.20 samples/sec#011loss=1.535596\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:29:51 INFO 140512987567936] Epoch[65] Batch[10] avg_epoch_loss=1.270147\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:29:51 INFO 140512987567936] #quality_metric: host=algo-1, epoch=65, batch=10 train loss <loss>=0.9516077160835266\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:29:51 INFO 140512987567936] Epoch[65] Batch [10]#011Speed: 234.46 samples/sec#011loss=0.951608\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:29:51 INFO 140512987567936] processed a total of 653 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742059784.5192435, \"EndTime\": 1742059791.0106184, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6491.316795349121, \"count\": 1, \"min\": 6491.316795349121, \"max\": 6491.316795349121}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:29:51 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=100.5944074708875 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:29:51 INFO 140512987567936] #progress_metric: host=algo-1, completed 33.0 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:29:51 INFO 140512987567936] #quality_metric: host=algo-1, epoch=65, train loss <loss>=1.2701465162363919\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:29:51 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:29:55 INFO 140512987567936] Epoch[66] Batch[0] avg_epoch_loss=1.309129\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:29:55 INFO 140512987567936] #quality_metric: host=algo-1, epoch=66, batch=0 train loss <loss>=1.309128999710083\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:29:56 INFO 140512987567936] Epoch[66] Batch[5] avg_epoch_loss=1.235888\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:29:56 INFO 140512987567936] #quality_metric: host=algo-1, epoch=66, batch=5 train loss <loss>=1.2358877857526143\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:29:56 INFO 140512987567936] Epoch[66] Batch [5]#011Speed: 291.68 samples/sec#011loss=1.235888\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:29:57 INFO 140512987567936] processed a total of 628 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742059791.010684, \"EndTime\": 1742059797.3428845, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6331.91704750061, \"count\": 1, \"min\": 6331.91704750061, \"max\": 6331.91704750061}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:29:57 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=99.17854641361221 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:29:57 INFO 140512987567936] #progress_metric: host=algo-1, completed 33.5 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:29:57 INFO 140512987567936] #quality_metric: host=algo-1, epoch=66, train loss <loss>=1.312814676761627\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:29:57 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:30:01 INFO 140512987567936] Epoch[67] Batch[0] avg_epoch_loss=1.863118\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:30:01 INFO 140512987567936] #quality_metric: host=algo-1, epoch=67, batch=0 train loss <loss>=1.8631176948547363\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:30:02 INFO 140512987567936] Epoch[67] Batch[5] avg_epoch_loss=1.376700\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:30:02 INFO 140512987567936] #quality_metric: host=algo-1, epoch=67, batch=5 train loss <loss>=1.3767000039418538\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:30:02 INFO 140512987567936] Epoch[67] Batch [5]#011Speed: 293.67 samples/sec#011loss=1.376700\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:30:04 INFO 140512987567936] Epoch[67] Batch[10] avg_epoch_loss=1.215779\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:30:04 INFO 140512987567936] #quality_metric: host=algo-1, epoch=67, batch=10 train loss <loss>=1.0226733148097993\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:30:04 INFO 140512987567936] Epoch[67] Batch [10]#011Speed: 210.17 samples/sec#011loss=1.022673\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:30:04 INFO 140512987567936] processed a total of 667 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742059797.3429515, \"EndTime\": 1742059804.1904252, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6846.977710723877, \"count\": 1, \"min\": 6846.977710723877, \"max\": 6846.977710723877}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:30:04 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=97.41357108979383 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:30:04 INFO 140512987567936] #progress_metric: host=algo-1, completed 34.0 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:30:04 INFO 140512987567936] #quality_metric: host=algo-1, epoch=67, train loss <loss>=1.2157787816091017\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:30:04 INFO 140512987567936] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:30:04 INFO 140512987567936] Saved checkpoint to \"/opt/ml/model/state_aa746652-eb3c-473a-bc49-c9db7200a14a-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742059804.1905105, \"EndTime\": 1742059804.2047908, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 13.992547988891602, \"count\": 1, \"min\": 13.992547988891602, \"max\": 13.992547988891602}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:30:08 INFO 140512987567936] Epoch[68] Batch[0] avg_epoch_loss=1.664002\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:30:08 INFO 140512987567936] #quality_metric: host=algo-1, epoch=68, batch=0 train loss <loss>=1.6640019416809082\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:30:09 INFO 140512987567936] Epoch[68] Batch[5] avg_epoch_loss=1.417977\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:30:09 INFO 140512987567936] #quality_metric: host=algo-1, epoch=68, batch=5 train loss <loss>=1.4179773728052776\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:30:09 INFO 140512987567936] Epoch[68] Batch [5]#011Speed: 282.67 samples/sec#011loss=1.417977\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:30:10 INFO 140512987567936] Epoch[68] Batch[10] avg_epoch_loss=1.652188\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:30:10 INFO 140512987567936] #quality_metric: host=algo-1, epoch=68, batch=10 train loss <loss>=1.9332407236099243\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:30:10 INFO 140512987567936] Epoch[68] Batch [10]#011Speed: 253.33 samples/sec#011loss=1.933241\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:30:10 INFO 140512987567936] processed a total of 646 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742059804.2048564, \"EndTime\": 1742059810.820619, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6615.703344345093, \"count\": 1, \"min\": 6615.703344345093, \"max\": 6615.703344345093}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:30:10 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=97.64493096164307 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:30:10 INFO 140512987567936] #progress_metric: host=algo-1, completed 34.5 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:30:10 INFO 140512987567936] #quality_metric: host=algo-1, epoch=68, train loss <loss>=1.6521879868073897\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:30:10 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:30:15 INFO 140512987567936] Epoch[69] Batch[0] avg_epoch_loss=1.414874\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:30:15 INFO 140512987567936] #quality_metric: host=algo-1, epoch=69, batch=0 train loss <loss>=1.4148736000061035\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:30:16 INFO 140512987567936] Epoch[69] Batch[5] avg_epoch_loss=1.402675\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:30:16 INFO 140512987567936] #quality_metric: host=algo-1, epoch=69, batch=5 train loss <loss>=1.4026745160420735\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:30:16 INFO 140512987567936] Epoch[69] Batch [5]#011Speed: 284.80 samples/sec#011loss=1.402675\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:30:17 INFO 140512987567936] processed a total of 639 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742059810.8206875, \"EndTime\": 1742059817.189924, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6368.796348571777, \"count\": 1, \"min\": 6368.796348571777, \"max\": 6368.796348571777}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:30:17 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=100.33137907229795 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:30:17 INFO 140512987567936] #progress_metric: host=algo-1, completed 35.0 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:30:17 INFO 140512987567936] #quality_metric: host=algo-1, epoch=69, train loss <loss>=1.3779226660728454\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:30:17 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:30:21 INFO 140512987567936] Epoch[70] Batch[0] avg_epoch_loss=1.228600\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:30:21 INFO 140512987567936] #quality_metric: host=algo-1, epoch=70, batch=0 train loss <loss>=1.2286003828048706\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:30:22 INFO 140512987567936] Epoch[70] Batch[5] avg_epoch_loss=1.319257\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:30:22 INFO 140512987567936] #quality_metric: host=algo-1, epoch=70, batch=5 train loss <loss>=1.3192568520704906\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:30:22 INFO 140512987567936] Epoch[70] Batch [5]#011Speed: 294.85 samples/sec#011loss=1.319257\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:30:23 INFO 140512987567936] Epoch[70] Batch[10] avg_epoch_loss=1.173277\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:30:23 INFO 140512987567936] #quality_metric: host=algo-1, epoch=70, batch=10 train loss <loss>=0.998100197315216\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:30:23 INFO 140512987567936] Epoch[70] Batch [10]#011Speed: 223.42 samples/sec#011loss=0.998100\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:30:23 INFO 140512987567936] processed a total of 646 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742059817.1899924, \"EndTime\": 1742059823.897913, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6707.468032836914, \"count\": 1, \"min\": 6707.468032836914, \"max\": 6707.468032836914}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:30:23 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=96.308676815109 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:30:23 INFO 140512987567936] #progress_metric: host=algo-1, completed 35.5 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:30:23 INFO 140512987567936] #quality_metric: host=algo-1, epoch=70, train loss <loss>=1.1732765544544568\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:30:23 INFO 140512987567936] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:30:23 INFO 140512987567936] Saved checkpoint to \"/opt/ml/model/state_1beac135-72ae-47d5-bd53-8ebe3c51a41a-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742059823.8980088, \"EndTime\": 1742059823.9111004, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 12.627840042114258, \"count\": 1, \"min\": 12.627840042114258, \"max\": 12.627840042114258}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:30:28 INFO 140512987567936] Epoch[71] Batch[0] avg_epoch_loss=1.163297\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:30:28 INFO 140512987567936] #quality_metric: host=algo-1, epoch=71, batch=0 train loss <loss>=1.1632970571517944\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:30:29 INFO 140512987567936] Epoch[71] Batch[5] avg_epoch_loss=1.291900\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:30:29 INFO 140512987567936] #quality_metric: host=algo-1, epoch=71, batch=5 train loss <loss>=1.2919004162152607\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:30:29 INFO 140512987567936] Epoch[71] Batch [5]#011Speed: 293.48 samples/sec#011loss=1.291900\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:30:30 INFO 140512987567936] Epoch[71] Batch[10] avg_epoch_loss=1.238666\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:30:30 INFO 140512987567936] #quality_metric: host=algo-1, epoch=71, batch=10 train loss <loss>=1.1747836589813232\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:30:30 INFO 140512987567936] Epoch[71] Batch [10]#011Speed: 239.90 samples/sec#011loss=1.174784\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:30:30 INFO 140512987567936] processed a total of 658 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742059823.9111865, \"EndTime\": 1742059830.463809, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6552.567005157471, \"count\": 1, \"min\": 6552.567005157471, \"max\": 6552.567005157471}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:30:30 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=100.41721339372565 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:30:30 INFO 140512987567936] #progress_metric: host=algo-1, completed 36.0 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:30:30 INFO 140512987567936] #quality_metric: host=algo-1, epoch=71, train loss <loss>=1.238665526563471\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:30:30 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:30:34 INFO 140512987567936] Epoch[72] Batch[0] avg_epoch_loss=1.434095\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:30:34 INFO 140512987567936] #quality_metric: host=algo-1, epoch=72, batch=0 train loss <loss>=1.4340951442718506\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:30:35 INFO 140512987567936] Epoch[72] Batch[5] avg_epoch_loss=1.190368\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:30:35 INFO 140512987567936] #quality_metric: host=algo-1, epoch=72, batch=5 train loss <loss>=1.190367599328359\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:30:35 INFO 140512987567936] Epoch[72] Batch [5]#011Speed: 301.00 samples/sec#011loss=1.190368\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:30:36 INFO 140512987567936] processed a total of 622 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742059830.4638722, \"EndTime\": 1742059836.7431052, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6278.894901275635, \"count\": 1, \"min\": 6278.894901275635, \"max\": 6278.894901275635}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:30:36 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=99.05988530370847 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:30:36 INFO 140512987567936] #progress_metric: host=algo-1, completed 36.5 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:30:36 INFO 140512987567936] #quality_metric: host=algo-1, epoch=72, train loss <loss>=1.338370156288147\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:30:36 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:30:40 INFO 140512987567936] Epoch[73] Batch[0] avg_epoch_loss=1.351667\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:30:40 INFO 140512987567936] #quality_metric: host=algo-1, epoch=73, batch=0 train loss <loss>=1.3516674041748047\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:30:42 INFO 140512987567936] Epoch[73] Batch[5] avg_epoch_loss=1.476600\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:30:42 INFO 140512987567936] #quality_metric: host=algo-1, epoch=73, batch=5 train loss <loss>=1.4765999217828114\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:30:42 INFO 140512987567936] Epoch[73] Batch [5]#011Speed: 299.34 samples/sec#011loss=1.476600\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:30:43 INFO 140512987567936] processed a total of 637 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742059836.7432091, \"EndTime\": 1742059843.0206604, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6276.728630065918, \"count\": 1, \"min\": 6276.728630065918, \"max\": 6276.728630065918}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:30:43 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=101.48439107313799 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:30:43 INFO 140512987567936] #progress_metric: host=algo-1, completed 37.0 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:30:43 INFO 140512987567936] #quality_metric: host=algo-1, epoch=73, train loss <loss>=1.3885260820388794\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:30:43 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:30:47 INFO 140512987567936] Epoch[74] Batch[0] avg_epoch_loss=0.789949\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:30:47 INFO 140512987567936] #quality_metric: host=algo-1, epoch=74, batch=0 train loss <loss>=0.789948582649231\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:30:48 INFO 140512987567936] Epoch[74] Batch[5] avg_epoch_loss=1.176257\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:30:48 INFO 140512987567936] #quality_metric: host=algo-1, epoch=74, batch=5 train loss <loss>=1.1762566367785137\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:30:48 INFO 140512987567936] Epoch[74] Batch [5]#011Speed: 301.30 samples/sec#011loss=1.176257\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:30:49 INFO 140512987567936] Epoch[74] Batch[10] avg_epoch_loss=1.528380\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:30:49 INFO 140512987567936] #quality_metric: host=algo-1, epoch=74, batch=10 train loss <loss>=1.9509280920028687\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:30:49 INFO 140512987567936] Epoch[74] Batch [10]#011Speed: 230.14 samples/sec#011loss=1.950928\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:30:49 INFO 140512987567936] processed a total of 663 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742059843.0207283, \"EndTime\": 1742059849.5994496, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6578.408479690552, \"count\": 1, \"min\": 6578.408479690552, \"max\": 6578.408479690552}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:30:49 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=100.78284543210732 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:30:49 INFO 140512987567936] #progress_metric: host=algo-1, completed 37.5 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:30:49 INFO 140512987567936] #quality_metric: host=algo-1, epoch=74, train loss <loss>=1.5283800255168567\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:30:49 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:30:53 INFO 140512987567936] Epoch[75] Batch[0] avg_epoch_loss=0.782930\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:30:53 INFO 140512987567936] #quality_metric: host=algo-1, epoch=75, batch=0 train loss <loss>=0.782929539680481\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:30:54 INFO 140512987567936] Epoch[75] Batch[5] avg_epoch_loss=1.345205\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:30:54 INFO 140512987567936] #quality_metric: host=algo-1, epoch=75, batch=5 train loss <loss>=1.3452047109603882\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:30:54 INFO 140512987567936] Epoch[75] Batch [5]#011Speed: 307.78 samples/sec#011loss=1.345205\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:30:55 INFO 140512987567936] processed a total of 624 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742059849.5995138, \"EndTime\": 1742059855.9068093, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6306.99610710144, \"count\": 1, \"min\": 6306.99610710144, \"max\": 6306.99610710144}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:30:55 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=98.93596163021027 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:30:55 INFO 140512987567936] #progress_metric: host=algo-1, completed 38.0 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:30:55 INFO 140512987567936] #quality_metric: host=algo-1, epoch=75, train loss <loss>=1.2216882407665253\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:30:55 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:31:00 INFO 140512987567936] Epoch[76] Batch[0] avg_epoch_loss=1.292646\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:31:00 INFO 140512987567936] #quality_metric: host=algo-1, epoch=76, batch=0 train loss <loss>=1.2926456928253174\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:31:01 INFO 140512987567936] Epoch[76] Batch[5] avg_epoch_loss=1.236858\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:31:01 INFO 140512987567936] #quality_metric: host=algo-1, epoch=76, batch=5 train loss <loss>=1.236857533454895\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:31:01 INFO 140512987567936] Epoch[76] Batch [5]#011Speed: 293.07 samples/sec#011loss=1.236858\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:31:02 INFO 140512987567936] Epoch[76] Batch[10] avg_epoch_loss=1.144715\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:31:02 INFO 140512987567936] #quality_metric: host=algo-1, epoch=76, batch=10 train loss <loss>=1.0341437816619874\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:31:02 INFO 140512987567936] Epoch[76] Batch [10]#011Speed: 202.81 samples/sec#011loss=1.034144\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:31:02 INFO 140512987567936] processed a total of 669 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742059855.906888, \"EndTime\": 1742059862.7268426, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6819.369554519653, \"count\": 1, \"min\": 6819.369554519653, \"max\": 6819.369554519653}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:31:02 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=98.10132576787215 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:31:02 INFO 140512987567936] #progress_metric: host=algo-1, completed 38.5 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:31:02 INFO 140512987567936] #quality_metric: host=algo-1, epoch=76, train loss <loss>=1.1447149190035733\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:31:02 INFO 140512987567936] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:31:02 INFO 140512987567936] Saved checkpoint to \"/opt/ml/model/state_e7bb40f1-c9d1-4701-9410-acd80ea65d9f-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742059862.726916, \"EndTime\": 1742059862.7401695, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 12.579679489135742, \"count\": 1, \"min\": 12.579679489135742, \"max\": 12.579679489135742}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:31:06 INFO 140512987567936] Epoch[77] Batch[0] avg_epoch_loss=1.344597\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:31:06 INFO 140512987567936] #quality_metric: host=algo-1, epoch=77, batch=0 train loss <loss>=1.3445974588394165\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:31:07 INFO 140512987567936] Epoch[77] Batch[5] avg_epoch_loss=1.229239\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:31:07 INFO 140512987567936] #quality_metric: host=algo-1, epoch=77, batch=5 train loss <loss>=1.2292393147945404\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:31:07 INFO 140512987567936] Epoch[77] Batch [5]#011Speed: 319.12 samples/sec#011loss=1.229239\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:31:08 INFO 140512987567936] processed a total of 636 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742059862.740241, \"EndTime\": 1742059868.8870502, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6146.751403808594, \"count\": 1, \"min\": 6146.751403808594, \"max\": 6146.751403808594}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:31:08 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=103.46758779304817 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:31:08 INFO 140512987567936] #progress_metric: host=algo-1, completed 39.0 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:31:08 INFO 140512987567936] #quality_metric: host=algo-1, epoch=77, train loss <loss>=1.3092005610466004\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:31:08 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:31:13 INFO 140512987567936] Epoch[78] Batch[0] avg_epoch_loss=1.469962\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:31:13 INFO 140512987567936] #quality_metric: host=algo-1, epoch=78, batch=0 train loss <loss>=1.469962477684021\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:31:14 INFO 140512987567936] Epoch[78] Batch[5] avg_epoch_loss=1.330711\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:31:14 INFO 140512987567936] #quality_metric: host=algo-1, epoch=78, batch=5 train loss <loss>=1.3307106097539265\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:31:14 INFO 140512987567936] Epoch[78] Batch [5]#011Speed: 308.78 samples/sec#011loss=1.330711\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:31:15 INFO 140512987567936] processed a total of 616 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742059868.887119, \"EndTime\": 1742059875.1307034, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6243.14022064209, \"count\": 1, \"min\": 6243.14022064209, \"max\": 6243.14022064209}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:31:15 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=98.66669721728194 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:31:15 INFO 140512987567936] #progress_metric: host=algo-1, completed 39.5 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:31:15 INFO 140512987567936] #quality_metric: host=algo-1, epoch=78, train loss <loss>=1.422277283668518\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:31:15 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:31:19 INFO 140512987567936] Epoch[79] Batch[0] avg_epoch_loss=1.670985\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:31:19 INFO 140512987567936] #quality_metric: host=algo-1, epoch=79, batch=0 train loss <loss>=1.670985221862793\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:31:20 INFO 140512987567936] Epoch[79] Batch[5] avg_epoch_loss=1.306381\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:31:20 INFO 140512987567936] #quality_metric: host=algo-1, epoch=79, batch=5 train loss <loss>=1.3063806593418121\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:31:20 INFO 140512987567936] Epoch[79] Batch [5]#011Speed: 317.17 samples/sec#011loss=1.306381\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:31:21 INFO 140512987567936] processed a total of 617 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742059875.130773, \"EndTime\": 1742059881.2494774, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6118.3922290802, \"count\": 1, \"min\": 6118.3922290802, \"max\": 6118.3922290802}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:31:21 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=100.84182361876053 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:31:21 INFO 140512987567936] #progress_metric: host=algo-1, completed 40.0 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:31:21 INFO 140512987567936] #quality_metric: host=algo-1, epoch=79, train loss <loss>=1.3210074484348298\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:31:21 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:31:25 INFO 140512987567936] Epoch[80] Batch[0] avg_epoch_loss=0.808507\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:31:25 INFO 140512987567936] #quality_metric: host=algo-1, epoch=80, batch=0 train loss <loss>=0.808506965637207\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:31:26 INFO 140512987567936] Epoch[80] Batch[5] avg_epoch_loss=1.112789\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:31:26 INFO 140512987567936] #quality_metric: host=algo-1, epoch=80, batch=5 train loss <loss>=1.1127891540527344\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:31:26 INFO 140512987567936] Epoch[80] Batch [5]#011Speed: 294.21 samples/sec#011loss=1.112789\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:31:27 INFO 140512987567936] processed a total of 623 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742059881.2495475, \"EndTime\": 1742059887.599311, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6348.996877670288, \"count\": 1, \"min\": 6348.996877670288, \"max\": 6348.996877670288}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:31:27 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=98.1240718248071 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:31:27 INFO 140512987567936] #progress_metric: host=algo-1, completed 40.5 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:31:27 INFO 140512987567936] #quality_metric: host=algo-1, epoch=80, train loss <loss>=1.1969298541545867\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:31:27 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:31:31 INFO 140512987567936] Epoch[81] Batch[0] avg_epoch_loss=1.911723\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:31:31 INFO 140512987567936] #quality_metric: host=algo-1, epoch=81, batch=0 train loss <loss>=1.9117234945297241\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:31:32 INFO 140512987567936] Epoch[81] Batch[5] avg_epoch_loss=1.434627\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:31:32 INFO 140512987567936] #quality_metric: host=algo-1, epoch=81, batch=5 train loss <loss>=1.4346274137496948\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:31:32 INFO 140512987567936] Epoch[81] Batch [5]#011Speed: 307.30 samples/sec#011loss=1.434627\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:31:34 INFO 140512987567936] Epoch[81] Batch[10] avg_epoch_loss=1.168061\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:31:34 INFO 140512987567936] #quality_metric: host=algo-1, epoch=81, batch=10 train loss <loss>=0.8481810092926025\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:31:34 INFO 140512987567936] Epoch[81] Batch [10]#011Speed: 242.83 samples/sec#011loss=0.848181\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:31:34 INFO 140512987567936] processed a total of 652 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742059887.5993834, \"EndTime\": 1742059894.0647476, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6464.923858642578, \"count\": 1, \"min\": 6464.923858642578, \"max\": 6464.923858642578}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:31:34 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=100.85041621335625 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:31:34 INFO 140512987567936] #progress_metric: host=algo-1, completed 41.0 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:31:34 INFO 140512987567936] #quality_metric: host=algo-1, epoch=81, train loss <loss>=1.1680608662691983\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:31:34 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:31:38 INFO 140512987567936] Epoch[82] Batch[0] avg_epoch_loss=1.537979\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:31:38 INFO 140512987567936] #quality_metric: host=algo-1, epoch=82, batch=0 train loss <loss>=1.5379791259765625\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:31:39 INFO 140512987567936] Epoch[82] Batch[5] avg_epoch_loss=1.406910\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:31:39 INFO 140512987567936] #quality_metric: host=algo-1, epoch=82, batch=5 train loss <loss>=1.4069097638130188\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:31:39 INFO 140512987567936] Epoch[82] Batch [5]#011Speed: 297.27 samples/sec#011loss=1.406910\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:31:40 INFO 140512987567936] processed a total of 611 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742059894.0648122, \"EndTime\": 1742059900.3310957, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6265.857934951782, \"count\": 1, \"min\": 6265.857934951782, \"max\": 6265.857934951782}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:31:40 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=97.51056587159532 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:31:40 INFO 140512987567936] #progress_metric: host=algo-1, completed 41.5 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:31:40 INFO 140512987567936] #quality_metric: host=algo-1, epoch=82, train loss <loss>=1.5285017132759093\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:31:40 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:31:44 INFO 140512987567936] Epoch[83] Batch[0] avg_epoch_loss=1.426836\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:31:44 INFO 140512987567936] #quality_metric: host=algo-1, epoch=83, batch=0 train loss <loss>=1.4268361330032349\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:31:45 INFO 140512987567936] Epoch[83] Batch[5] avg_epoch_loss=1.527900\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:31:45 INFO 140512987567936] #quality_metric: host=algo-1, epoch=83, batch=5 train loss <loss>=1.5278997818628948\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:31:45 INFO 140512987567936] Epoch[83] Batch [5]#011Speed: 312.37 samples/sec#011loss=1.527900\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:31:46 INFO 140512987567936] Epoch[83] Batch[10] avg_epoch_loss=1.508128\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:31:46 INFO 140512987567936] #quality_metric: host=algo-1, epoch=83, batch=10 train loss <loss>=1.4844011783599853\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:31:46 INFO 140512987567936] Epoch[83] Batch [10]#011Speed: 218.86 samples/sec#011loss=1.484401\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:31:46 INFO 140512987567936] processed a total of 679 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742059900.3311927, \"EndTime\": 1742059906.8301475, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6498.64649772644, \"count\": 1, \"min\": 6498.64649772644, \"max\": 6498.64649772644}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:31:46 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=104.48172737047443 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:31:46 INFO 140512987567936] #progress_metric: host=algo-1, completed 42.0 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:31:46 INFO 140512987567936] #quality_metric: host=algo-1, epoch=83, train loss <loss>=1.5081276893615723\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:31:46 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:31:50 INFO 140512987567936] Epoch[84] Batch[0] avg_epoch_loss=1.304329\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:31:50 INFO 140512987567936] #quality_metric: host=algo-1, epoch=84, batch=0 train loss <loss>=1.3043290376663208\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:31:52 INFO 140512987567936] Epoch[84] Batch[5] avg_epoch_loss=1.492886\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:31:52 INFO 140512987567936] #quality_metric: host=algo-1, epoch=84, batch=5 train loss <loss>=1.492886225382487\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:31:52 INFO 140512987567936] Epoch[84] Batch [5]#011Speed: 291.58 samples/sec#011loss=1.492886\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:31:53 INFO 140512987567936] Epoch[84] Batch[10] avg_epoch_loss=1.502338\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:31:53 INFO 140512987567936] #quality_metric: host=algo-1, epoch=84, batch=10 train loss <loss>=1.513679575920105\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:31:53 INFO 140512987567936] Epoch[84] Batch [10]#011Speed: 205.50 samples/sec#011loss=1.513680\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:31:53 INFO 140512987567936] processed a total of 668 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742059906.8302112, \"EndTime\": 1742059913.5774639, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6746.844291687012, \"count\": 1, \"min\": 6746.844291687012, \"max\": 6746.844291687012}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:31:53 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=99.00783053029744 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:31:53 INFO 140512987567936] #progress_metric: host=algo-1, completed 42.5 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:31:53 INFO 140512987567936] #quality_metric: host=algo-1, epoch=84, train loss <loss>=1.5023377483541316\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:31:53 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:31:57 INFO 140512987567936] Epoch[85] Batch[0] avg_epoch_loss=0.840908\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:31:57 INFO 140512987567936] #quality_metric: host=algo-1, epoch=85, batch=0 train loss <loss>=0.8409081697463989\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:31:58 INFO 140512987567936] Epoch[85] Batch[5] avg_epoch_loss=1.215810\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:31:58 INFO 140512987567936] #quality_metric: host=algo-1, epoch=85, batch=5 train loss <loss>=1.215809981028239\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:31:58 INFO 140512987567936] Epoch[85] Batch [5]#011Speed: 285.22 samples/sec#011loss=1.215810\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:31:59 INFO 140512987567936] processed a total of 636 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742059913.5775301, \"EndTime\": 1742059919.953885, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6376.012563705444, \"count\": 1, \"min\": 6376.012563705444, \"max\": 6376.012563705444}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:31:59 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=99.74731079885987 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:31:59 INFO 140512987567936] #progress_metric: host=algo-1, completed 43.0 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:31:59 INFO 140512987567936] #quality_metric: host=algo-1, epoch=85, train loss <loss>=1.2257359504699707\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:31:59 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:32:04 INFO 140512987567936] Epoch[86] Batch[0] avg_epoch_loss=0.818563\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:32:04 INFO 140512987567936] #quality_metric: host=algo-1, epoch=86, batch=0 train loss <loss>=0.8185634613037109\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:32:05 INFO 140512987567936] Epoch[86] Batch[5] avg_epoch_loss=1.177168\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:32:05 INFO 140512987567936] #quality_metric: host=algo-1, epoch=86, batch=5 train loss <loss>=1.17716779311498\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:32:05 INFO 140512987567936] Epoch[86] Batch [5]#011Speed: 284.45 samples/sec#011loss=1.177168\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:32:06 INFO 140512987567936] processed a total of 630 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742059919.9539528, \"EndTime\": 1742059926.4276054, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6473.344087600708, \"count\": 1, \"min\": 6473.344087600708, \"max\": 6473.344087600708}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:32:06 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=97.32031177720906 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:32:06 INFO 140512987567936] #progress_metric: host=algo-1, completed 43.5 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:32:06 INFO 140512987567936] #quality_metric: host=algo-1, epoch=86, train loss <loss>=1.1232259094715118\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:32:06 INFO 140512987567936] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:32:06 INFO 140512987567936] Saved checkpoint to \"/opt/ml/model/state_41e28703-4192-4068-aa6e-c6a72036936f-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742059926.4276967, \"EndTime\": 1742059926.441616, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 13.098001480102539, \"count\": 1, \"min\": 13.098001480102539, \"max\": 13.098001480102539}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:32:10 INFO 140512987567936] Epoch[87] Batch[0] avg_epoch_loss=1.557362\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:32:10 INFO 140512987567936] #quality_metric: host=algo-1, epoch=87, batch=0 train loss <loss>=1.5573619604110718\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:32:11 INFO 140512987567936] Epoch[87] Batch[5] avg_epoch_loss=1.229108\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:32:11 INFO 140512987567936] #quality_metric: host=algo-1, epoch=87, batch=5 train loss <loss>=1.2291082839171092\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:32:11 INFO 140512987567936] Epoch[87] Batch [5]#011Speed: 290.52 samples/sec#011loss=1.229108\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:32:12 INFO 140512987567936] processed a total of 616 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742059926.4416757, \"EndTime\": 1742059932.7497323, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6308.003187179565, \"count\": 1, \"min\": 6308.003187179565, \"max\": 6308.003187179565}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:32:12 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=97.65220300252153 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:32:12 INFO 140512987567936] #progress_metric: host=algo-1, completed 44.0 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:32:12 INFO 140512987567936] #quality_metric: host=algo-1, epoch=87, train loss <loss>=1.183350247144699\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:32:12 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:32:16 INFO 140512987567936] Epoch[88] Batch[0] avg_epoch_loss=1.392207\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:32:16 INFO 140512987567936] #quality_metric: host=algo-1, epoch=88, batch=0 train loss <loss>=1.392207145690918\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:32:17 INFO 140512987567936] Epoch[88] Batch[5] avg_epoch_loss=1.319241\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:32:17 INFO 140512987567936] #quality_metric: host=algo-1, epoch=88, batch=5 train loss <loss>=1.319240669409434\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:32:17 INFO 140512987567936] Epoch[88] Batch [5]#011Speed: 314.08 samples/sec#011loss=1.319241\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:32:18 INFO 140512987567936] processed a total of 626 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742059932.7497997, \"EndTime\": 1742059938.9309802, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6180.407762527466, \"count\": 1, \"min\": 6180.407762527466, \"max\": 6180.407762527466}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:32:18 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=101.28551785305591 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:32:18 INFO 140512987567936] #progress_metric: host=algo-1, completed 44.5 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:32:18 INFO 140512987567936] #quality_metric: host=algo-1, epoch=88, train loss <loss>=1.4257485270500183\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:32:18 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:32:23 INFO 140512987567936] Epoch[89] Batch[0] avg_epoch_loss=1.280886\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:32:23 INFO 140512987567936] #quality_metric: host=algo-1, epoch=89, batch=0 train loss <loss>=1.2808860540390015\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:32:24 INFO 140512987567936] Epoch[89] Batch[5] avg_epoch_loss=1.202845\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:32:24 INFO 140512987567936] #quality_metric: host=algo-1, epoch=89, batch=5 train loss <loss>=1.2028448681036632\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:32:24 INFO 140512987567936] Epoch[89] Batch [5]#011Speed: 296.67 samples/sec#011loss=1.202845\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:32:25 INFO 140512987567936] processed a total of 639 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742059938.9310844, \"EndTime\": 1742059945.2795532, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6347.618579864502, \"count\": 1, \"min\": 6347.618579864502, \"max\": 6347.618579864502}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:32:25 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=100.66605226486764 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:32:25 INFO 140512987567936] #progress_metric: host=algo-1, completed 45.0 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:32:25 INFO 140512987567936] #quality_metric: host=algo-1, epoch=89, train loss <loss>=1.2063881576061248\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:32:25 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:32:29 INFO 140512987567936] Epoch[90] Batch[0] avg_epoch_loss=1.138808\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:32:29 INFO 140512987567936] #quality_metric: host=algo-1, epoch=90, batch=0 train loss <loss>=1.1388078927993774\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:32:30 INFO 140512987567936] Epoch[90] Batch[5] avg_epoch_loss=1.414192\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:32:30 INFO 140512987567936] #quality_metric: host=algo-1, epoch=90, batch=5 train loss <loss>=1.414192020893097\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:32:30 INFO 140512987567936] Epoch[90] Batch [5]#011Speed: 292.93 samples/sec#011loss=1.414192\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:32:31 INFO 140512987567936] Epoch[90] Batch[10] avg_epoch_loss=1.343774\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:32:31 INFO 140512987567936] #quality_metric: host=algo-1, epoch=90, batch=10 train loss <loss>=1.2592717170715333\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:32:31 INFO 140512987567936] Epoch[90] Batch [10]#011Speed: 216.83 samples/sec#011loss=1.259272\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:32:31 INFO 140512987567936] processed a total of 659 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742059945.2796233, \"EndTime\": 1742059951.914841, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6634.813547134399, \"count\": 1, \"min\": 6634.813547134399, \"max\": 6634.813547134399}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:32:31 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=99.3232133395595 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:32:31 INFO 140512987567936] #progress_metric: host=algo-1, completed 45.5 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:32:31 INFO 140512987567936] #quality_metric: host=algo-1, epoch=90, train loss <loss>=1.3437737009742043\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:32:31 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:32:35 INFO 140512987567936] Epoch[91] Batch[0] avg_epoch_loss=1.085184\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:32:35 INFO 140512987567936] #quality_metric: host=algo-1, epoch=91, batch=0 train loss <loss>=1.0851839780807495\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:32:36 INFO 140512987567936] Epoch[91] Batch[5] avg_epoch_loss=1.205355\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:32:36 INFO 140512987567936] #quality_metric: host=algo-1, epoch=91, batch=5 train loss <loss>=1.2053554753462474\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:32:36 INFO 140512987567936] Epoch[91] Batch [5]#011Speed: 298.35 samples/sec#011loss=1.205355\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:32:38 INFO 140512987567936] Epoch[91] Batch[10] avg_epoch_loss=1.132884\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:32:38 INFO 140512987567936] #quality_metric: host=algo-1, epoch=91, batch=10 train loss <loss>=1.0459189772605897\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:32:38 INFO 140512987567936] Epoch[91] Batch [10]#011Speed: 194.26 samples/sec#011loss=1.045919\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:32:38 INFO 140512987567936] processed a total of 701 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742059951.9149027, \"EndTime\": 1742059958.5339732, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6618.791103363037, \"count\": 1, \"min\": 6618.791103363037, \"max\": 6618.791103363037}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:32:38 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=105.90904566678088 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:32:38 INFO 140512987567936] #progress_metric: host=algo-1, completed 46.0 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:32:38 INFO 140512987567936] #quality_metric: host=algo-1, epoch=91, train loss <loss>=1.1328843398527666\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:32:38 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:32:42 INFO 140512987567936] Epoch[92] Batch[0] avg_epoch_loss=1.589649\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:32:42 INFO 140512987567936] #quality_metric: host=algo-1, epoch=92, batch=0 train loss <loss>=1.5896492004394531\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:32:43 INFO 140512987567936] Epoch[92] Batch[5] avg_epoch_loss=1.385668\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:32:43 INFO 140512987567936] #quality_metric: host=algo-1, epoch=92, batch=5 train loss <loss>=1.3856679995854695\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:32:43 INFO 140512987567936] Epoch[92] Batch [5]#011Speed: 298.61 samples/sec#011loss=1.385668\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:32:45 INFO 140512987567936] Epoch[92] Batch[10] avg_epoch_loss=1.297992\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:32:45 INFO 140512987567936] #quality_metric: host=algo-1, epoch=92, batch=10 train loss <loss>=1.1927799716591836\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:32:45 INFO 140512987567936] Epoch[92] Batch [10]#011Speed: 198.91 samples/sec#011loss=1.192780\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:32:45 INFO 140512987567936] processed a total of 685 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742059958.5340393, \"EndTime\": 1742059965.1959777, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6661.59987449646, \"count\": 1, \"min\": 6661.59987449646, \"max\": 6661.59987449646}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:32:45 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=102.8258806862593 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:32:45 INFO 140512987567936] #progress_metric: host=algo-1, completed 46.5 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:32:45 INFO 140512987567936] #quality_metric: host=algo-1, epoch=92, train loss <loss>=1.2979916232553395\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:32:45 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:32:49 INFO 140512987567936] Epoch[93] Batch[0] avg_epoch_loss=0.834716\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:32:49 INFO 140512987567936] #quality_metric: host=algo-1, epoch=93, batch=0 train loss <loss>=0.8347163200378418\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:32:50 INFO 140512987567936] Epoch[93] Batch[5] avg_epoch_loss=1.140362\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:32:50 INFO 140512987567936] #quality_metric: host=algo-1, epoch=93, batch=5 train loss <loss>=1.140361895163854\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:32:50 INFO 140512987567936] Epoch[93] Batch [5]#011Speed: 296.23 samples/sec#011loss=1.140362\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:32:51 INFO 140512987567936] Epoch[93] Batch[10] avg_epoch_loss=1.195032\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:32:51 INFO 140512987567936] #quality_metric: host=algo-1, epoch=93, batch=10 train loss <loss>=1.2606369733810425\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:32:51 INFO 140512987567936] Epoch[93] Batch [10]#011Speed: 225.08 samples/sec#011loss=1.260637\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:32:51 INFO 140512987567936] processed a total of 652 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742059965.1960647, \"EndTime\": 1742059971.8697, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6673.253536224365, \"count\": 1, \"min\": 6673.253536224365, \"max\": 6673.253536224365}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:32:51 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=97.70181143866152 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:32:51 INFO 140512987567936] #progress_metric: host=algo-1, completed 47.0 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:32:51 INFO 140512987567936] #quality_metric: host=algo-1, epoch=93, train loss <loss>=1.195032385262576\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:32:51 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:32:55 INFO 140512987567936] Epoch[94] Batch[0] avg_epoch_loss=0.741328\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:32:55 INFO 140512987567936] #quality_metric: host=algo-1, epoch=94, batch=0 train loss <loss>=0.7413279414176941\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:32:57 INFO 140512987567936] Epoch[94] Batch[5] avg_epoch_loss=1.144463\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:32:57 INFO 140512987567936] #quality_metric: host=algo-1, epoch=94, batch=5 train loss <loss>=1.1444625556468964\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:32:57 INFO 140512987567936] Epoch[94] Batch [5]#011Speed: 291.01 samples/sec#011loss=1.144463\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:32:58 INFO 140512987567936] Epoch[94] Batch[10] avg_epoch_loss=1.264771\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:32:58 INFO 140512987567936] #quality_metric: host=algo-1, epoch=94, batch=10 train loss <loss>=1.409141516685486\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:32:58 INFO 140512987567936] Epoch[94] Batch [10]#011Speed: 237.55 samples/sec#011loss=1.409142\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:32:58 INFO 140512987567936] processed a total of 661 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742059971.8697774, \"EndTime\": 1742059978.407874, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6537.6856327056885, \"count\": 1, \"min\": 6537.6856327056885, \"max\": 6537.6856327056885}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:32:58 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=101.10464454091397 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:32:58 INFO 140512987567936] #progress_metric: host=algo-1, completed 47.5 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:32:58 INFO 140512987567936] #quality_metric: host=algo-1, epoch=94, train loss <loss>=1.2647711743008008\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:32:58 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:33:02 INFO 140512987567936] Epoch[95] Batch[0] avg_epoch_loss=1.308268\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:33:02 INFO 140512987567936] #quality_metric: host=algo-1, epoch=95, batch=0 train loss <loss>=1.3082679510116577\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:33:03 INFO 140512987567936] Epoch[95] Batch[5] avg_epoch_loss=1.134702\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:33:03 INFO 140512987567936] #quality_metric: host=algo-1, epoch=95, batch=5 train loss <loss>=1.1347021361192067\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:33:03 INFO 140512987567936] Epoch[95] Batch [5]#011Speed: 297.73 samples/sec#011loss=1.134702\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:33:04 INFO 140512987567936] Epoch[95] Batch[10] avg_epoch_loss=1.249360\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:33:04 INFO 140512987567936] #quality_metric: host=algo-1, epoch=95, batch=10 train loss <loss>=1.3869499921798707\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:33:04 INFO 140512987567936] Epoch[95] Batch [10]#011Speed: 209.79 samples/sec#011loss=1.386950\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:33:04 INFO 140512987567936] processed a total of 682 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742059978.4079378, \"EndTime\": 1742059984.9588852, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6550.6591796875, \"count\": 1, \"min\": 6550.6591796875, \"max\": 6550.6591796875}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:33:04 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=104.10991298306804 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:33:04 INFO 140512987567936] #progress_metric: host=algo-1, completed 48.0 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:33:04 INFO 140512987567936] #quality_metric: host=algo-1, epoch=95, train loss <loss>=1.2493602525104175\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:33:04 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:33:09 INFO 140512987567936] Epoch[96] Batch[0] avg_epoch_loss=1.221915\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:33:09 INFO 140512987567936] #quality_metric: host=algo-1, epoch=96, batch=0 train loss <loss>=1.2219147682189941\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:33:10 INFO 140512987567936] Epoch[96] Batch[5] avg_epoch_loss=1.291786\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:33:10 INFO 140512987567936] #quality_metric: host=algo-1, epoch=96, batch=5 train loss <loss>=1.291786253452301\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:33:10 INFO 140512987567936] Epoch[96] Batch [5]#011Speed: 299.53 samples/sec#011loss=1.291786\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:33:11 INFO 140512987567936] processed a total of 613 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742059984.9589596, \"EndTime\": 1742059991.1482654, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6188.036918640137, \"count\": 1, \"min\": 6188.036918640137, \"max\": 6188.036918640137}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:33:11 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=99.06046217020729 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:33:11 INFO 140512987567936] #progress_metric: host=algo-1, completed 48.5 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:33:11 INFO 140512987567936] #quality_metric: host=algo-1, epoch=96, train loss <loss>=1.1676116913557053\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:33:11 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:33:15 INFO 140512987567936] Epoch[97] Batch[0] avg_epoch_loss=1.149642\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:33:15 INFO 140512987567936] #quality_metric: host=algo-1, epoch=97, batch=0 train loss <loss>=1.149641990661621\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:33:16 INFO 140512987567936] Epoch[97] Batch[5] avg_epoch_loss=1.260146\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:33:16 INFO 140512987567936] #quality_metric: host=algo-1, epoch=97, batch=5 train loss <loss>=1.2601458231608074\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:33:16 INFO 140512987567936] Epoch[97] Batch [5]#011Speed: 312.93 samples/sec#011loss=1.260146\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:33:17 INFO 140512987567936] processed a total of 630 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742059991.1483362, \"EndTime\": 1742059997.2865906, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6137.791872024536, \"count\": 1, \"min\": 6137.791872024536, \"max\": 6137.791872024536}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:33:17 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=102.64048807001804 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:33:17 INFO 140512987567936] #progress_metric: host=algo-1, completed 49.0 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:33:17 INFO 140512987567936] #quality_metric: host=algo-1, epoch=97, train loss <loss>=1.2258256137371064\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:33:17 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:33:21 INFO 140512987567936] Epoch[98] Batch[0] avg_epoch_loss=1.954203\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:33:21 INFO 140512987567936] #quality_metric: host=algo-1, epoch=98, batch=0 train loss <loss>=1.9542030096054077\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:33:22 INFO 140512987567936] Epoch[98] Batch[5] avg_epoch_loss=1.040389\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:33:22 INFO 140512987567936] #quality_metric: host=algo-1, epoch=98, batch=5 train loss <loss>=1.040388862291972\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:33:22 INFO 140512987567936] Epoch[98] Batch [5]#011Speed: 321.82 samples/sec#011loss=1.040389\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:33:23 INFO 140512987567936] Epoch[98] Batch[10] avg_epoch_loss=0.837877\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:33:23 INFO 140512987567936] #quality_metric: host=algo-1, epoch=98, batch=10 train loss <loss>=0.5948622941970825\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:33:23 INFO 140512987567936] Epoch[98] Batch [10]#011Speed: 224.01 samples/sec#011loss=0.594862\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:33:23 INFO 140512987567936] processed a total of 648 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742059997.2866921, \"EndTime\": 1742060003.7858853, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6498.653888702393, \"count\": 1, \"min\": 6498.653888702393, \"max\": 6498.653888702393}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:33:23 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=99.71148341646754 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:33:23 INFO 140512987567936] #progress_metric: host=algo-1, completed 49.5 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:33:23 INFO 140512987567936] #quality_metric: host=algo-1, epoch=98, train loss <loss>=0.837876785885204\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:33:23 INFO 140512987567936] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:33:23 INFO 140512987567936] Saved checkpoint to \"/opt/ml/model/state_7b1b5d29-2ff0-4ee8-97a3-3fe5d53d3243-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742060003.7859507, \"EndTime\": 1742060003.7994697, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 13.164997100830078, \"count\": 1, \"min\": 13.164997100830078, \"max\": 13.164997100830078}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:33:27 INFO 140512987567936] Epoch[99] Batch[0] avg_epoch_loss=1.475935\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:33:27 INFO 140512987567936] #quality_metric: host=algo-1, epoch=99, batch=0 train loss <loss>=1.475935459136963\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:33:29 INFO 140512987567936] Epoch[99] Batch[5] avg_epoch_loss=1.553837\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:33:29 INFO 140512987567936] #quality_metric: host=algo-1, epoch=99, batch=5 train loss <loss>=1.5538365642229717\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:33:29 INFO 140512987567936] Epoch[99] Batch [5]#011Speed: 299.50 samples/sec#011loss=1.553837\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:33:30 INFO 140512987567936] processed a total of 638 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742060003.7995243, \"EndTime\": 1742060010.017041, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6217.466354370117, \"count\": 1, \"min\": 6217.466354370117, \"max\": 6217.466354370117}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:33:30 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=102.61251300359812 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:33:30 INFO 140512987567936] #progress_metric: host=algo-1, completed 50.0 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:33:30 INFO 140512987567936] #quality_metric: host=algo-1, epoch=99, train loss <loss>=1.5115360379219056\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:33:30 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:33:33 INFO 140512987567936] Epoch[100] Batch[0] avg_epoch_loss=1.306361\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:33:33 INFO 140512987567936] #quality_metric: host=algo-1, epoch=100, batch=0 train loss <loss>=1.3063606023788452\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:33:35 INFO 140512987567936] Epoch[100] Batch[5] avg_epoch_loss=1.238010\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:33:35 INFO 140512987567936] #quality_metric: host=algo-1, epoch=100, batch=5 train loss <loss>=1.2380101482073467\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:33:35 INFO 140512987567936] Epoch[100] Batch [5]#011Speed: 300.05 samples/sec#011loss=1.238010\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:33:36 INFO 140512987567936] Epoch[100] Batch[10] avg_epoch_loss=1.301640\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:33:36 INFO 140512987567936] #quality_metric: host=algo-1, epoch=100, batch=10 train loss <loss>=1.3779954671859742\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:33:36 INFO 140512987567936] Epoch[100] Batch [10]#011Speed: 210.17 samples/sec#011loss=1.377995\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:33:36 INFO 140512987567936] processed a total of 684 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742060010.017109, \"EndTime\": 1742060016.5605614, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6543.14661026001, \"count\": 1, \"min\": 6543.14661026001, \"max\": 6543.14661026001}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:33:36 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=104.53510766456377 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:33:36 INFO 140512987567936] #progress_metric: host=algo-1, completed 50.5 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:33:36 INFO 140512987567936] #quality_metric: host=algo-1, epoch=100, train loss <loss>=1.3016398386521773\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:33:36 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:33:40 INFO 140512987567936] Epoch[101] Batch[0] avg_epoch_loss=1.123603\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:33:40 INFO 140512987567936] #quality_metric: host=algo-1, epoch=101, batch=0 train loss <loss>=1.1236026287078857\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:33:41 INFO 140512987567936] Epoch[101] Batch[5] avg_epoch_loss=1.133199\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:33:41 INFO 140512987567936] #quality_metric: host=algo-1, epoch=101, batch=5 train loss <loss>=1.1331991056601207\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:33:41 INFO 140512987567936] Epoch[101] Batch [5]#011Speed: 291.05 samples/sec#011loss=1.133199\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:33:43 INFO 140512987567936] Epoch[101] Batch[10] avg_epoch_loss=1.177293\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:33:43 INFO 140512987567936] #quality_metric: host=algo-1, epoch=101, batch=10 train loss <loss>=1.2302056193351745\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:33:43 INFO 140512987567936] Epoch[101] Batch [10]#011Speed: 224.11 samples/sec#011loss=1.230206\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:33:43 INFO 140512987567936] processed a total of 657 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742060016.5606372, \"EndTime\": 1742060023.207391, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6646.365165710449, \"count\": 1, \"min\": 6646.365165710449, \"max\": 6646.365165710449}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:33:43 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=98.84962297780152 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:33:43 INFO 140512987567936] #progress_metric: host=algo-1, completed 51.0 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:33:43 INFO 140512987567936] #quality_metric: host=algo-1, epoch=101, train loss <loss>=1.177292975512418\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:33:43 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:33:47 INFO 140512987567936] Epoch[102] Batch[0] avg_epoch_loss=1.438614\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:33:47 INFO 140512987567936] #quality_metric: host=algo-1, epoch=102, batch=0 train loss <loss>=1.4386135339736938\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:33:48 INFO 140512987567936] Epoch[102] Batch[5] avg_epoch_loss=1.112069\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:33:48 INFO 140512987567936] #quality_metric: host=algo-1, epoch=102, batch=5 train loss <loss>=1.112068663040797\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:33:48 INFO 140512987567936] Epoch[102] Batch [5]#011Speed: 295.28 samples/sec#011loss=1.112069\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:33:49 INFO 140512987567936] processed a total of 635 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742060023.2074533, \"EndTime\": 1742060029.4892504, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6281.455993652344, \"count\": 1, \"min\": 6281.455993652344, \"max\": 6281.455993652344}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:33:49 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=101.08959288273722 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:33:49 INFO 140512987567936] #progress_metric: host=algo-1, completed 51.5 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:33:49 INFO 140512987567936] #quality_metric: host=algo-1, epoch=102, train loss <loss>=1.044462615251541\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:33:49 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:33:53 INFO 140512987567936] Epoch[103] Batch[0] avg_epoch_loss=0.983401\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:33:53 INFO 140512987567936] #quality_metric: host=algo-1, epoch=103, batch=0 train loss <loss>=0.9834007620811462\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:33:54 INFO 140512987567936] Epoch[103] Batch[5] avg_epoch_loss=1.114803\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:33:54 INFO 140512987567936] #quality_metric: host=algo-1, epoch=103, batch=5 train loss <loss>=1.114802857240041\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:33:54 INFO 140512987567936] Epoch[103] Batch [5]#011Speed: 291.81 samples/sec#011loss=1.114803\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:33:55 INFO 140512987567936] processed a total of 638 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742060029.489317, \"EndTime\": 1742060035.865222, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6375.4563331604, \"count\": 1, \"min\": 6375.4563331604, \"max\": 6375.4563331604}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:33:55 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=100.06971507478568 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:33:55 INFO 140512987567936] #progress_metric: host=algo-1, completed 52.0 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:33:55 INFO 140512987567936] #quality_metric: host=algo-1, epoch=103, train loss <loss>=1.143512785434723\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:33:55 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:34:00 INFO 140512987567936] Epoch[104] Batch[0] avg_epoch_loss=1.073765\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:34:00 INFO 140512987567936] #quality_metric: host=algo-1, epoch=104, batch=0 train loss <loss>=1.0737648010253906\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:34:01 INFO 140512987567936] Epoch[104] Batch[5] avg_epoch_loss=1.104795\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:34:01 INFO 140512987567936] #quality_metric: host=algo-1, epoch=104, batch=5 train loss <loss>=1.1047945320606232\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:34:01 INFO 140512987567936] Epoch[104] Batch [5]#011Speed: 279.20 samples/sec#011loss=1.104795\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:34:02 INFO 140512987567936] Epoch[104] Batch[10] avg_epoch_loss=0.956557\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:34:02 INFO 140512987567936] #quality_metric: host=algo-1, epoch=104, batch=10 train loss <loss>=0.7786725759506226\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:34:02 INFO 140512987567936] Epoch[104] Batch [10]#011Speed: 210.27 samples/sec#011loss=0.778673\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:34:02 INFO 140512987567936] processed a total of 649 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742060035.8652885, \"EndTime\": 1742060042.7770169, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6910.809278488159, \"count\": 1, \"min\": 6910.809278488159, \"max\": 6910.809278488159}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:34:02 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=93.90937647471713 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:34:02 INFO 140512987567936] #progress_metric: host=algo-1, completed 52.5 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:34:02 INFO 140512987567936] #quality_metric: host=algo-1, epoch=104, train loss <loss>=0.9565572792833502\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:34:02 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:34:06 INFO 140512987567936] Epoch[105] Batch[0] avg_epoch_loss=1.241376\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:34:06 INFO 140512987567936] #quality_metric: host=algo-1, epoch=105, batch=0 train loss <loss>=1.2413756847381592\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:34:08 INFO 140512987567936] Epoch[105] Batch[5] avg_epoch_loss=1.045171\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:34:08 INFO 140512987567936] #quality_metric: host=algo-1, epoch=105, batch=5 train loss <loss>=1.0451706945896149\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:34:08 INFO 140512987567936] Epoch[105] Batch [5]#011Speed: 310.65 samples/sec#011loss=1.045171\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:34:09 INFO 140512987567936] processed a total of 628 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742060042.7770925, \"EndTime\": 1742060049.0433118, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6265.795946121216, \"count\": 1, \"min\": 6265.795946121216, \"max\": 6265.795946121216}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:34:09 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=100.2249605391432 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:34:09 INFO 140512987567936] #progress_metric: host=algo-1, completed 53.0 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:34:09 INFO 140512987567936] #quality_metric: host=algo-1, epoch=105, train loss <loss>=0.9486818075180053\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:34:09 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:34:13 INFO 140512987567936] Epoch[106] Batch[0] avg_epoch_loss=1.068226\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:34:13 INFO 140512987567936] #quality_metric: host=algo-1, epoch=106, batch=0 train loss <loss>=1.0682255029678345\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:34:14 INFO 140512987567936] Epoch[106] Batch[5] avg_epoch_loss=0.963121\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:34:14 INFO 140512987567936] #quality_metric: host=algo-1, epoch=106, batch=5 train loss <loss>=0.9631206591924032\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:34:14 INFO 140512987567936] Epoch[106] Batch [5]#011Speed: 299.15 samples/sec#011loss=0.963121\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:34:15 INFO 140512987567936] Epoch[106] Batch[10] avg_epoch_loss=0.814396\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:34:15 INFO 140512987567936] #quality_metric: host=algo-1, epoch=106, batch=10 train loss <loss>=0.6359256505966187\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:34:15 INFO 140512987567936] Epoch[106] Batch [10]#011Speed: 241.96 samples/sec#011loss=0.635926\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:34:15 INFO 140512987567936] processed a total of 648 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742060049.0433834, \"EndTime\": 1742060055.579998, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6535.743236541748, \"count\": 1, \"min\": 6535.743236541748, \"max\": 6535.743236541748}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:34:15 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=99.14553212948096 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:34:15 INFO 140512987567936] #progress_metric: host=algo-1, completed 53.5 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:34:15 INFO 140512987567936] #quality_metric: host=algo-1, epoch=106, train loss <loss>=0.8143956552852284\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:34:15 INFO 140512987567936] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:34:15 INFO 140512987567936] Saved checkpoint to \"/opt/ml/model/state_159ebd6c-eaa2-4832-b56d-489a971280b4-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742060055.5800686, \"EndTime\": 1742060055.5933423, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 12.854814529418945, \"count\": 1, \"min\": 12.854814529418945, \"max\": 12.854814529418945}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:34:19 INFO 140512987567936] Epoch[107] Batch[0] avg_epoch_loss=1.507537\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:34:19 INFO 140512987567936] #quality_metric: host=algo-1, epoch=107, batch=0 train loss <loss>=1.5075368881225586\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:34:20 INFO 140512987567936] Epoch[107] Batch[5] avg_epoch_loss=1.139181\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:34:20 INFO 140512987567936] #quality_metric: host=algo-1, epoch=107, batch=5 train loss <loss>=1.1391810675462086\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:34:20 INFO 140512987567936] Epoch[107] Batch [5]#011Speed: 303.33 samples/sec#011loss=1.139181\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:34:21 INFO 140512987567936] Epoch[107] Batch[10] avg_epoch_loss=1.251168\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:34:21 INFO 140512987567936] #quality_metric: host=algo-1, epoch=107, batch=10 train loss <loss>=1.385551428794861\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:34:21 INFO 140512987567936] Epoch[107] Batch [10]#011Speed: 233.53 samples/sec#011loss=1.385551\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:34:21 INFO 140512987567936] processed a total of 676 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742060055.5934026, \"EndTime\": 1742060061.9669435, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6373.484373092651, \"count\": 1, \"min\": 6373.484373092651, \"max\": 6373.484373092651}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:34:21 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=106.0626704111339 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:34:21 INFO 140512987567936] #progress_metric: host=algo-1, completed 54.0 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:34:21 INFO 140512987567936] #quality_metric: host=algo-1, epoch=107, train loss <loss>=1.2511675953865051\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:34:21 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:34:25 INFO 140512987567936] Epoch[108] Batch[0] avg_epoch_loss=1.302354\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:34:25 INFO 140512987567936] #quality_metric: host=algo-1, epoch=108, batch=0 train loss <loss>=1.3023535013198853\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:34:26 INFO 140512987567936] Epoch[108] Batch[5] avg_epoch_loss=1.164767\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:34:26 INFO 140512987567936] #quality_metric: host=algo-1, epoch=108, batch=5 train loss <loss>=1.164766550064087\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:34:26 INFO 140512987567936] Epoch[108] Batch [5]#011Speed: 297.00 samples/sec#011loss=1.164767\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:34:28 INFO 140512987567936] Epoch[108] Batch[10] avg_epoch_loss=1.218777\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:34:28 INFO 140512987567936] #quality_metric: host=algo-1, epoch=108, batch=10 train loss <loss>=1.2835895776748658\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:34:28 INFO 140512987567936] Epoch[108] Batch [10]#011Speed: 210.56 samples/sec#011loss=1.283590\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:34:28 INFO 140512987567936] processed a total of 685 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742060061.967016, \"EndTime\": 1742060068.4273667, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6459.944009780884, \"count\": 1, \"min\": 6459.944009780884, \"max\": 6459.944009780884}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:34:28 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=106.03620793089023 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:34:28 INFO 140512987567936] #progress_metric: host=algo-1, completed 54.5 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:34:28 INFO 140512987567936] #quality_metric: host=algo-1, epoch=108, train loss <loss>=1.2187770171598955\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:34:28 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:34:32 INFO 140512987567936] Epoch[109] Batch[0] avg_epoch_loss=1.214507\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:34:32 INFO 140512987567936] #quality_metric: host=algo-1, epoch=109, batch=0 train loss <loss>=1.2145066261291504\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:34:33 INFO 140512987567936] Epoch[109] Batch[5] avg_epoch_loss=1.186353\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:34:33 INFO 140512987567936] #quality_metric: host=algo-1, epoch=109, batch=5 train loss <loss>=1.186352699995041\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:34:33 INFO 140512987567936] Epoch[109] Batch [5]#011Speed: 304.55 samples/sec#011loss=1.186353\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:34:34 INFO 140512987567936] processed a total of 605 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742060068.4274433, \"EndTime\": 1742060074.5703282, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6142.459869384766, \"count\": 1, \"min\": 6142.459869384766, \"max\": 6142.459869384766}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:34:34 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=98.49297958900092 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:34:34 INFO 140512987567936] #progress_metric: host=algo-1, completed 55.0 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:34:34 INFO 140512987567936] #quality_metric: host=algo-1, epoch=109, train loss <loss>=1.095127487182617\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:34:34 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:34:38 INFO 140512987567936] Epoch[110] Batch[0] avg_epoch_loss=1.033044\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:34:38 INFO 140512987567936] #quality_metric: host=algo-1, epoch=110, batch=0 train loss <loss>=1.0330438613891602\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:34:39 INFO 140512987567936] Epoch[110] Batch[5] avg_epoch_loss=0.987153\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:34:39 INFO 140512987567936] #quality_metric: host=algo-1, epoch=110, batch=5 train loss <loss>=0.9871534407138824\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:34:39 INFO 140512987567936] Epoch[110] Batch [5]#011Speed: 301.49 samples/sec#011loss=0.987153\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:34:41 INFO 140512987567936] Epoch[110] Batch[10] avg_epoch_loss=0.988012\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:34:41 INFO 140512987567936] #quality_metric: host=algo-1, epoch=110, batch=10 train loss <loss>=0.9890430927276611\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:34:41 INFO 140512987567936] Epoch[110] Batch [10]#011Speed: 219.14 samples/sec#011loss=0.989043\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:34:41 INFO 140512987567936] processed a total of 671 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742060074.570405, \"EndTime\": 1742060081.071329, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6500.36096572876, \"count\": 1, \"min\": 6500.36096572876, \"max\": 6500.36096572876}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:34:41 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=103.2235337707433 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:34:41 INFO 140512987567936] #progress_metric: host=algo-1, completed 55.5 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:34:41 INFO 140512987567936] #quality_metric: host=algo-1, epoch=110, train loss <loss>=0.9880123734474182\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:34:41 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:34:45 INFO 140512987567936] Epoch[111] Batch[0] avg_epoch_loss=1.016177\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:34:45 INFO 140512987567936] #quality_metric: host=algo-1, epoch=111, batch=0 train loss <loss>=1.0161774158477783\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:34:46 INFO 140512987567936] Epoch[111] Batch[5] avg_epoch_loss=1.179401\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:34:46 INFO 140512987567936] #quality_metric: host=algo-1, epoch=111, batch=5 train loss <loss>=1.1794014076391857\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:34:46 INFO 140512987567936] Epoch[111] Batch [5]#011Speed: 301.60 samples/sec#011loss=1.179401\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:34:47 INFO 140512987567936] processed a total of 623 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742060081.0713913, \"EndTime\": 1742060087.291664, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6219.996452331543, \"count\": 1, \"min\": 6219.996452331543, \"max\": 6219.996452331543}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:34:47 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=100.15917797648402 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:34:47 INFO 140512987567936] #progress_metric: host=algo-1, completed 56.0 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:34:47 INFO 140512987567936] #quality_metric: host=algo-1, epoch=111, train loss <loss>=1.153783494234085\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:34:47 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:34:51 INFO 140512987567936] Epoch[112] Batch[0] avg_epoch_loss=1.158517\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:34:51 INFO 140512987567936] #quality_metric: host=algo-1, epoch=112, batch=0 train loss <loss>=1.158516764640808\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:34:52 INFO 140512987567936] Epoch[112] Batch[5] avg_epoch_loss=1.053886\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:34:52 INFO 140512987567936] #quality_metric: host=algo-1, epoch=112, batch=5 train loss <loss>=1.053885668516159\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:34:52 INFO 140512987567936] Epoch[112] Batch [5]#011Speed: 296.27 samples/sec#011loss=1.053886\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:34:53 INFO 140512987567936] Epoch[112] Batch[10] avg_epoch_loss=0.935585\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:34:53 INFO 140512987567936] #quality_metric: host=algo-1, epoch=112, batch=10 train loss <loss>=0.7936244428157806\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:34:53 INFO 140512987567936] Epoch[112] Batch [10]#011Speed: 208.14 samples/sec#011loss=0.793624\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:34:53 INFO 140512987567936] processed a total of 661 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742060087.2917366, \"EndTime\": 1742060093.952162, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6660.001277923584, \"count\": 1, \"min\": 6660.001277923584, \"max\": 6660.001277923584}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:34:53 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=99.24778416291811 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:34:53 INFO 140512987567936] #progress_metric: host=algo-1, completed 56.5 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:34:53 INFO 140512987567936] #quality_metric: host=algo-1, epoch=112, train loss <loss>=0.9355851113796234\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:34:53 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:34:58 INFO 140512987567936] Epoch[113] Batch[0] avg_epoch_loss=1.060844\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:34:58 INFO 140512987567936] #quality_metric: host=algo-1, epoch=113, batch=0 train loss <loss>=1.060843825340271\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:34:59 INFO 140512987567936] Epoch[113] Batch[5] avg_epoch_loss=1.348041\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:34:59 INFO 140512987567936] #quality_metric: host=algo-1, epoch=113, batch=5 train loss <loss>=1.3480409383773804\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:34:59 INFO 140512987567936] Epoch[113] Batch [5]#011Speed: 297.39 samples/sec#011loss=1.348041\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:35:00 INFO 140512987567936] Epoch[113] Batch[10] avg_epoch_loss=1.149954\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:35:00 INFO 140512987567936] #quality_metric: host=algo-1, epoch=113, batch=10 train loss <loss>=0.9122490525245667\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:35:00 INFO 140512987567936] Epoch[113] Batch [10]#011Speed: 220.96 samples/sec#011loss=0.912249\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:35:00 INFO 140512987567936] processed a total of 655 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742060093.9522254, \"EndTime\": 1742060100.622445, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6669.862747192383, \"count\": 1, \"min\": 6669.862747192383, \"max\": 6669.862747192383}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:35:00 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=98.20109148739276 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:35:00 INFO 140512987567936] #progress_metric: host=algo-1, completed 57.0 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:35:00 INFO 140512987567936] #quality_metric: host=algo-1, epoch=113, train loss <loss>=1.1499537175351924\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:35:00 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:35:04 INFO 140512987567936] Epoch[114] Batch[0] avg_epoch_loss=0.682309\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:35:04 INFO 140512987567936] #quality_metric: host=algo-1, epoch=114, batch=0 train loss <loss>=0.6823087334632874\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:35:05 INFO 140512987567936] Epoch[114] Batch[5] avg_epoch_loss=1.441143\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:35:05 INFO 140512987567936] #quality_metric: host=algo-1, epoch=114, batch=5 train loss <loss>=1.4411433041095734\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:35:05 INFO 140512987567936] Epoch[114] Batch [5]#011Speed: 306.56 samples/sec#011loss=1.441143\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:35:07 INFO 140512987567936] Epoch[114] Batch[10] avg_epoch_loss=1.280016\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:35:07 INFO 140512987567936] #quality_metric: host=algo-1, epoch=114, batch=10 train loss <loss>=1.0866637408733368\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:35:07 INFO 140512987567936] Epoch[114] Batch [10]#011Speed: 234.62 samples/sec#011loss=1.086664\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:35:07 INFO 140512987567936] processed a total of 664 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742060100.6225212, \"EndTime\": 1742060107.1778011, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6554.962396621704, \"count\": 1, \"min\": 6554.962396621704, \"max\": 6554.962396621704}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:35:07 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=101.29580163055734 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:35:07 INFO 140512987567936] #progress_metric: host=algo-1, completed 57.5 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:35:07 INFO 140512987567936] #quality_metric: host=algo-1, epoch=114, train loss <loss>=1.280016229911284\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:35:07 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:35:11 INFO 140512987567936] Epoch[115] Batch[0] avg_epoch_loss=0.767018\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:35:11 INFO 140512987567936] #quality_metric: host=algo-1, epoch=115, batch=0 train loss <loss>=0.7670178413391113\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:35:12 INFO 140512987567936] Epoch[115] Batch[5] avg_epoch_loss=1.072949\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:35:12 INFO 140512987567936] #quality_metric: host=algo-1, epoch=115, batch=5 train loss <loss>=1.0729488134384155\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:35:12 INFO 140512987567936] Epoch[115] Batch [5]#011Speed: 311.64 samples/sec#011loss=1.072949\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:35:13 INFO 140512987567936] processed a total of 619 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742060107.177867, \"EndTime\": 1742060113.2961042, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6117.81907081604, \"count\": 1, \"min\": 6117.81907081604, \"max\": 6117.81907081604}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:35:13 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=101.17819526825991 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:35:13 INFO 140512987567936] #progress_metric: host=algo-1, completed 58.0 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:35:13 INFO 140512987567936] #quality_metric: host=algo-1, epoch=115, train loss <loss>=1.2059000730514526\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:35:13 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:35:17 INFO 140512987567936] Epoch[116] Batch[0] avg_epoch_loss=1.301687\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:35:17 INFO 140512987567936] #quality_metric: host=algo-1, epoch=116, batch=0 train loss <loss>=1.3016865253448486\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:35:18 INFO 140512987567936] Epoch[116] Batch[5] avg_epoch_loss=1.222099\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:35:18 INFO 140512987567936] #quality_metric: host=algo-1, epoch=116, batch=5 train loss <loss>=1.2220990856488545\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:35:18 INFO 140512987567936] Epoch[116] Batch [5]#011Speed: 302.53 samples/sec#011loss=1.222099\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:35:19 INFO 140512987567936] Epoch[116] Batch[10] avg_epoch_loss=1.023522\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:35:19 INFO 140512987567936] #quality_metric: host=algo-1, epoch=116, batch=10 train loss <loss>=0.785228681564331\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:35:19 INFO 140512987567936] Epoch[116] Batch [10]#011Speed: 212.20 samples/sec#011loss=0.785229\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:35:19 INFO 140512987567936] processed a total of 660 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742060113.2961712, \"EndTime\": 1742060119.903342, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6606.636762619019, \"count\": 1, \"min\": 6606.636762619019, \"max\": 6606.636762619019}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:35:19 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=99.89801544654266 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:35:19 INFO 140512987567936] #progress_metric: host=algo-1, completed 58.5 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:35:19 INFO 140512987567936] #quality_metric: host=algo-1, epoch=116, train loss <loss>=1.0235216292467983\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:35:19 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:35:24 INFO 140512987567936] Epoch[117] Batch[0] avg_epoch_loss=1.312156\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:35:24 INFO 140512987567936] #quality_metric: host=algo-1, epoch=117, batch=0 train loss <loss>=1.3121556043624878\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:35:25 INFO 140512987567936] Epoch[117] Batch[5] avg_epoch_loss=1.145025\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:35:25 INFO 140512987567936] #quality_metric: host=algo-1, epoch=117, batch=5 train loss <loss>=1.1450254321098328\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:35:25 INFO 140512987567936] Epoch[117] Batch [5]#011Speed: 291.28 samples/sec#011loss=1.145025\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:35:26 INFO 140512987567936] processed a total of 594 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742060119.9034083, \"EndTime\": 1742060126.1096683, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6205.864429473877, \"count\": 1, \"min\": 6205.864429473877, \"max\": 6205.864429473877}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:35:26 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=95.71441963828303 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:35:26 INFO 140512987567936] #progress_metric: host=algo-1, completed 59.0 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:35:26 INFO 140512987567936] #quality_metric: host=algo-1, epoch=117, train loss <loss>=0.8742126941680908\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:35:26 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:35:29 INFO 140512987567936] Epoch[118] Batch[0] avg_epoch_loss=1.237379\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:35:29 INFO 140512987567936] #quality_metric: host=algo-1, epoch=118, batch=0 train loss <loss>=1.2373790740966797\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:35:31 INFO 140512987567936] Epoch[118] Batch[5] avg_epoch_loss=1.278067\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:35:31 INFO 140512987567936] #quality_metric: host=algo-1, epoch=118, batch=5 train loss <loss>=1.2780667344729106\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:35:31 INFO 140512987567936] Epoch[118] Batch [5]#011Speed: 278.30 samples/sec#011loss=1.278067\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:35:32 INFO 140512987567936] Epoch[118] Batch[10] avg_epoch_loss=1.120788\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:35:32 INFO 140512987567936] #quality_metric: host=algo-1, epoch=118, batch=10 train loss <loss>=0.9320528149604798\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:35:32 INFO 140512987567936] Epoch[118] Batch [10]#011Speed: 203.77 samples/sec#011loss=0.932053\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:35:32 INFO 140512987567936] processed a total of 722 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742060126.1097329, \"EndTime\": 1742060132.9558704, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6845.834970474243, \"count\": 1, \"min\": 6845.834970474243, \"max\": 6845.834970474243}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:35:32 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=105.46406952501027 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:35:32 INFO 140512987567936] #progress_metric: host=algo-1, completed 59.5 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:35:32 INFO 140512987567936] #quality_metric: host=algo-1, epoch=118, train loss <loss>=1.2511652062336605\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:35:32 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:35:37 INFO 140512987567936] Epoch[119] Batch[0] avg_epoch_loss=0.932990\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:35:37 INFO 140512987567936] #quality_metric: host=algo-1, epoch=119, batch=0 train loss <loss>=0.9329895973205566\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:35:38 INFO 140512987567936] Epoch[119] Batch[5] avg_epoch_loss=1.104029\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:35:38 INFO 140512987567936] #quality_metric: host=algo-1, epoch=119, batch=5 train loss <loss>=1.1040286719799042\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:35:38 INFO 140512987567936] Epoch[119] Batch [5]#011Speed: 299.18 samples/sec#011loss=1.104029\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:35:39 INFO 140512987567936] processed a total of 633 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742060132.9559393, \"EndTime\": 1742060139.1580815, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6201.186895370483, \"count\": 1, \"min\": 6201.186895370483, \"max\": 6201.186895370483}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:35:39 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=102.07547880420344 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:35:39 INFO 140512987567936] #progress_metric: host=algo-1, completed 60.0 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:35:39 INFO 140512987567936] #quality_metric: host=algo-1, epoch=119, train loss <loss>=1.0711436867713928\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:35:39 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:35:43 INFO 140512987567936] Epoch[120] Batch[0] avg_epoch_loss=0.746329\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:35:43 INFO 140512987567936] #quality_metric: host=algo-1, epoch=120, batch=0 train loss <loss>=0.7463293075561523\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:35:44 INFO 140512987567936] Epoch[120] Batch[5] avg_epoch_loss=1.060630\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:35:44 INFO 140512987567936] #quality_metric: host=algo-1, epoch=120, batch=5 train loss <loss>=1.0606296161810558\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:35:44 INFO 140512987567936] Epoch[120] Batch [5]#011Speed: 293.74 samples/sec#011loss=1.060630\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:35:45 INFO 140512987567936] Epoch[120] Batch[10] avg_epoch_loss=1.117691\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:35:45 INFO 140512987567936] #quality_metric: host=algo-1, epoch=120, batch=10 train loss <loss>=1.186165177822113\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:35:45 INFO 140512987567936] Epoch[120] Batch [10]#011Speed: 213.01 samples/sec#011loss=1.186165\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:35:45 INFO 140512987567936] processed a total of 663 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742060139.1581542, \"EndTime\": 1742060145.764239, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6605.647325515747, \"count\": 1, \"min\": 6605.647325515747, \"max\": 6605.647325515747}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:35:45 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=100.367175591695 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:35:45 INFO 140512987567936] #progress_metric: host=algo-1, completed 60.5 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:35:45 INFO 140512987567936] #quality_metric: host=algo-1, epoch=120, train loss <loss>=1.117691235108809\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:35:45 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:35:49 INFO 140512987567936] Epoch[121] Batch[0] avg_epoch_loss=1.255519\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:35:49 INFO 140512987567936] #quality_metric: host=algo-1, epoch=121, batch=0 train loss <loss>=1.2555186748504639\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:35:50 INFO 140512987567936] Epoch[121] Batch[5] avg_epoch_loss=1.275471\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:35:50 INFO 140512987567936] #quality_metric: host=algo-1, epoch=121, batch=5 train loss <loss>=1.2754708329836528\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:35:50 INFO 140512987567936] Epoch[121] Batch [5]#011Speed: 319.77 samples/sec#011loss=1.275471\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:35:51 INFO 140512987567936] processed a total of 607 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742060145.7643065, \"EndTime\": 1742060151.8323984, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6067.802429199219, \"count\": 1, \"min\": 6067.802429199219, \"max\": 6067.802429199219}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:35:51 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=100.03457392364336 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:35:51 INFO 140512987567936] #progress_metric: host=algo-1, completed 61.0 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:35:51 INFO 140512987567936] #quality_metric: host=algo-1, epoch=121, train loss <loss>=1.115654593706131\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:35:51 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:35:55 INFO 140512987567936] Epoch[122] Batch[0] avg_epoch_loss=1.055394\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:35:55 INFO 140512987567936] #quality_metric: host=algo-1, epoch=122, batch=0 train loss <loss>=1.0553940534591675\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:35:57 INFO 140512987567936] Epoch[122] Batch[5] avg_epoch_loss=1.031733\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:35:57 INFO 140512987567936] #quality_metric: host=algo-1, epoch=122, batch=5 train loss <loss>=1.0317325790723164\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:35:57 INFO 140512987567936] Epoch[122] Batch [5]#011Speed: 287.12 samples/sec#011loss=1.031733\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:35:58 INFO 140512987567936] Epoch[122] Batch[10] avg_epoch_loss=0.802646\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:35:58 INFO 140512987567936] #quality_metric: host=algo-1, epoch=122, batch=10 train loss <loss>=0.5277414083480835\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:35:58 INFO 140512987567936] Epoch[122] Batch [10]#011Speed: 230.28 samples/sec#011loss=0.527741\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:35:58 INFO 140512987567936] processed a total of 642 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742060151.8324676, \"EndTime\": 1742060158.4980085, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6665.225267410278, \"count\": 1, \"min\": 6665.225267410278, \"max\": 6665.225267410278}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:35:58 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=96.31927853449099 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:35:58 INFO 140512987567936] #progress_metric: host=algo-1, completed 61.5 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:35:58 INFO 140512987567936] #quality_metric: host=algo-1, epoch=122, train loss <loss>=0.8026456832885742\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:35:58 INFO 140512987567936] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:35:58 INFO 140512987567936] Saved checkpoint to \"/opt/ml/model/state_a6d0ddbe-8310-48a6-a0a8-c5d0792d7bf3-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742060158.498083, \"EndTime\": 1742060158.5116668, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 13.141155242919922, \"count\": 1, \"min\": 13.141155242919922, \"max\": 13.141155242919922}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:36:02 INFO 140512987567936] Epoch[123] Batch[0] avg_epoch_loss=0.391701\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:36:02 INFO 140512987567936] #quality_metric: host=algo-1, epoch=123, batch=0 train loss <loss>=0.3917011320590973\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:36:03 INFO 140512987567936] Epoch[123] Batch[5] avg_epoch_loss=0.808144\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:36:03 INFO 140512987567936] #quality_metric: host=algo-1, epoch=123, batch=5 train loss <loss>=0.8081440875927607\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:36:03 INFO 140512987567936] Epoch[123] Batch [5]#011Speed: 306.74 samples/sec#011loss=0.808144\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:36:04 INFO 140512987567936] Epoch[123] Batch[10] avg_epoch_loss=0.757043\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:36:04 INFO 140512987567936] #quality_metric: host=algo-1, epoch=123, batch=10 train loss <loss>=0.6957226514816284\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:36:04 INFO 140512987567936] Epoch[123] Batch [10]#011Speed: 259.65 samples/sec#011loss=0.695723\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:36:04 INFO 140512987567936] processed a total of 642 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742060158.511728, \"EndTime\": 1742060164.9898345, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6478.055000305176, \"count\": 1, \"min\": 6478.055000305176, \"max\": 6478.055000305176}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:36:04 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=99.10242356691238 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:36:04 INFO 140512987567936] #progress_metric: host=algo-1, completed 62.0 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:36:04 INFO 140512987567936] #quality_metric: host=algo-1, epoch=123, train loss <loss>=0.7570434348149733\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:36:04 INFO 140512987567936] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:36:05 INFO 140512987567936] Saved checkpoint to \"/opt/ml/model/state_7b61226a-e19c-4982-a346-11ec5f4da8b7-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742060164.9898973, \"EndTime\": 1742060165.0038643, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 13.662338256835938, \"count\": 1, \"min\": 13.662338256835938, \"max\": 13.662338256835938}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:36:09 INFO 140512987567936] Epoch[124] Batch[0] avg_epoch_loss=1.551568\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:36:09 INFO 140512987567936] #quality_metric: host=algo-1, epoch=124, batch=0 train loss <loss>=1.551567554473877\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:36:10 INFO 140512987567936] Epoch[124] Batch[5] avg_epoch_loss=1.227296\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:36:10 INFO 140512987567936] #quality_metric: host=algo-1, epoch=124, batch=5 train loss <loss>=1.2272963027159374\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:36:10 INFO 140512987567936] Epoch[124] Batch [5]#011Speed: 286.69 samples/sec#011loss=1.227296\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:36:11 INFO 140512987567936] Epoch[124] Batch[10] avg_epoch_loss=1.036737\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:36:11 INFO 140512987567936] #quality_metric: host=algo-1, epoch=124, batch=10 train loss <loss>=0.8080652832984925\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:36:11 INFO 140512987567936] Epoch[124] Batch [10]#011Speed: 223.05 samples/sec#011loss=0.808065\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:36:11 INFO 140512987567936] processed a total of 658 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742060165.003926, \"EndTime\": 1742060171.615688, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6611.708402633667, \"count\": 1, \"min\": 6611.708402633667, \"max\": 6611.708402633667}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:36:11 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=99.51912851186586 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:36:11 INFO 140512987567936] #progress_metric: host=algo-1, completed 62.5 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:36:11 INFO 140512987567936] #quality_metric: host=algo-1, epoch=124, train loss <loss>=1.0367367484352805\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:36:11 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:36:15 INFO 140512987567936] Epoch[125] Batch[0] avg_epoch_loss=0.951138\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:36:15 INFO 140512987567936] #quality_metric: host=algo-1, epoch=125, batch=0 train loss <loss>=0.951138436794281\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:36:16 INFO 140512987567936] Epoch[125] Batch[5] avg_epoch_loss=1.061309\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:36:16 INFO 140512987567936] #quality_metric: host=algo-1, epoch=125, batch=5 train loss <loss>=1.0613093276818593\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:36:16 INFO 140512987567936] Epoch[125] Batch [5]#011Speed: 303.00 samples/sec#011loss=1.061309\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:36:17 INFO 140512987567936] processed a total of 636 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742060171.6157458, \"EndTime\": 1742060177.829096, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6213.083505630493, \"count\": 1, \"min\": 6213.083505630493, \"max\": 6213.083505630493}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:36:17 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=102.3629850828742 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:36:17 INFO 140512987567936] #progress_metric: host=algo-1, completed 63.0 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:36:17 INFO 140512987567936] #quality_metric: host=algo-1, epoch=125, train loss <loss>=1.0129137933254242\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:36:17 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:36:21 INFO 140512987567936] Epoch[126] Batch[0] avg_epoch_loss=0.973589\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:36:21 INFO 140512987567936] #quality_metric: host=algo-1, epoch=126, batch=0 train loss <loss>=0.9735887050628662\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:36:23 INFO 140512987567936] Epoch[126] Batch[5] avg_epoch_loss=0.999625\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:36:23 INFO 140512987567936] #quality_metric: host=algo-1, epoch=126, batch=5 train loss <loss>=0.9996246993541718\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:36:23 INFO 140512987567936] Epoch[126] Batch [5]#011Speed: 305.25 samples/sec#011loss=0.999625\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:36:24 INFO 140512987567936] processed a total of 627 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742060177.8291638, \"EndTime\": 1742060184.0379374, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6208.325624465942, \"count\": 1, \"min\": 6208.325624465942, \"max\": 6208.325624465942}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:36:24 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=100.99107534010481 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:36:24 INFO 140512987567936] #progress_metric: host=algo-1, completed 63.5 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:36:24 INFO 140512987567936] #quality_metric: host=algo-1, epoch=126, train loss <loss>=0.9365153759717941\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:36:24 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:36:28 INFO 140512987567936] Epoch[127] Batch[0] avg_epoch_loss=0.812616\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:36:28 INFO 140512987567936] #quality_metric: host=algo-1, epoch=127, batch=0 train loss <loss>=0.8126158118247986\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:36:29 INFO 140512987567936] Epoch[127] Batch[5] avg_epoch_loss=0.967171\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:36:29 INFO 140512987567936] #quality_metric: host=algo-1, epoch=127, batch=5 train loss <loss>=0.967170774936676\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:36:29 INFO 140512987567936] Epoch[127] Batch [5]#011Speed: 303.91 samples/sec#011loss=0.967171\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:36:30 INFO 140512987567936] Epoch[127] Batch[10] avg_epoch_loss=1.048011\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:36:30 INFO 140512987567936] #quality_metric: host=algo-1, epoch=127, batch=10 train loss <loss>=1.1450196504592896\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:36:30 INFO 140512987567936] Epoch[127] Batch [10]#011Speed: 235.12 samples/sec#011loss=1.145020\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:36:30 INFO 140512987567936] processed a total of 646 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742060184.0380454, \"EndTime\": 1742060190.5973196, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6558.72917175293, \"count\": 1, \"min\": 6558.72917175293, \"max\": 6558.72917175293}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:36:30 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=98.49299701702417 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:36:30 INFO 140512987567936] #progress_metric: host=algo-1, completed 64.0 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:36:30 INFO 140512987567936] #quality_metric: host=algo-1, epoch=127, train loss <loss>=1.0480111729015003\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:36:30 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:36:34 INFO 140512987567936] Epoch[128] Batch[0] avg_epoch_loss=0.867582\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:36:34 INFO 140512987567936] #quality_metric: host=algo-1, epoch=128, batch=0 train loss <loss>=0.8675820827484131\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:36:35 INFO 140512987567936] Epoch[128] Batch[5] avg_epoch_loss=1.123396\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:36:35 INFO 140512987567936] #quality_metric: host=algo-1, epoch=128, batch=5 train loss <loss>=1.1233960688114166\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:36:35 INFO 140512987567936] Epoch[128] Batch [5]#011Speed: 289.55 samples/sec#011loss=1.123396\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:36:37 INFO 140512987567936] Epoch[128] Batch[10] avg_epoch_loss=1.337570\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:36:37 INFO 140512987567936] #quality_metric: host=algo-1, epoch=128, batch=10 train loss <loss>=1.5945781588554382\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:36:37 INFO 140512987567936] Epoch[128] Batch [10]#011Speed: 229.89 samples/sec#011loss=1.594578\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:36:37 INFO 140512987567936] processed a total of 646 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742060190.5973964, \"EndTime\": 1742060197.23116, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6633.352041244507, \"count\": 1, \"min\": 6633.352041244507, \"max\": 6633.352041244507}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:36:37 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=97.38471038123626 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:36:37 INFO 140512987567936] #progress_metric: host=algo-1, completed 64.5 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:36:37 INFO 140512987567936] #quality_metric: host=algo-1, epoch=128, train loss <loss>=1.3375697461041538\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:36:37 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:36:41 INFO 140512987567936] Epoch[129] Batch[0] avg_epoch_loss=0.833107\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:36:41 INFO 140512987567936] #quality_metric: host=algo-1, epoch=129, batch=0 train loss <loss>=0.8331067562103271\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:36:42 INFO 140512987567936] Epoch[129] Batch[5] avg_epoch_loss=1.007169\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:36:42 INFO 140512987567936] #quality_metric: host=algo-1, epoch=129, batch=5 train loss <loss>=1.0071690579255421\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:36:42 INFO 140512987567936] Epoch[129] Batch [5]#011Speed: 300.50 samples/sec#011loss=1.007169\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:36:43 INFO 140512987567936] processed a total of 628 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742060197.2312562, \"EndTime\": 1742060203.4699173, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6238.39807510376, \"count\": 1, \"min\": 6238.39807510376, \"max\": 6238.39807510376}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:36:43 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=100.66529519993907 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:36:43 INFO 140512987567936] #progress_metric: host=algo-1, completed 65.0 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:36:43 INFO 140512987567936] #quality_metric: host=algo-1, epoch=129, train loss <loss>=1.0350748956203462\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:36:43 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:36:47 INFO 140512987567936] Epoch[130] Batch[0] avg_epoch_loss=0.819377\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:36:47 INFO 140512987567936] #quality_metric: host=algo-1, epoch=130, batch=0 train loss <loss>=0.8193774819374084\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:36:48 INFO 140512987567936] Epoch[130] Batch[5] avg_epoch_loss=1.155550\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:36:48 INFO 140512987567936] #quality_metric: host=algo-1, epoch=130, batch=5 train loss <loss>=1.1555495758851368\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:36:48 INFO 140512987567936] Epoch[130] Batch [5]#011Speed: 296.03 samples/sec#011loss=1.155550\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:36:49 INFO 140512987567936] processed a total of 631 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742060203.4699852, \"EndTime\": 1742060209.6962512, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6225.749731063843, \"count\": 1, \"min\": 6225.749731063843, \"max\": 6225.749731063843}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:36:49 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=101.35159766851245 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:36:49 INFO 140512987567936] #progress_metric: host=algo-1, completed 65.5 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:36:49 INFO 140512987567936] #quality_metric: host=algo-1, epoch=130, train loss <loss>=1.2798386991024018\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:36:49 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:36:53 INFO 140512987567936] Epoch[131] Batch[0] avg_epoch_loss=0.514065\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:36:53 INFO 140512987567936] #quality_metric: host=algo-1, epoch=131, batch=0 train loss <loss>=0.514064610004425\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:36:54 INFO 140512987567936] Epoch[131] Batch[5] avg_epoch_loss=1.138479\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:36:54 INFO 140512987567936] #quality_metric: host=algo-1, epoch=131, batch=5 train loss <loss>=1.1384792228539784\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:36:54 INFO 140512987567936] Epoch[131] Batch [5]#011Speed: 299.65 samples/sec#011loss=1.138479\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:36:55 INFO 140512987567936] processed a total of 629 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742060209.6963203, \"EndTime\": 1742060215.979397, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6281.760215759277, \"count\": 1, \"min\": 6281.760215759277, \"max\": 6281.760215759277}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:36:55 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=100.12955105044254 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:36:55 INFO 140512987567936] #progress_metric: host=algo-1, completed 66.0 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:36:55 INFO 140512987567936] #quality_metric: host=algo-1, epoch=131, train loss <loss>=1.2224976241588592\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:36:55 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:37:00 INFO 140512987567936] Epoch[132] Batch[0] avg_epoch_loss=1.088778\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:37:00 INFO 140512987567936] #quality_metric: host=algo-1, epoch=132, batch=0 train loss <loss>=1.0887782573699951\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:37:01 INFO 140512987567936] Epoch[132] Batch[5] avg_epoch_loss=0.901521\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:37:01 INFO 140512987567936] #quality_metric: host=algo-1, epoch=132, batch=5 train loss <loss>=0.9015208085378011\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:37:01 INFO 140512987567936] Epoch[132] Batch [5]#011Speed: 299.13 samples/sec#011loss=0.901521\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:37:02 INFO 140512987567936] Epoch[132] Batch[10] avg_epoch_loss=1.264482\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:37:02 INFO 140512987567936] #quality_metric: host=algo-1, epoch=132, batch=10 train loss <loss>=1.7000346302986145\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:37:02 INFO 140512987567936] Epoch[132] Batch [10]#011Speed: 232.99 samples/sec#011loss=1.700035\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:37:02 INFO 140512987567936] processed a total of 666 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742060215.9794652, \"EndTime\": 1742060222.5141819, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6534.28840637207, \"count\": 1, \"min\": 6534.28840637207, \"max\": 6534.28840637207}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:37:02 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=101.92223206471692 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:37:02 INFO 140512987567936] #progress_metric: host=algo-1, completed 66.5 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:37:02 INFO 140512987567936] #quality_metric: host=algo-1, epoch=132, train loss <loss>=1.2644816366108982\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:37:02 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:37:06 INFO 140512987567936] Epoch[133] Batch[0] avg_epoch_loss=0.842855\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:37:06 INFO 140512987567936] #quality_metric: host=algo-1, epoch=133, batch=0 train loss <loss>=0.8428552746772766\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:37:07 INFO 140512987567936] Epoch[133] Batch[5] avg_epoch_loss=1.073369\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:37:07 INFO 140512987567936] #quality_metric: host=algo-1, epoch=133, batch=5 train loss <loss>=1.0733691553274791\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:37:07 INFO 140512987567936] Epoch[133] Batch [5]#011Speed: 310.09 samples/sec#011loss=1.073369\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:37:08 INFO 140512987567936] processed a total of 636 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742060222.51425, \"EndTime\": 1742060228.6602893, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6145.615577697754, \"count\": 1, \"min\": 6145.615577697754, \"max\": 6145.615577697754}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:37:08 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=103.48659783639947 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:37:08 INFO 140512987567936] #progress_metric: host=algo-1, completed 67.0 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:37:08 INFO 140512987567936] #quality_metric: host=algo-1, epoch=133, train loss <loss>=1.1104653120040893\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:37:08 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:37:12 INFO 140512987567936] Epoch[134] Batch[0] avg_epoch_loss=1.043660\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:37:12 INFO 140512987567936] #quality_metric: host=algo-1, epoch=134, batch=0 train loss <loss>=1.0436596870422363\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:37:13 INFO 140512987567936] Epoch[134] Batch[5] avg_epoch_loss=0.946228\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:37:13 INFO 140512987567936] #quality_metric: host=algo-1, epoch=134, batch=5 train loss <loss>=0.9462283849716187\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:37:13 INFO 140512987567936] Epoch[134] Batch [5]#011Speed: 292.68 samples/sec#011loss=0.946228\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:37:14 INFO 140512987567936] processed a total of 613 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742060228.6603649, \"EndTime\": 1742060234.864219, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6203.416347503662, \"count\": 1, \"min\": 6203.416347503662, \"max\": 6203.416347503662}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:37:14 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=98.81481607882591 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:37:14 INFO 140512987567936] #progress_metric: host=algo-1, completed 67.5 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:37:14 INFO 140512987567936] #quality_metric: host=algo-1, epoch=134, train loss <loss>=0.9853561252355576\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:37:14 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:37:18 INFO 140512987567936] Epoch[135] Batch[0] avg_epoch_loss=0.789059\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:37:18 INFO 140512987567936] #quality_metric: host=algo-1, epoch=135, batch=0 train loss <loss>=0.7890588641166687\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:37:19 INFO 140512987567936] Epoch[135] Batch[5] avg_epoch_loss=0.837682\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:37:19 INFO 140512987567936] #quality_metric: host=algo-1, epoch=135, batch=5 train loss <loss>=0.8376822421948115\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:37:19 INFO 140512987567936] Epoch[135] Batch [5]#011Speed: 316.81 samples/sec#011loss=0.837682\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:37:20 INFO 140512987567936] processed a total of 628 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742060234.8642902, \"EndTime\": 1742060240.952096, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6087.3565673828125, \"count\": 1, \"min\": 6087.3565673828125, \"max\": 6087.3565673828125}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:37:20 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=103.16287866889868 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:37:20 INFO 140512987567936] #progress_metric: host=algo-1, completed 68.0 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:37:20 INFO 140512987567936] #quality_metric: host=algo-1, epoch=135, train loss <loss>=0.9342274218797684\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:37:20 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:37:24 INFO 140512987567936] Epoch[136] Batch[0] avg_epoch_loss=0.616746\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:37:24 INFO 140512987567936] #quality_metric: host=algo-1, epoch=136, batch=0 train loss <loss>=0.6167458295822144\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:37:26 INFO 140512987567936] Epoch[136] Batch[5] avg_epoch_loss=0.971025\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:37:26 INFO 140512987567936] #quality_metric: host=algo-1, epoch=136, batch=5 train loss <loss>=0.9710248907407125\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:37:26 INFO 140512987567936] Epoch[136] Batch [5]#011Speed: 305.58 samples/sec#011loss=0.971025\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:37:27 INFO 140512987567936] Epoch[136] Batch[10] avg_epoch_loss=0.950486\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:37:27 INFO 140512987567936] #quality_metric: host=algo-1, epoch=136, batch=10 train loss <loss>=0.9258384227752685\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:37:27 INFO 140512987567936] Epoch[136] Batch [10]#011Speed: 234.08 samples/sec#011loss=0.925838\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:37:27 INFO 140512987567936] processed a total of 658 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742060240.9521666, \"EndTime\": 1742060247.3967152, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6444.136142730713, \"count\": 1, \"min\": 6444.136142730713, \"max\": 6444.136142730713}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:37:27 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=102.10072147275905 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:37:27 INFO 140512987567936] #progress_metric: host=algo-1, completed 68.5 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:37:27 INFO 140512987567936] #quality_metric: host=algo-1, epoch=136, train loss <loss>=0.9504855871200562\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:37:27 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:37:31 INFO 140512987567936] Epoch[137] Batch[0] avg_epoch_loss=1.366011\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:37:31 INFO 140512987567936] #quality_metric: host=algo-1, epoch=137, batch=0 train loss <loss>=1.3660105466842651\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:37:32 INFO 140512987567936] Epoch[137] Batch[5] avg_epoch_loss=0.994641\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:37:32 INFO 140512987567936] #quality_metric: host=algo-1, epoch=137, batch=5 train loss <loss>=0.9946405539909998\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:37:32 INFO 140512987567936] Epoch[137] Batch [5]#011Speed: 309.16 samples/sec#011loss=0.994641\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:37:33 INFO 140512987567936] processed a total of 636 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742060247.3971622, \"EndTime\": 1742060253.5940037, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6196.44832611084, \"count\": 1, \"min\": 6196.44832611084, \"max\": 6196.44832611084}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:37:33 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=102.6378349219773 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:37:33 INFO 140512987567936] #progress_metric: host=algo-1, completed 69.0 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:37:33 INFO 140512987567936] #quality_metric: host=algo-1, epoch=137, train loss <loss>=0.940290555357933\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:37:33 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:37:37 INFO 140512987567936] Epoch[138] Batch[0] avg_epoch_loss=0.841654\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:37:37 INFO 140512987567936] #quality_metric: host=algo-1, epoch=138, batch=0 train loss <loss>=0.84165358543396\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:37:38 INFO 140512987567936] Epoch[138] Batch[5] avg_epoch_loss=1.104219\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:37:38 INFO 140512987567936] #quality_metric: host=algo-1, epoch=138, batch=5 train loss <loss>=1.1042190194129944\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:37:38 INFO 140512987567936] Epoch[138] Batch [5]#011Speed: 307.73 samples/sec#011loss=1.104219\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:37:39 INFO 140512987567936] Epoch[138] Batch[10] avg_epoch_loss=1.274909\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:37:39 INFO 140512987567936] #quality_metric: host=algo-1, epoch=138, batch=10 train loss <loss>=1.4797362208366394\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:37:39 INFO 140512987567936] Epoch[138] Batch [10]#011Speed: 249.82 samples/sec#011loss=1.479736\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:37:39 INFO 140512987567936] processed a total of 657 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742060253.5940702, \"EndTime\": 1742060259.9807937, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6386.390924453735, \"count\": 1, \"min\": 6386.390924453735, \"max\": 6386.390924453735}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:37:39 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=102.87331929065039 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:37:39 INFO 140512987567936] #progress_metric: host=algo-1, completed 69.5 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:37:39 INFO 140512987567936] #quality_metric: host=algo-1, epoch=138, train loss <loss>=1.2749086564237422\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:37:39 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:37:44 INFO 140512987567936] Epoch[139] Batch[0] avg_epoch_loss=1.729348\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:37:44 INFO 140512987567936] #quality_metric: host=algo-1, epoch=139, batch=0 train loss <loss>=1.7293481826782227\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:37:45 INFO 140512987567936] Epoch[139] Batch[5] avg_epoch_loss=1.051260\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:37:45 INFO 140512987567936] #quality_metric: host=algo-1, epoch=139, batch=5 train loss <loss>=1.0512597958246868\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:37:45 INFO 140512987567936] Epoch[139] Batch [5]#011Speed: 295.78 samples/sec#011loss=1.051260\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:37:46 INFO 140512987567936] processed a total of 630 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742060259.980863, \"EndTime\": 1742060266.2151146, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6233.813524246216, \"count\": 1, \"min\": 6233.813524246216, \"max\": 6233.813524246216}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:37:46 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=101.05983554518683 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:37:46 INFO 140512987567936] #progress_metric: host=algo-1, completed 70.0 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:37:46 INFO 140512987567936] #quality_metric: host=algo-1, epoch=139, train loss <loss>=0.9066210746765136\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:37:46 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:37:50 INFO 140512987567936] Epoch[140] Batch[0] avg_epoch_loss=1.079302\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:37:50 INFO 140512987567936] #quality_metric: host=algo-1, epoch=140, batch=0 train loss <loss>=1.0793017148971558\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:37:51 INFO 140512987567936] Epoch[140] Batch[5] avg_epoch_loss=1.038997\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:37:51 INFO 140512987567936] #quality_metric: host=algo-1, epoch=140, batch=5 train loss <loss>=1.0389966865380604\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:37:51 INFO 140512987567936] Epoch[140] Batch [5]#011Speed: 307.77 samples/sec#011loss=1.038997\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:37:52 INFO 140512987567936] processed a total of 624 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742060266.2151952, \"EndTime\": 1742060272.4493947, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6232.895135879517, \"count\": 1, \"min\": 6232.895135879517, \"max\": 6232.895135879517}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:37:52 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=100.11237758614345 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:37:52 INFO 140512987567936] #progress_metric: host=algo-1, completed 70.5 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:37:52 INFO 140512987567936] #quality_metric: host=algo-1, epoch=140, train loss <loss>=0.9933443188667297\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:37:52 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:37:56 INFO 140512987567936] Epoch[141] Batch[0] avg_epoch_loss=0.642764\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:37:56 INFO 140512987567936] #quality_metric: host=algo-1, epoch=141, batch=0 train loss <loss>=0.6427639126777649\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:37:57 INFO 140512987567936] Epoch[141] Batch[5] avg_epoch_loss=0.966883\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:37:57 INFO 140512987567936] #quality_metric: host=algo-1, epoch=141, batch=5 train loss <loss>=0.9668828745683035\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:37:57 INFO 140512987567936] Epoch[141] Batch [5]#011Speed: 291.77 samples/sec#011loss=0.966883\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:37:59 INFO 140512987567936] Epoch[141] Batch[10] avg_epoch_loss=0.851674\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:37:59 INFO 140512987567936] #quality_metric: host=algo-1, epoch=141, batch=10 train loss <loss>=0.7134242713451385\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:37:59 INFO 140512987567936] Epoch[141] Batch [10]#011Speed: 203.71 samples/sec#011loss=0.713424\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:37:59 INFO 140512987567936] processed a total of 676 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742060272.4494622, \"EndTime\": 1742060279.1259801, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6676.203012466431, \"count\": 1, \"min\": 6676.203012466431, \"max\": 6676.203012466431}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:37:59 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=101.25374914985011 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:37:59 INFO 140512987567936] #progress_metric: host=algo-1, completed 71.0 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:37:59 INFO 140512987567936] #quality_metric: host=algo-1, epoch=141, train loss <loss>=0.8516744185577739\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:37:59 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:38:03 INFO 140512987567936] Epoch[142] Batch[0] avg_epoch_loss=0.977336\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:38:03 INFO 140512987567936] #quality_metric: host=algo-1, epoch=142, batch=0 train loss <loss>=0.9773359298706055\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:38:04 INFO 140512987567936] Epoch[142] Batch[5] avg_epoch_loss=1.105043\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:38:04 INFO 140512987567936] #quality_metric: host=algo-1, epoch=142, batch=5 train loss <loss>=1.1050425668557484\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:38:04 INFO 140512987567936] Epoch[142] Batch [5]#011Speed: 320.86 samples/sec#011loss=1.105043\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:38:05 INFO 140512987567936] processed a total of 623 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742060279.126043, \"EndTime\": 1742060285.2764478, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6150.1100063323975, \"count\": 1, \"min\": 6150.1100063323975, \"max\": 6150.1100063323975}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:38:05 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=101.29712397419296 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:38:05 INFO 140512987567936] #progress_metric: host=algo-1, completed 71.5 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:38:05 INFO 140512987567936] #quality_metric: host=algo-1, epoch=142, train loss <loss>=1.0159874171018601\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:38:05 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:38:09 INFO 140512987567936] Epoch[143] Batch[0] avg_epoch_loss=1.015218\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:38:09 INFO 140512987567936] #quality_metric: host=algo-1, epoch=143, batch=0 train loss <loss>=1.0152181386947632\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:38:10 INFO 140512987567936] Epoch[143] Batch[5] avg_epoch_loss=1.002063\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:38:10 INFO 140512987567936] #quality_metric: host=algo-1, epoch=143, batch=5 train loss <loss>=1.0020630260308583\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:38:10 INFO 140512987567936] Epoch[143] Batch [5]#011Speed: 296.32 samples/sec#011loss=1.002063\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:38:11 INFO 140512987567936] processed a total of 599 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742060285.2765272, \"EndTime\": 1742060291.517302, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6240.339994430542, \"count\": 1, \"min\": 6240.339994430542, \"max\": 6240.339994430542}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:38:11 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=95.9867790995401 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:38:11 INFO 140512987567936] #progress_metric: host=algo-1, completed 72.0 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:38:11 INFO 140512987567936] #quality_metric: host=algo-1, epoch=143, train loss <loss>=1.2531796157360078\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:38:11 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:38:15 INFO 140512987567936] Epoch[144] Batch[0] avg_epoch_loss=1.303413\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:38:15 INFO 140512987567936] #quality_metric: host=algo-1, epoch=144, batch=0 train loss <loss>=1.3034125566482544\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:38:16 INFO 140512987567936] Epoch[144] Batch[5] avg_epoch_loss=1.053814\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:38:16 INFO 140512987567936] #quality_metric: host=algo-1, epoch=144, batch=5 train loss <loss>=1.0538136065006256\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:38:16 INFO 140512987567936] Epoch[144] Batch [5]#011Speed: 319.14 samples/sec#011loss=1.053814\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:38:17 INFO 140512987567936] Epoch[144] Batch[10] avg_epoch_loss=1.353854\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:38:17 INFO 140512987567936] #quality_metric: host=algo-1, epoch=144, batch=10 train loss <loss>=1.7139015436172484\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:38:17 INFO 140512987567936] Epoch[144] Batch [10]#011Speed: 264.35 samples/sec#011loss=1.713902\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:38:17 INFO 140512987567936] processed a total of 654 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742060291.517371, \"EndTime\": 1742060297.8284676, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6310.685396194458, \"count\": 1, \"min\": 6310.685396194458, \"max\": 6310.685396194458}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:38:17 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=103.6320558641261 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:38:17 INFO 140512987567936] #progress_metric: host=algo-1, completed 72.5 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:38:17 INFO 140512987567936] #quality_metric: host=algo-1, epoch=144, train loss <loss>=1.3538535779172725\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:38:17 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:38:21 INFO 140512987567936] Epoch[145] Batch[0] avg_epoch_loss=0.915269\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:38:21 INFO 140512987567936] #quality_metric: host=algo-1, epoch=145, batch=0 train loss <loss>=0.9152687788009644\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:38:23 INFO 140512987567936] Epoch[145] Batch[5] avg_epoch_loss=0.834115\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:38:23 INFO 140512987567936] #quality_metric: host=algo-1, epoch=145, batch=5 train loss <loss>=0.8341147899627686\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:38:23 INFO 140512987567936] Epoch[145] Batch [5]#011Speed: 283.45 samples/sec#011loss=0.834115\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:38:24 INFO 140512987567936] processed a total of 627 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742060297.8285377, \"EndTime\": 1742060304.1368032, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6307.90376663208, \"count\": 1, \"min\": 6307.90376663208, \"max\": 6307.90376663208}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:38:24 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=99.39752886930475 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:38:24 INFO 140512987567936] #progress_metric: host=algo-1, completed 73.0 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:38:24 INFO 140512987567936] #quality_metric: host=algo-1, epoch=145, train loss <loss>=0.8478517174720764\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:38:24 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:38:28 INFO 140512987567936] Epoch[146] Batch[0] avg_epoch_loss=0.802456\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:38:28 INFO 140512987567936] #quality_metric: host=algo-1, epoch=146, batch=0 train loss <loss>=0.8024563193321228\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:38:29 INFO 140512987567936] Epoch[146] Batch[5] avg_epoch_loss=0.769517\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:38:29 INFO 140512987567936] #quality_metric: host=algo-1, epoch=146, batch=5 train loss <loss>=0.7695171038309733\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:38:29 INFO 140512987567936] Epoch[146] Batch [5]#011Speed: 288.86 samples/sec#011loss=0.769517\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:38:30 INFO 140512987567936] Epoch[146] Batch[10] avg_epoch_loss=0.823323\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:38:30 INFO 140512987567936] #quality_metric: host=algo-1, epoch=146, batch=10 train loss <loss>=0.8878895737230778\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:38:30 INFO 140512987567936] Epoch[146] Batch [10]#011Speed: 234.13 samples/sec#011loss=0.887890\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:38:30 INFO 140512987567936] processed a total of 664 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742060304.136871, \"EndTime\": 1742060310.6862142, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6548.895359039307, \"count\": 1, \"min\": 6548.895359039307, \"max\": 6548.895359039307}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:38:30 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=101.38968714401207 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:38:30 INFO 140512987567936] #progress_metric: host=algo-1, completed 73.5 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:38:30 INFO 140512987567936] #quality_metric: host=algo-1, epoch=146, train loss <loss>=0.823322771963748\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:38:30 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:38:34 INFO 140512987567936] Epoch[147] Batch[0] avg_epoch_loss=0.841400\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:38:34 INFO 140512987567936] #quality_metric: host=algo-1, epoch=147, batch=0 train loss <loss>=0.8413999080657959\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:38:35 INFO 140512987567936] Epoch[147] Batch[5] avg_epoch_loss=0.976337\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:38:35 INFO 140512987567936] #quality_metric: host=algo-1, epoch=147, batch=5 train loss <loss>=0.9763373484214147\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:38:35 INFO 140512987567936] Epoch[147] Batch [5]#011Speed: 295.26 samples/sec#011loss=0.976337\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:38:36 INFO 140512987567936] processed a total of 585 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742060310.686278, \"EndTime\": 1742060316.8327715, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6146.193504333496, \"count\": 1, \"min\": 6146.193504333496, \"max\": 6146.193504333496}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:38:36 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=95.1786326619086 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:38:36 INFO 140512987567936] #progress_metric: host=algo-1, completed 74.0 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:38:36 INFO 140512987567936] #quality_metric: host=algo-1, epoch=147, train loss <loss>=0.8084507398307323\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:38:36 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:38:40 INFO 140512987567936] Epoch[148] Batch[0] avg_epoch_loss=1.273454\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:38:40 INFO 140512987567936] #quality_metric: host=algo-1, epoch=148, batch=0 train loss <loss>=1.273453950881958\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:38:41 INFO 140512987567936] Epoch[148] Batch[5] avg_epoch_loss=1.011626\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:38:41 INFO 140512987567936] #quality_metric: host=algo-1, epoch=148, batch=5 train loss <loss>=1.0116261045138042\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:38:41 INFO 140512987567936] Epoch[148] Batch [5]#011Speed: 302.60 samples/sec#011loss=1.011626\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:38:43 INFO 140512987567936] Epoch[148] Batch[10] avg_epoch_loss=0.927284\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:38:43 INFO 140512987567936] #quality_metric: host=algo-1, epoch=148, batch=10 train loss <loss>=0.8260727912187577\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:38:43 INFO 140512987567936] Epoch[148] Batch [10]#011Speed: 238.96 samples/sec#011loss=0.826073\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:38:43 INFO 140512987567936] processed a total of 657 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742060316.8328786, \"EndTime\": 1742060323.3281229, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6494.728803634644, \"count\": 1, \"min\": 6494.728803634644, \"max\": 6494.728803634644}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:38:43 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=101.15744689473775 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:38:43 INFO 140512987567936] #progress_metric: host=algo-1, completed 74.5 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:38:43 INFO 140512987567936] #quality_metric: host=algo-1, epoch=148, train loss <loss>=0.9272836893796921\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:38:43 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:38:47 INFO 140512987567936] Epoch[149] Batch[0] avg_epoch_loss=0.880381\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:38:47 INFO 140512987567936] #quality_metric: host=algo-1, epoch=149, batch=0 train loss <loss>=0.8803813457489014\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:38:48 INFO 140512987567936] Epoch[149] Batch[5] avg_epoch_loss=0.849142\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:38:48 INFO 140512987567936] #quality_metric: host=algo-1, epoch=149, batch=5 train loss <loss>=0.8491422782341639\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:38:48 INFO 140512987567936] Epoch[149] Batch [5]#011Speed: 294.85 samples/sec#011loss=0.849142\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:38:49 INFO 140512987567936] processed a total of 633 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742060323.328186, \"EndTime\": 1742060329.6145153, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6285.9296798706055, \"count\": 1, \"min\": 6285.9296798706055, \"max\": 6285.9296798706055}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:38:49 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=100.69863501670956 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:38:49 INFO 140512987567936] #progress_metric: host=algo-1, completed 75.0 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:38:49 INFO 140512987567936] #quality_metric: host=algo-1, epoch=149, train loss <loss>=0.8343546956777572\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:38:49 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:38:53 INFO 140512987567936] Epoch[150] Batch[0] avg_epoch_loss=1.134704\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:38:53 INFO 140512987567936] #quality_metric: host=algo-1, epoch=150, batch=0 train loss <loss>=1.1347038745880127\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:38:54 INFO 140512987567936] Epoch[150] Batch[5] avg_epoch_loss=0.928674\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:38:54 INFO 140512987567936] #quality_metric: host=algo-1, epoch=150, batch=5 train loss <loss>=0.9286739975214005\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:38:54 INFO 140512987567936] Epoch[150] Batch [5]#011Speed: 298.80 samples/sec#011loss=0.928674\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:38:56 INFO 140512987567936] Epoch[150] Batch[10] avg_epoch_loss=0.807961\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:38:56 INFO 140512987567936] #quality_metric: host=algo-1, epoch=150, batch=10 train loss <loss>=0.6631048798561097\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:38:56 INFO 140512987567936] Epoch[150] Batch [10]#011Speed: 222.44 samples/sec#011loss=0.663105\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:38:56 INFO 140512987567936] processed a total of 674 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742060329.6145875, \"EndTime\": 1742060336.1579256, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6543.004989624023, \"count\": 1, \"min\": 6543.004989624023, \"max\": 6543.004989624023}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:38:56 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=103.0092675735713 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:38:56 INFO 140512987567936] #progress_metric: host=algo-1, completed 75.5 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:38:56 INFO 140512987567936] #quality_metric: host=algo-1, epoch=150, train loss <loss>=0.8079607622189955\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:38:56 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:39:00 INFO 140512987567936] Epoch[151] Batch[0] avg_epoch_loss=0.604054\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:39:00 INFO 140512987567936] #quality_metric: host=algo-1, epoch=151, batch=0 train loss <loss>=0.6040544509887695\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:39:01 INFO 140512987567936] Epoch[151] Batch[5] avg_epoch_loss=1.022617\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:39:01 INFO 140512987567936] #quality_metric: host=algo-1, epoch=151, batch=5 train loss <loss>=1.0226171414057414\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:39:01 INFO 140512987567936] Epoch[151] Batch [5]#011Speed: 318.68 samples/sec#011loss=1.022617\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:39:02 INFO 140512987567936] Epoch[151] Batch[10] avg_epoch_loss=1.171446\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:39:02 INFO 140512987567936] #quality_metric: host=algo-1, epoch=151, batch=10 train loss <loss>=1.3500412702560425\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:39:02 INFO 140512987567936] Epoch[151] Batch [10]#011Speed: 219.76 samples/sec#011loss=1.350041\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:39:02 INFO 140512987567936] processed a total of 656 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742060336.157989, \"EndTime\": 1742060342.7521567, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6593.717575073242, \"count\": 1, \"min\": 6593.717575073242, \"max\": 6593.717575073242}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:39:02 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=99.48723794807185 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:39:02 INFO 140512987567936] #progress_metric: host=algo-1, completed 76.0 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:39:02 INFO 140512987567936] #quality_metric: host=algo-1, epoch=151, train loss <loss>=1.1714462908831509\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:39:02 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:39:06 INFO 140512987567936] Epoch[152] Batch[0] avg_epoch_loss=1.510121\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:39:06 INFO 140512987567936] #quality_metric: host=algo-1, epoch=152, batch=0 train loss <loss>=1.510121464729309\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:39:07 INFO 140512987567936] Epoch[152] Batch[5] avg_epoch_loss=1.006346\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:39:07 INFO 140512987567936] #quality_metric: host=algo-1, epoch=152, batch=5 train loss <loss>=1.006346081693967\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:39:07 INFO 140512987567936] Epoch[152] Batch [5]#011Speed: 311.87 samples/sec#011loss=1.006346\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:39:08 INFO 140512987567936] processed a total of 601 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742060342.7522197, \"EndTime\": 1742060348.8598018, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6107.288122177124, \"count\": 1, \"min\": 6107.288122177124, \"max\": 6107.288122177124}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:39:08 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=98.40538333326566 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:39:08 INFO 140512987567936] #progress_metric: host=algo-1, completed 76.5 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:39:08 INFO 140512987567936] #quality_metric: host=algo-1, epoch=152, train loss <loss>=1.3031404346227646\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:39:08 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:39:12 INFO 140512987567936] Epoch[153] Batch[0] avg_epoch_loss=0.856648\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:39:12 INFO 140512987567936] #quality_metric: host=algo-1, epoch=153, batch=0 train loss <loss>=0.8566478490829468\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:39:14 INFO 140512987567936] Epoch[153] Batch[5] avg_epoch_loss=1.096013\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:39:14 INFO 140512987567936] #quality_metric: host=algo-1, epoch=153, batch=5 train loss <loss>=1.096013218164444\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:39:14 INFO 140512987567936] Epoch[153] Batch [5]#011Speed: 303.46 samples/sec#011loss=1.096013\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:39:15 INFO 140512987567936] processed a total of 609 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742060348.8598723, \"EndTime\": 1742060355.0390248, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6178.813457489014, \"count\": 1, \"min\": 6178.813457489014, \"max\": 6178.813457489014}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:39:15 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=98.56097391856763 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:39:15 INFO 140512987567936] #progress_metric: host=algo-1, completed 77.0 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:39:15 INFO 140512987567936] #quality_metric: host=algo-1, epoch=153, train loss <loss>=0.9166034758090973\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:39:15 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:39:19 INFO 140512987567936] Epoch[154] Batch[0] avg_epoch_loss=0.555238\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:39:19 INFO 140512987567936] #quality_metric: host=algo-1, epoch=154, batch=0 train loss <loss>=0.5552381277084351\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:39:20 INFO 140512987567936] Epoch[154] Batch[5] avg_epoch_loss=0.985204\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:39:20 INFO 140512987567936] #quality_metric: host=algo-1, epoch=154, batch=5 train loss <loss>=0.9852041800816854\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:39:20 INFO 140512987567936] Epoch[154] Batch [5]#011Speed: 303.07 samples/sec#011loss=0.985204\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:39:21 INFO 140512987567936] Epoch[154] Batch[10] avg_epoch_loss=0.898384\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:39:21 INFO 140512987567936] #quality_metric: host=algo-1, epoch=154, batch=10 train loss <loss>=0.7941991180181504\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:39:21 INFO 140512987567936] Epoch[154] Batch [10]#011Speed: 249.97 samples/sec#011loss=0.794199\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:39:21 INFO 140512987567936] processed a total of 644 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742060355.0390959, \"EndTime\": 1742060361.513172, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6473.737716674805, \"count\": 1, \"min\": 6473.737716674805, \"max\": 6473.737716674805}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:39:21 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=99.47704643230144 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:39:21 INFO 140512987567936] #progress_metric: host=algo-1, completed 77.5 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:39:21 INFO 140512987567936] #quality_metric: host=algo-1, epoch=154, train loss <loss>=0.8983836973255331\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:39:21 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:39:25 INFO 140512987567936] Epoch[155] Batch[0] avg_epoch_loss=0.935325\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:39:25 INFO 140512987567936] #quality_metric: host=algo-1, epoch=155, batch=0 train loss <loss>=0.9353254437446594\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:39:26 INFO 140512987567936] Epoch[155] Batch[5] avg_epoch_loss=0.934431\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:39:26 INFO 140512987567936] #quality_metric: host=algo-1, epoch=155, batch=5 train loss <loss>=0.9344309071699778\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:39:26 INFO 140512987567936] Epoch[155] Batch [5]#011Speed: 301.12 samples/sec#011loss=0.934431\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:39:27 INFO 140512987567936] Epoch[155] Batch[10] avg_epoch_loss=1.096083\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:39:27 INFO 140512987567936] #quality_metric: host=algo-1, epoch=155, batch=10 train loss <loss>=1.290065598487854\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:39:27 INFO 140512987567936] Epoch[155] Batch [10]#011Speed: 243.07 samples/sec#011loss=1.290066\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:39:27 INFO 140512987567936] processed a total of 668 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742060361.513255, \"EndTime\": 1742060367.9491618, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6435.49919128418, \"count\": 1, \"min\": 6435.49919128418, \"max\": 6435.49919128418}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:39:27 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=103.79774038538304 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:39:27 INFO 140512987567936] #progress_metric: host=algo-1, completed 78.0 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:39:27 INFO 140512987567936] #quality_metric: host=algo-1, epoch=155, train loss <loss>=1.0960830395871943\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:39:27 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:39:32 INFO 140512987567936] Epoch[156] Batch[0] avg_epoch_loss=1.352135\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:39:32 INFO 140512987567936] #quality_metric: host=algo-1, epoch=156, batch=0 train loss <loss>=1.3521345853805542\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:39:33 INFO 140512987567936] Epoch[156] Batch[5] avg_epoch_loss=1.001298\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:39:33 INFO 140512987567936] #quality_metric: host=algo-1, epoch=156, batch=5 train loss <loss>=1.0012981792291005\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:39:33 INFO 140512987567936] Epoch[156] Batch [5]#011Speed: 308.56 samples/sec#011loss=1.001298\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:39:34 INFO 140512987567936] processed a total of 625 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742060367.9492242, \"EndTime\": 1742060374.119386, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6169.870615005493, \"count\": 1, \"min\": 6169.870615005493, \"max\": 6169.870615005493}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:39:34 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=101.29706489528701 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:39:34 INFO 140512987567936] #progress_metric: host=algo-1, completed 78.5 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:39:34 INFO 140512987567936] #quality_metric: host=algo-1, epoch=156, train loss <loss>=1.1499646067619325\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:39:34 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:39:38 INFO 140512987567936] Epoch[157] Batch[0] avg_epoch_loss=0.932607\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:39:38 INFO 140512987567936] #quality_metric: host=algo-1, epoch=157, batch=0 train loss <loss>=0.9326067566871643\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:39:39 INFO 140512987567936] Epoch[157] Batch[5] avg_epoch_loss=1.013488\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:39:39 INFO 140512987567936] #quality_metric: host=algo-1, epoch=157, batch=5 train loss <loss>=1.013487696647644\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:39:39 INFO 140512987567936] Epoch[157] Batch [5]#011Speed: 298.89 samples/sec#011loss=1.013488\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:39:40 INFO 140512987567936] processed a total of 627 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742060374.1194532, \"EndTime\": 1742060380.2989938, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6179.055213928223, \"count\": 1, \"min\": 6179.055213928223, \"max\": 6179.055213928223}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:39:40 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=101.4701673506446 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:39:40 INFO 140512987567936] #progress_metric: host=algo-1, completed 79.0 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:39:40 INFO 140512987567936] #quality_metric: host=algo-1, epoch=157, train loss <loss>=1.0799790740013122\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:39:40 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:39:44 INFO 140512987567936] Epoch[158] Batch[0] avg_epoch_loss=0.956799\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:39:44 INFO 140512987567936] #quality_metric: host=algo-1, epoch=158, batch=0 train loss <loss>=0.9567992091178894\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:39:45 INFO 140512987567936] Epoch[158] Batch[5] avg_epoch_loss=0.872354\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:39:45 INFO 140512987567936] #quality_metric: host=algo-1, epoch=158, batch=5 train loss <loss>=0.8723544279734293\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:39:45 INFO 140512987567936] Epoch[158] Batch [5]#011Speed: 299.00 samples/sec#011loss=0.872354\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:39:46 INFO 140512987567936] processed a total of 618 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742060380.2990613, \"EndTime\": 1742060386.511579, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6212.143421173096, \"count\": 1, \"min\": 6212.143421173096, \"max\": 6212.143421173096}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:39:46 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=99.48092560004012 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:39:46 INFO 140512987567936] #progress_metric: host=algo-1, completed 79.5 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:39:46 INFO 140512987567936] #quality_metric: host=algo-1, epoch=158, train loss <loss>=0.8387643277645112\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:39:46 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:39:50 INFO 140512987567936] Epoch[159] Batch[0] avg_epoch_loss=1.186976\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:39:50 INFO 140512987567936] #quality_metric: host=algo-1, epoch=159, batch=0 train loss <loss>=1.1869758367538452\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:39:51 INFO 140512987567936] Epoch[159] Batch[5] avg_epoch_loss=0.934980\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:39:51 INFO 140512987567936] #quality_metric: host=algo-1, epoch=159, batch=5 train loss <loss>=0.9349800745646158\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:39:51 INFO 140512987567936] Epoch[159] Batch [5]#011Speed: 312.65 samples/sec#011loss=0.934980\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:39:52 INFO 140512987567936] Epoch[159] Batch[10] avg_epoch_loss=0.766256\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:39:52 INFO 140512987567936] #quality_metric: host=algo-1, epoch=159, batch=10 train loss <loss>=0.5637870401144027\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:39:52 INFO 140512987567936] Epoch[159] Batch [10]#011Speed: 257.65 samples/sec#011loss=0.563787\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:39:52 INFO 140512987567936] processed a total of 644 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742060386.5116508, \"EndTime\": 1742060392.867863, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6355.757474899292, \"count\": 1, \"min\": 6355.757474899292, \"max\": 6355.757474899292}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:39:52 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=101.32374937200909 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:39:52 INFO 140512987567936] #progress_metric: host=algo-1, completed 80.0 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:39:52 INFO 140512987567936] #quality_metric: host=algo-1, epoch=159, train loss <loss>=0.7662559679963372\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:39:52 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:39:56 INFO 140512987567936] Epoch[160] Batch[0] avg_epoch_loss=1.263260\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:39:56 INFO 140512987567936] #quality_metric: host=algo-1, epoch=160, batch=0 train loss <loss>=1.2632596492767334\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:39:58 INFO 140512987567936] Epoch[160] Batch[5] avg_epoch_loss=1.082155\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:39:58 INFO 140512987567936] #quality_metric: host=algo-1, epoch=160, batch=5 train loss <loss>=1.0821549793084462\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:39:58 INFO 140512987567936] Epoch[160] Batch [5]#011Speed: 305.89 samples/sec#011loss=1.082155\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:39:59 INFO 140512987567936] Epoch[160] Batch[10] avg_epoch_loss=1.145684\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:39:59 INFO 140512987567936] #quality_metric: host=algo-1, epoch=160, batch=10 train loss <loss>=1.2219177722930907\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:39:59 INFO 140512987567936] Epoch[160] Batch [10]#011Speed: 223.53 samples/sec#011loss=1.221918\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:39:59 INFO 140512987567936] processed a total of 668 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742060392.8679357, \"EndTime\": 1742060399.4480686, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6579.733848571777, \"count\": 1, \"min\": 6579.733848571777, \"max\": 6579.733848571777}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:39:59 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=101.5223918540765 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:39:59 INFO 140512987567936] #progress_metric: host=algo-1, completed 80.5 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:39:59 INFO 140512987567936] #quality_metric: host=algo-1, epoch=160, train loss <loss>=1.1456835215741938\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:39:59 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:40:03 INFO 140512987567936] Epoch[161] Batch[0] avg_epoch_loss=0.767239\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:40:03 INFO 140512987567936] #quality_metric: host=algo-1, epoch=161, batch=0 train loss <loss>=0.7672386169433594\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:40:04 INFO 140512987567936] Epoch[161] Batch[5] avg_epoch_loss=1.275867\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:40:04 INFO 140512987567936] #quality_metric: host=algo-1, epoch=161, batch=5 train loss <loss>=1.275867094596227\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:40:04 INFO 140512987567936] Epoch[161] Batch [5]#011Speed: 306.35 samples/sec#011loss=1.275867\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:40:05 INFO 140512987567936] processed a total of 635 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742060399.4481325, \"EndTime\": 1742060405.7807198, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6332.306861877441, \"count\": 1, \"min\": 6332.306861877441, \"max\": 6332.306861877441}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:40:05 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=100.27726233424016 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:40:05 INFO 140512987567936] #progress_metric: host=algo-1, completed 81.0 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:40:05 INFO 140512987567936] #quality_metric: host=algo-1, epoch=161, train loss <loss>=1.1210196673870088\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:40:05 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:40:09 INFO 140512987567936] Epoch[162] Batch[0] avg_epoch_loss=0.921190\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:40:09 INFO 140512987567936] #quality_metric: host=algo-1, epoch=162, batch=0 train loss <loss>=0.9211898446083069\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:40:11 INFO 140512987567936] Epoch[162] Batch[5] avg_epoch_loss=0.756289\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:40:11 INFO 140512987567936] #quality_metric: host=algo-1, epoch=162, batch=5 train loss <loss>=0.756289134422938\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:40:11 INFO 140512987567936] Epoch[162] Batch [5]#011Speed: 288.41 samples/sec#011loss=0.756289\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:40:12 INFO 140512987567936] Epoch[162] Batch[10] avg_epoch_loss=0.744351\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:40:12 INFO 140512987567936] #quality_metric: host=algo-1, epoch=162, batch=10 train loss <loss>=0.7300261080265045\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:40:12 INFO 140512987567936] Epoch[162] Batch [10]#011Speed: 229.46 samples/sec#011loss=0.730026\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:40:12 INFO 140512987567936] processed a total of 647 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742060405.7808223, \"EndTime\": 1742060412.4102435, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6628.73649597168, \"count\": 1, \"min\": 6628.73649597168, \"max\": 6628.73649597168}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:40:12 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=97.60355518584912 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:40:12 INFO 140512987567936] #progress_metric: host=algo-1, completed 81.5 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:40:12 INFO 140512987567936] #quality_metric: host=algo-1, epoch=162, train loss <loss>=0.7443513951518319\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:40:12 INFO 140512987567936] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:40:12 INFO 140512987567936] Saved checkpoint to \"/opt/ml/model/state_4a362fae-bd50-4bd6-b2f5-d11f738b272b-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742060412.4103131, \"EndTime\": 1742060412.424459, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 13.693094253540039, \"count\": 1, \"min\": 13.693094253540039, \"max\": 13.693094253540039}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:40:16 INFO 140512987567936] Epoch[163] Batch[0] avg_epoch_loss=0.609834\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:40:16 INFO 140512987567936] #quality_metric: host=algo-1, epoch=163, batch=0 train loss <loss>=0.6098343729972839\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:40:17 INFO 140512987567936] Epoch[163] Batch[5] avg_epoch_loss=0.751681\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:40:17 INFO 140512987567936] #quality_metric: host=algo-1, epoch=163, batch=5 train loss <loss>=0.7516814768314362\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:40:17 INFO 140512987567936] Epoch[163] Batch [5]#011Speed: 288.77 samples/sec#011loss=0.751681\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:40:19 INFO 140512987567936] Epoch[163] Batch[10] avg_epoch_loss=0.930856\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:40:19 INFO 140512987567936] #quality_metric: host=algo-1, epoch=163, batch=10 train loss <loss>=1.1458654999732971\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:40:19 INFO 140512987567936] Epoch[163] Batch [10]#011Speed: 210.90 samples/sec#011loss=1.145865\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:40:19 INFO 140512987567936] processed a total of 670 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742060412.4245155, \"EndTime\": 1742060419.0457377, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6621.171474456787, \"count\": 1, \"min\": 6621.171474456787, \"max\": 6621.171474456787}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:40:19 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=101.18914244494366 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:40:19 INFO 140512987567936] #progress_metric: host=algo-1, completed 82.0 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:40:19 INFO 140512987567936] #quality_metric: host=algo-1, epoch=163, train loss <loss>=0.9308560328050093\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:40:19 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:40:23 INFO 140512987567936] Epoch[164] Batch[0] avg_epoch_loss=1.172626\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:40:23 INFO 140512987567936] #quality_metric: host=algo-1, epoch=164, batch=0 train loss <loss>=1.1726256608963013\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:40:24 INFO 140512987567936] Epoch[164] Batch[5] avg_epoch_loss=0.988539\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:40:24 INFO 140512987567936] #quality_metric: host=algo-1, epoch=164, batch=5 train loss <loss>=0.9885387321313223\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:40:24 INFO 140512987567936] Epoch[164] Batch [5]#011Speed: 300.63 samples/sec#011loss=0.988539\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:40:25 INFO 140512987567936] processed a total of 623 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742060419.0458007, \"EndTime\": 1742060425.2688768, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6222.764015197754, \"count\": 1, \"min\": 6222.764015197754, \"max\": 6222.764015197754}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:40:25 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=100.11387363264686 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:40:25 INFO 140512987567936] #progress_metric: host=algo-1, completed 82.5 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:40:25 INFO 140512987567936] #quality_metric: host=algo-1, epoch=164, train loss <loss>=0.8345285058021545\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:40:25 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:40:29 INFO 140512987567936] Epoch[165] Batch[0] avg_epoch_loss=0.764596\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:40:29 INFO 140512987567936] #quality_metric: host=algo-1, epoch=165, batch=0 train loss <loss>=0.7645964026451111\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:40:30 INFO 140512987567936] Epoch[165] Batch[5] avg_epoch_loss=0.880630\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:40:30 INFO 140512987567936] #quality_metric: host=algo-1, epoch=165, batch=5 train loss <loss>=0.8806300461292267\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:40:30 INFO 140512987567936] Epoch[165] Batch [5]#011Speed: 305.37 samples/sec#011loss=0.880630\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:40:31 INFO 140512987567936] processed a total of 640 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742060425.268946, \"EndTime\": 1742060431.448106, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6178.688764572144, \"count\": 1, \"min\": 6178.688764572144, \"max\": 6178.688764572144}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:40:31 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=103.58025717000781 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:40:31 INFO 140512987567936] #progress_metric: host=algo-1, completed 83.0 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:40:31 INFO 140512987567936] #quality_metric: host=algo-1, epoch=165, train loss <loss>=0.8786825180053711\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:40:31 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:40:35 INFO 140512987567936] Epoch[166] Batch[0] avg_epoch_loss=1.194450\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:40:35 INFO 140512987567936] #quality_metric: host=algo-1, epoch=166, batch=0 train loss <loss>=1.194449782371521\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:40:36 INFO 140512987567936] Epoch[166] Batch[5] avg_epoch_loss=0.860647\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:40:36 INFO 140512987567936] #quality_metric: host=algo-1, epoch=166, batch=5 train loss <loss>=0.8606466179092725\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:40:36 INFO 140512987567936] Epoch[166] Batch [5]#011Speed: 289.58 samples/sec#011loss=0.860647\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:40:37 INFO 140512987567936] Epoch[166] Batch[10] avg_epoch_loss=0.665885\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:40:37 INFO 140512987567936] #quality_metric: host=algo-1, epoch=166, batch=10 train loss <loss>=0.43217009902000425\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:40:37 INFO 140512987567936] Epoch[166] Batch [10]#011Speed: 254.96 samples/sec#011loss=0.432170\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:40:37 INFO 140512987567936] processed a total of 649 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742060431.4481704, \"EndTime\": 1742060437.9201074, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6471.4953899383545, \"count\": 1, \"min\": 6471.4953899383545, \"max\": 6471.4953899383545}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:40:37 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=100.28436634529854 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:40:37 INFO 140512987567936] #progress_metric: host=algo-1, completed 83.5 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:40:37 INFO 140512987567936] #quality_metric: host=algo-1, epoch=166, train loss <loss>=0.665884563868696\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:40:37 INFO 140512987567936] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:40:37 INFO 140512987567936] Saved checkpoint to \"/opt/ml/model/state_610b4b1b-1bfc-477e-a5f5-2354ea6878d5-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742060437.9201746, \"EndTime\": 1742060437.9333172, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 12.822389602661133, \"count\": 1, \"min\": 12.822389602661133, \"max\": 12.822389602661133}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:40:41 INFO 140512987567936] Epoch[167] Batch[0] avg_epoch_loss=0.988404\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:40:41 INFO 140512987567936] #quality_metric: host=algo-1, epoch=167, batch=0 train loss <loss>=0.9884042739868164\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:40:42 INFO 140512987567936] Epoch[167] Batch[5] avg_epoch_loss=0.919536\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:40:42 INFO 140512987567936] #quality_metric: host=algo-1, epoch=167, batch=5 train loss <loss>=0.9195363620917002\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:40:42 INFO 140512987567936] Epoch[167] Batch [5]#011Speed: 316.91 samples/sec#011loss=0.919536\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:40:44 INFO 140512987567936] Epoch[167] Batch[10] avg_epoch_loss=0.965296\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:40:44 INFO 140512987567936] #quality_metric: host=algo-1, epoch=167, batch=10 train loss <loss>=1.0202074885368346\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:40:44 INFO 140512987567936] Epoch[167] Batch [10]#011Speed: 242.93 samples/sec#011loss=1.020207\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:40:44 INFO 140512987567936] processed a total of 662 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742060437.9333708, \"EndTime\": 1742060444.3034391, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6370.0151443481445, \"count\": 1, \"min\": 6370.0151443481445, \"max\": 6370.0151443481445}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:40:44 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=103.92247433638586 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:40:44 INFO 140512987567936] #progress_metric: host=algo-1, completed 84.0 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:40:44 INFO 140512987567936] #quality_metric: host=algo-1, epoch=167, train loss <loss>=0.9652959650213068\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:40:44 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:40:48 INFO 140512987567936] Epoch[168] Batch[0] avg_epoch_loss=0.729371\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:40:48 INFO 140512987567936] #quality_metric: host=algo-1, epoch=168, batch=0 train loss <loss>=0.7293710112571716\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:40:49 INFO 140512987567936] Epoch[168] Batch[5] avg_epoch_loss=0.777712\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:40:49 INFO 140512987567936] #quality_metric: host=algo-1, epoch=168, batch=5 train loss <loss>=0.777711883187294\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:40:49 INFO 140512987567936] Epoch[168] Batch [5]#011Speed: 298.83 samples/sec#011loss=0.777712\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:40:50 INFO 140512987567936] processed a total of 618 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742060444.303521, \"EndTime\": 1742060450.5618675, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6257.939100265503, \"count\": 1, \"min\": 6257.939100265503, \"max\": 6257.939100265503}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:40:50 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=98.75286337333729 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:40:50 INFO 140512987567936] #progress_metric: host=algo-1, completed 84.5 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:40:50 INFO 140512987567936] #quality_metric: host=algo-1, epoch=168, train loss <loss>=0.8766994029283524\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:40:50 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:40:54 INFO 140512987567936] Epoch[169] Batch[0] avg_epoch_loss=0.810581\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:40:54 INFO 140512987567936] #quality_metric: host=algo-1, epoch=169, batch=0 train loss <loss>=0.8105814456939697\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:40:55 INFO 140512987567936] Epoch[169] Batch[5] avg_epoch_loss=1.155749\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:40:55 INFO 140512987567936] #quality_metric: host=algo-1, epoch=169, batch=5 train loss <loss>=1.1557489236195881\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:40:55 INFO 140512987567936] Epoch[169] Batch [5]#011Speed: 306.31 samples/sec#011loss=1.155749\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:40:57 INFO 140512987567936] Epoch[169] Batch[10] avg_epoch_loss=1.031443\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:40:57 INFO 140512987567936] #quality_metric: host=algo-1, epoch=169, batch=10 train loss <loss>=0.8822747945785523\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:40:57 INFO 140512987567936] Epoch[169] Batch [10]#011Speed: 224.06 samples/sec#011loss=0.882275\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:40:57 INFO 140512987567936] processed a total of 650 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742060450.561941, \"EndTime\": 1742060457.1044877, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6541.248559951782, \"count\": 1, \"min\": 6541.248559951782, \"max\": 6541.248559951782}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:40:57 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=99.36784021879241 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:40:57 INFO 140512987567936] #progress_metric: host=algo-1, completed 85.0 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:40:57 INFO 140512987567936] #quality_metric: host=algo-1, epoch=169, train loss <loss>=1.0314425013282082\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:40:57 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:41:01 INFO 140512987567936] Epoch[170] Batch[0] avg_epoch_loss=0.987068\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:41:01 INFO 140512987567936] #quality_metric: host=algo-1, epoch=170, batch=0 train loss <loss>=0.9870679378509521\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:41:02 INFO 140512987567936] Epoch[170] Batch[5] avg_epoch_loss=0.934727\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:41:02 INFO 140512987567936] #quality_metric: host=algo-1, epoch=170, batch=5 train loss <loss>=0.9347270031770071\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:41:02 INFO 140512987567936] Epoch[170] Batch [5]#011Speed: 294.37 samples/sec#011loss=0.934727\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:41:03 INFO 140512987567936] processed a total of 629 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742060457.1045566, \"EndTime\": 1742060463.3585186, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6253.5035610198975, \"count\": 1, \"min\": 6253.5035610198975, \"max\": 6253.5035610198975}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:41:03 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=100.58172917476742 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:41:03 INFO 140512987567936] #progress_metric: host=algo-1, completed 85.5 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:41:03 INFO 140512987567936] #quality_metric: host=algo-1, epoch=170, train loss <loss>=0.8204107746481896\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:41:03 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:41:07 INFO 140512987567936] Epoch[171] Batch[0] avg_epoch_loss=0.483933\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:41:07 INFO 140512987567936] #quality_metric: host=algo-1, epoch=171, batch=0 train loss <loss>=0.48393332958221436\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:41:08 INFO 140512987567936] Epoch[171] Batch[5] avg_epoch_loss=0.847818\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:41:08 INFO 140512987567936] #quality_metric: host=algo-1, epoch=171, batch=5 train loss <loss>=0.8478179474671682\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:41:08 INFO 140512987567936] Epoch[171] Batch [5]#011Speed: 300.99 samples/sec#011loss=0.847818\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:41:09 INFO 140512987567936] Epoch[171] Batch[10] avg_epoch_loss=0.874704\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:41:09 INFO 140512987567936] #quality_metric: host=algo-1, epoch=171, batch=10 train loss <loss>=0.906967556476593\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:41:09 INFO 140512987567936] Epoch[171] Batch [10]#011Speed: 213.83 samples/sec#011loss=0.906968\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:41:10 INFO 140512987567936] processed a total of 705 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742060463.3585896, \"EndTime\": 1742060470.2015092, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6842.406272888184, \"count\": 1, \"min\": 6842.406272888184, \"max\": 6842.406272888184}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:41:10 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=103.03236700400393 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:41:10 INFO 140512987567936] #progress_metric: host=algo-1, completed 86.0 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:41:10 INFO 140512987567936] #quality_metric: host=algo-1, epoch=171, train loss <loss>=1.0862771372000377\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:41:10 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:41:14 INFO 140512987567936] Epoch[172] Batch[0] avg_epoch_loss=1.050310\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:41:14 INFO 140512987567936] #quality_metric: host=algo-1, epoch=172, batch=0 train loss <loss>=1.0503097772598267\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:41:15 INFO 140512987567936] Epoch[172] Batch[5] avg_epoch_loss=0.830682\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:41:15 INFO 140512987567936] #quality_metric: host=algo-1, epoch=172, batch=5 train loss <loss>=0.8306816418965658\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:41:15 INFO 140512987567936] Epoch[172] Batch [5]#011Speed: 299.31 samples/sec#011loss=0.830682\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:41:16 INFO 140512987567936] Epoch[172] Batch[10] avg_epoch_loss=0.821248\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:41:16 INFO 140512987567936] #quality_metric: host=algo-1, epoch=172, batch=10 train loss <loss>=0.8099265933036804\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:41:16 INFO 140512987567936] Epoch[172] Batch [10]#011Speed: 189.45 samples/sec#011loss=0.809927\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:41:16 INFO 140512987567936] processed a total of 699 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742060470.20158, \"EndTime\": 1742060476.8353522, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6633.337020874023, \"count\": 1, \"min\": 6633.337020874023, \"max\": 6633.337020874023}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:41:16 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=105.37529571245514 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:41:16 INFO 140512987567936] #progress_metric: host=algo-1, completed 86.5 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:41:16 INFO 140512987567936] #quality_metric: host=algo-1, epoch=172, train loss <loss>=0.8212475288997997\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:41:16 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:41:21 INFO 140512987567936] Epoch[173] Batch[0] avg_epoch_loss=0.523471\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:41:21 INFO 140512987567936] #quality_metric: host=algo-1, epoch=173, batch=0 train loss <loss>=0.5234714150428772\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:41:22 INFO 140512987567936] Epoch[173] Batch[5] avg_epoch_loss=0.868403\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:41:22 INFO 140512987567936] #quality_metric: host=algo-1, epoch=173, batch=5 train loss <loss>=0.8684026499589285\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:41:22 INFO 140512987567936] Epoch[173] Batch [5]#011Speed: 302.32 samples/sec#011loss=0.868403\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:41:23 INFO 140512987567936] processed a total of 637 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742060476.8354182, \"EndTime\": 1742060483.0763648, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6240.650415420532, \"count\": 1, \"min\": 6240.650415420532, \"max\": 6240.650415420532}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:41:23 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=102.07104836193483 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:41:23 INFO 140512987567936] #progress_metric: host=algo-1, completed 87.0 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:41:23 INFO 140512987567936] #quality_metric: host=algo-1, epoch=173, train loss <loss>=0.9487931311130524\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:41:23 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:41:27 INFO 140512987567936] Epoch[174] Batch[0] avg_epoch_loss=0.972260\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:41:27 INFO 140512987567936] #quality_metric: host=algo-1, epoch=174, batch=0 train loss <loss>=0.9722604155540466\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:41:28 INFO 140512987567936] Epoch[174] Batch[5] avg_epoch_loss=0.940134\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:41:28 INFO 140512987567936] #quality_metric: host=algo-1, epoch=174, batch=5 train loss <loss>=0.9401338795820872\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:41:28 INFO 140512987567936] Epoch[174] Batch [5]#011Speed: 309.45 samples/sec#011loss=0.940134\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:41:29 INFO 140512987567936] processed a total of 637 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742060483.0764346, \"EndTime\": 1742060489.2676895, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6190.947532653809, \"count\": 1, \"min\": 6190.947532653809, \"max\": 6190.947532653809}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:41:29 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=102.89050159401484 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:41:29 INFO 140512987567936] #progress_metric: host=algo-1, completed 87.5 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:41:29 INFO 140512987567936] #quality_metric: host=algo-1, epoch=174, train loss <loss>=0.8472488045692443\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:41:29 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:41:33 INFO 140512987567936] Epoch[175] Batch[0] avg_epoch_loss=1.093835\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:41:33 INFO 140512987567936] #quality_metric: host=algo-1, epoch=175, batch=0 train loss <loss>=1.0938349962234497\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:41:34 INFO 140512987567936] Epoch[175] Batch[5] avg_epoch_loss=0.743247\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:41:34 INFO 140512987567936] #quality_metric: host=algo-1, epoch=175, batch=5 train loss <loss>=0.7432469290991625\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:41:34 INFO 140512987567936] Epoch[175] Batch [5]#011Speed: 292.71 samples/sec#011loss=0.743247\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:41:35 INFO 140512987567936] Epoch[175] Batch[10] avg_epoch_loss=0.688384\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:41:35 INFO 140512987567936] #quality_metric: host=algo-1, epoch=175, batch=10 train loss <loss>=0.6225489109754563\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:41:35 INFO 140512987567936] Epoch[175] Batch [10]#011Speed: 234.22 samples/sec#011loss=0.622549\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:41:35 INFO 140512987567936] processed a total of 642 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742060489.2677588, \"EndTime\": 1742060495.8798616, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6611.788749694824, \"count\": 1, \"min\": 6611.788749694824, \"max\": 6611.788749694824}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:41:35 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=97.09736881418958 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:41:35 INFO 140512987567936] #progress_metric: host=algo-1, completed 88.0 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:41:35 INFO 140512987567936] #quality_metric: host=algo-1, epoch=175, train loss <loss>=0.6883841935883869\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:41:35 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:41:40 INFO 140512987567936] Epoch[176] Batch[0] avg_epoch_loss=1.275057\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:41:40 INFO 140512987567936] #quality_metric: host=algo-1, epoch=176, batch=0 train loss <loss>=1.2750571966171265\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:41:41 INFO 140512987567936] Epoch[176] Batch[5] avg_epoch_loss=0.741576\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:41:41 INFO 140512987567936] #quality_metric: host=algo-1, epoch=176, batch=5 train loss <loss>=0.7415760656197866\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:41:41 INFO 140512987567936] Epoch[176] Batch [5]#011Speed: 293.27 samples/sec#011loss=0.741576\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:41:42 INFO 140512987567936] Epoch[176] Batch[10] avg_epoch_loss=0.644499\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:41:42 INFO 140512987567936] #quality_metric: host=algo-1, epoch=176, batch=10 train loss <loss>=0.5280068278312683\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:41:42 INFO 140512987567936] Epoch[176] Batch [10]#011Speed: 226.27 samples/sec#011loss=0.528007\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:41:42 INFO 140512987567936] processed a total of 648 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742060495.879957, \"EndTime\": 1742060502.513141, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6632.701396942139, \"count\": 1, \"min\": 6632.701396942139, \"max\": 6632.701396942139}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:41:42 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=97.6960533750318 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:41:42 INFO 140512987567936] #progress_metric: host=algo-1, completed 88.5 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:41:42 INFO 140512987567936] #quality_metric: host=algo-1, epoch=176, train loss <loss>=0.6444991393522783\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:41:42 INFO 140512987567936] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:41:42 INFO 140512987567936] Saved checkpoint to \"/opt/ml/model/state_625620c9-14a3-4594-95ec-3dc122ec285c-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742060502.5132194, \"EndTime\": 1742060502.5268323, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 13.185501098632812, \"count\": 1, \"min\": 13.185501098632812, \"max\": 13.185501098632812}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:41:46 INFO 140512987567936] Epoch[177] Batch[0] avg_epoch_loss=0.625307\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:41:46 INFO 140512987567936] #quality_metric: host=algo-1, epoch=177, batch=0 train loss <loss>=0.6253074407577515\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:41:47 INFO 140512987567936] Epoch[177] Batch[5] avg_epoch_loss=0.902431\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:41:47 INFO 140512987567936] #quality_metric: host=algo-1, epoch=177, batch=5 train loss <loss>=0.9024306734402975\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:41:47 INFO 140512987567936] Epoch[177] Batch [5]#011Speed: 301.14 samples/sec#011loss=0.902431\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:41:48 INFO 140512987567936] processed a total of 602 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742060502.5268917, \"EndTime\": 1742060508.6909037, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6163.956642150879, \"count\": 1, \"min\": 6163.956642150879, \"max\": 6163.956642150879}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:41:48 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=97.66269439885832 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:41:48 INFO 140512987567936] #progress_metric: host=algo-1, completed 89.0 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:41:48 INFO 140512987567936] #quality_metric: host=algo-1, epoch=177, train loss <loss>=1.1901798784732818\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:41:48 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:41:52 INFO 140512987567936] Epoch[178] Batch[0] avg_epoch_loss=0.987649\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:41:52 INFO 140512987567936] #quality_metric: host=algo-1, epoch=178, batch=0 train loss <loss>=0.9876492023468018\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:41:53 INFO 140512987567936] Epoch[178] Batch[5] avg_epoch_loss=1.037345\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:41:53 INFO 140512987567936] #quality_metric: host=algo-1, epoch=178, batch=5 train loss <loss>=1.0373449623584747\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:41:53 INFO 140512987567936] Epoch[178] Batch [5]#011Speed: 307.96 samples/sec#011loss=1.037345\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:41:55 INFO 140512987567936] Epoch[178] Batch[10] avg_epoch_loss=1.344017\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:41:55 INFO 140512987567936] #quality_metric: host=algo-1, epoch=178, batch=10 train loss <loss>=1.7120235323905946\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:41:55 INFO 140512987567936] Epoch[178] Batch [10]#011Speed: 263.02 samples/sec#011loss=1.712024\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:41:55 INFO 140512987567936] processed a total of 646 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742060508.690985, \"EndTime\": 1742060515.0575342, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6366.0571575164795, \"count\": 1, \"min\": 6366.0571575164795, \"max\": 6366.0571575164795}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:41:55 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=101.4740073448727 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:41:55 INFO 140512987567936] #progress_metric: host=algo-1, completed 89.5 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:41:55 INFO 140512987567936] #quality_metric: host=algo-1, epoch=178, train loss <loss>=1.344017039645802\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:41:55 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:41:59 INFO 140512987567936] Epoch[179] Batch[0] avg_epoch_loss=0.819233\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:41:59 INFO 140512987567936] #quality_metric: host=algo-1, epoch=179, batch=0 train loss <loss>=0.8192330598831177\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:42:00 INFO 140512987567936] Epoch[179] Batch[5] avg_epoch_loss=0.819484\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:42:00 INFO 140512987567936] #quality_metric: host=algo-1, epoch=179, batch=5 train loss <loss>=0.8194840053717295\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:42:00 INFO 140512987567936] Epoch[179] Batch [5]#011Speed: 295.01 samples/sec#011loss=0.819484\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:42:01 INFO 140512987567936] processed a total of 633 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742060515.0576043, \"EndTime\": 1742060521.3748527, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6316.805601119995, \"count\": 1, \"min\": 6316.805601119995, \"max\": 6316.805601119995}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:42:01 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=100.20016419274899 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:42:01 INFO 140512987567936] #progress_metric: host=algo-1, completed 90.0 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:42:01 INFO 140512987567936] #quality_metric: host=algo-1, epoch=179, train loss <loss>=0.9513246834278106\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:42:01 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:42:05 INFO 140512987567936] Epoch[180] Batch[0] avg_epoch_loss=1.133077\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:42:05 INFO 140512987567936] #quality_metric: host=algo-1, epoch=180, batch=0 train loss <loss>=1.1330771446228027\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:42:06 INFO 140512987567936] Epoch[180] Batch[5] avg_epoch_loss=0.853837\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:42:06 INFO 140512987567936] #quality_metric: host=algo-1, epoch=180, batch=5 train loss <loss>=0.8538369387388229\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:42:06 INFO 140512987567936] Epoch[180] Batch [5]#011Speed: 297.41 samples/sec#011loss=0.853837\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:42:08 INFO 140512987567936] Epoch[180] Batch[10] avg_epoch_loss=0.728711\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:42:08 INFO 140512987567936] #quality_metric: host=algo-1, epoch=180, batch=10 train loss <loss>=0.5785592079162598\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:42:08 INFO 140512987567936] Epoch[180] Batch [10]#011Speed: 220.93 samples/sec#011loss=0.578559\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:42:08 INFO 140512987567936] processed a total of 651 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742060521.3753002, \"EndTime\": 1742060528.0402453, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6664.595127105713, \"count\": 1, \"min\": 6664.595127105713, \"max\": 6664.595127105713}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:42:08 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=97.67892311538772 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:42:08 INFO 140512987567936] #progress_metric: host=algo-1, completed 90.5 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:42:08 INFO 140512987567936] #quality_metric: host=algo-1, epoch=180, train loss <loss>=0.7287106974558397\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:42:08 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:42:12 INFO 140512987567936] Epoch[181] Batch[0] avg_epoch_loss=0.867170\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:42:12 INFO 140512987567936] #quality_metric: host=algo-1, epoch=181, batch=0 train loss <loss>=0.8671697378158569\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:42:13 INFO 140512987567936] Epoch[181] Batch[5] avg_epoch_loss=0.824179\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:42:13 INFO 140512987567936] #quality_metric: host=algo-1, epoch=181, batch=5 train loss <loss>=0.8241788198550543\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:42:13 INFO 140512987567936] Epoch[181] Batch [5]#011Speed: 321.57 samples/sec#011loss=0.824179\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:42:14 INFO 140512987567936] processed a total of 633 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742060528.040311, \"EndTime\": 1742060534.1940448, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6153.1946659088135, \"count\": 1, \"min\": 6153.1946659088135, \"max\": 6153.1946659088135}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:42:14 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=102.87191207319691 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:42:14 INFO 140512987567936] #progress_metric: host=algo-1, completed 91.0 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:42:14 INFO 140512987567936] #quality_metric: host=algo-1, epoch=181, train loss <loss>=0.8345757007598877\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:42:14 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:42:18 INFO 140512987567936] Epoch[182] Batch[0] avg_epoch_loss=1.146227\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:42:18 INFO 140512987567936] #quality_metric: host=algo-1, epoch=182, batch=0 train loss <loss>=1.1462265253067017\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:42:19 INFO 140512987567936] Epoch[182] Batch[5] avg_epoch_loss=0.848157\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:42:19 INFO 140512987567936] #quality_metric: host=algo-1, epoch=182, batch=5 train loss <loss>=0.8481566781798998\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:42:19 INFO 140512987567936] Epoch[182] Batch [5]#011Speed: 299.53 samples/sec#011loss=0.848157\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:42:20 INFO 140512987567936] processed a total of 638 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742060534.1941037, \"EndTime\": 1742060540.4444406, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6250.044345855713, \"count\": 1, \"min\": 6250.044345855713, \"max\": 6250.044345855713}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:42:20 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=102.07765973382949 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:42:20 INFO 140512987567936] #progress_metric: host=algo-1, completed 91.5 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:42:20 INFO 140512987567936] #quality_metric: host=algo-1, epoch=182, train loss <loss>=0.777607960999012\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:42:20 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:42:24 INFO 140512987567936] Epoch[183] Batch[0] avg_epoch_loss=0.731352\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:42:24 INFO 140512987567936] #quality_metric: host=algo-1, epoch=183, batch=0 train loss <loss>=0.7313517928123474\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:42:25 INFO 140512987567936] Epoch[183] Batch[5] avg_epoch_loss=0.709023\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:42:25 INFO 140512987567936] #quality_metric: host=algo-1, epoch=183, batch=5 train loss <loss>=0.7090226610501608\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:42:25 INFO 140512987567936] Epoch[183] Batch [5]#011Speed: 301.46 samples/sec#011loss=0.709023\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:42:26 INFO 140512987567936] processed a total of 623 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742060540.4445102, \"EndTime\": 1742060546.689162, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6244.16971206665, \"count\": 1, \"min\": 6244.16971206665, \"max\": 6244.16971206665}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:42:26 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=99.77135863085078 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:42:26 INFO 140512987567936] #progress_metric: host=algo-1, completed 92.0 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:42:26 INFO 140512987567936] #quality_metric: host=algo-1, epoch=183, train loss <loss>=0.6935948252677917\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:42:26 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:42:30 INFO 140512987567936] Epoch[184] Batch[0] avg_epoch_loss=0.611526\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:42:30 INFO 140512987567936] #quality_metric: host=algo-1, epoch=184, batch=0 train loss <loss>=0.6115255951881409\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:42:31 INFO 140512987567936] Epoch[184] Batch[5] avg_epoch_loss=0.796076\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:42:31 INFO 140512987567936] #quality_metric: host=algo-1, epoch=184, batch=5 train loss <loss>=0.7960759202639262\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:42:31 INFO 140512987567936] Epoch[184] Batch [5]#011Speed: 296.33 samples/sec#011loss=0.796076\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:42:32 INFO 140512987567936] processed a total of 594 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742060546.6892345, \"EndTime\": 1742060552.8725414, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6182.906866073608, \"count\": 1, \"min\": 6182.906866073608, \"max\": 6182.906866073608}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:42:32 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=96.06961572794981 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:42:32 INFO 140512987567936] #progress_metric: host=algo-1, completed 92.5 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:42:32 INFO 140512987567936] #quality_metric: host=algo-1, epoch=184, train loss <loss>=0.9642230063676834\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:42:32 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:42:36 INFO 140512987567936] Epoch[185] Batch[0] avg_epoch_loss=1.129701\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:42:36 INFO 140512987567936] #quality_metric: host=algo-1, epoch=185, batch=0 train loss <loss>=1.1297012567520142\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:42:37 INFO 140512987567936] Epoch[185] Batch[5] avg_epoch_loss=0.854030\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:42:37 INFO 140512987567936] #quality_metric: host=algo-1, epoch=185, batch=5 train loss <loss>=0.85403045018514\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:42:37 INFO 140512987567936] Epoch[185] Batch [5]#011Speed: 291.16 samples/sec#011loss=0.854030\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:42:39 INFO 140512987567936] Epoch[185] Batch[10] avg_epoch_loss=0.650299\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:42:39 INFO 140512987567936] #quality_metric: host=algo-1, epoch=185, batch=10 train loss <loss>=0.40582039803266523\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:42:39 INFO 140512987567936] Epoch[185] Batch [10]#011Speed: 212.17 samples/sec#011loss=0.405820\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:42:39 INFO 140512987567936] processed a total of 673 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742060552.872613, \"EndTime\": 1742060559.4732828, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6600.231170654297, \"count\": 1, \"min\": 6600.231170654297, \"max\": 6600.231170654297}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:42:39 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=101.96467432000772 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:42:39 INFO 140512987567936] #progress_metric: host=algo-1, completed 93.0 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:42:39 INFO 140512987567936] #quality_metric: host=algo-1, epoch=185, train loss <loss>=0.6502986082976515\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:42:39 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:42:43 INFO 140512987567936] Epoch[186] Batch[0] avg_epoch_loss=1.344493\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:42:43 INFO 140512987567936] #quality_metric: host=algo-1, epoch=186, batch=0 train loss <loss>=1.3444925546646118\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:42:44 INFO 140512987567936] Epoch[186] Batch[5] avg_epoch_loss=0.783138\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:42:44 INFO 140512987567936] #quality_metric: host=algo-1, epoch=186, batch=5 train loss <loss>=0.7831380404531956\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:42:44 INFO 140512987567936] Epoch[186] Batch [5]#011Speed: 332.07 samples/sec#011loss=0.783138\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:42:45 INFO 140512987567936] Epoch[186] Batch[10] avg_epoch_loss=0.849073\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:42:45 INFO 140512987567936] #quality_metric: host=algo-1, epoch=186, batch=10 train loss <loss>=0.9281959950923919\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:42:45 INFO 140512987567936] Epoch[186] Batch [10]#011Speed: 214.92 samples/sec#011loss=0.928196\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:42:45 INFO 140512987567936] processed a total of 669 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742060559.4733458, \"EndTime\": 1742060565.9303017, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6456.665992736816, \"count\": 1, \"min\": 6456.665992736816, \"max\": 6456.665992736816}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:42:45 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=103.61207904294866 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:42:45 INFO 140512987567936] #progress_metric: host=algo-1, completed 93.5 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:42:45 INFO 140512987567936] #quality_metric: host=algo-1, epoch=186, train loss <loss>=0.849073474380103\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:42:45 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:42:50 INFO 140512987567936] Epoch[187] Batch[0] avg_epoch_loss=0.906539\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:42:50 INFO 140512987567936] #quality_metric: host=algo-1, epoch=187, batch=0 train loss <loss>=0.906538724899292\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:42:51 INFO 140512987567936] Epoch[187] Batch[5] avg_epoch_loss=0.887073\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:42:51 INFO 140512987567936] #quality_metric: host=algo-1, epoch=187, batch=5 train loss <loss>=0.8870725184679031\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:42:51 INFO 140512987567936] Epoch[187] Batch [5]#011Speed: 312.23 samples/sec#011loss=0.887073\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:42:52 INFO 140512987567936] Epoch[187] Batch[10] avg_epoch_loss=0.689923\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:42:52 INFO 140512987567936] #quality_metric: host=algo-1, epoch=187, batch=10 train loss <loss>=0.45334293842315676\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:42:52 INFO 140512987567936] Epoch[187] Batch [10]#011Speed: 249.48 samples/sec#011loss=0.453343\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:42:52 INFO 140512987567936] processed a total of 650 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742060565.9303777, \"EndTime\": 1742060572.3554082, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6424.577236175537, \"count\": 1, \"min\": 6424.577236175537, \"max\": 6424.577236175537}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:42:52 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=101.17250061992007 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:42:52 INFO 140512987567936] #progress_metric: host=algo-1, completed 94.0 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:42:52 INFO 140512987567936] #quality_metric: host=algo-1, epoch=187, train loss <loss>=0.6899227093566548\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:42:52 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:42:56 INFO 140512987567936] Epoch[188] Batch[0] avg_epoch_loss=0.827179\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:42:56 INFO 140512987567936] #quality_metric: host=algo-1, epoch=188, batch=0 train loss <loss>=0.827179491519928\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:42:57 INFO 140512987567936] Epoch[188] Batch[5] avg_epoch_loss=1.012079\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:42:57 INFO 140512987567936] #quality_metric: host=algo-1, epoch=188, batch=5 train loss <loss>=1.012079268693924\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:42:57 INFO 140512987567936] Epoch[188] Batch [5]#011Speed: 311.15 samples/sec#011loss=1.012079\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:42:58 INFO 140512987567936] Epoch[188] Batch[10] avg_epoch_loss=1.050557\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:42:58 INFO 140512987567936] #quality_metric: host=algo-1, epoch=188, batch=10 train loss <loss>=1.0967311322689057\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:42:58 INFO 140512987567936] Epoch[188] Batch [10]#011Speed: 197.66 samples/sec#011loss=1.096731\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:42:58 INFO 140512987567936] processed a total of 686 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742060572.3554695, \"EndTime\": 1742060578.9513483, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6595.561742782593, \"count\": 1, \"min\": 6595.561742782593, \"max\": 6595.561742782593}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:42:58 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=104.00787010466712 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:42:58 INFO 140512987567936] #progress_metric: host=algo-1, completed 94.5 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:42:58 INFO 140512987567936] #quality_metric: host=algo-1, epoch=188, train loss <loss>=1.0505573885007338\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:42:58 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:43:03 INFO 140512987567936] Epoch[189] Batch[0] avg_epoch_loss=1.001901\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:43:03 INFO 140512987567936] #quality_metric: host=algo-1, epoch=189, batch=0 train loss <loss>=1.0019007921218872\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:43:04 INFO 140512987567936] Epoch[189] Batch[5] avg_epoch_loss=0.860572\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:43:04 INFO 140512987567936] #quality_metric: host=algo-1, epoch=189, batch=5 train loss <loss>=0.860571950674057\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:43:04 INFO 140512987567936] Epoch[189] Batch [5]#011Speed: 296.90 samples/sec#011loss=0.860572\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:43:05 INFO 140512987567936] Epoch[189] Batch[10] avg_epoch_loss=1.143341\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:43:05 INFO 140512987567936] #quality_metric: host=algo-1, epoch=189, batch=10 train loss <loss>=1.482664120197296\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:43:05 INFO 140512987567936] Epoch[189] Batch [10]#011Speed: 216.51 samples/sec#011loss=1.482664\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:43:05 INFO 140512987567936] processed a total of 661 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742060578.9514117, \"EndTime\": 1742060585.6136026, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6661.900281906128, \"count\": 1, \"min\": 6661.900281906128, \"max\": 6661.900281906128}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:43:05 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=99.21946515069365 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:43:05 INFO 140512987567936] #progress_metric: host=algo-1, completed 95.0 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:43:05 INFO 140512987567936] #quality_metric: host=algo-1, epoch=189, train loss <loss>=1.1433411186391658\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:43:05 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:43:09 INFO 140512987567936] Epoch[190] Batch[0] avg_epoch_loss=0.808045\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:43:09 INFO 140512987567936] #quality_metric: host=algo-1, epoch=190, batch=0 train loss <loss>=0.8080453276634216\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:43:10 INFO 140512987567936] Epoch[190] Batch[5] avg_epoch_loss=0.910571\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:43:10 INFO 140512987567936] #quality_metric: host=algo-1, epoch=190, batch=5 train loss <loss>=0.9105712076028188\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:43:10 INFO 140512987567936] Epoch[190] Batch [5]#011Speed: 304.94 samples/sec#011loss=0.910571\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:43:12 INFO 140512987567936] Epoch[190] Batch[10] avg_epoch_loss=0.841243\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:43:12 INFO 140512987567936] #quality_metric: host=algo-1, epoch=190, batch=10 train loss <loss>=0.7580486953258514\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:43:12 INFO 140512987567936] Epoch[190] Batch [10]#011Speed: 200.72 samples/sec#011loss=0.758049\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:43:12 INFO 140512987567936] processed a total of 714 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742060585.6136737, \"EndTime\": 1742060592.507624, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6893.559455871582, \"count\": 1, \"min\": 6893.559455871582, \"max\": 6893.559455871582}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:43:12 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=103.57335208495141 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:43:12 INFO 140512987567936] #progress_metric: host=algo-1, completed 95.5 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:43:12 INFO 140512987567936] #quality_metric: host=algo-1, epoch=190, train loss <loss>=1.1151857400933902\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:43:12 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:43:16 INFO 140512987567936] Epoch[191] Batch[0] avg_epoch_loss=1.244373\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:43:16 INFO 140512987567936] #quality_metric: host=algo-1, epoch=191, batch=0 train loss <loss>=1.2443727254867554\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:43:17 INFO 140512987567936] Epoch[191] Batch[5] avg_epoch_loss=0.806883\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:43:17 INFO 140512987567936] #quality_metric: host=algo-1, epoch=191, batch=5 train loss <loss>=0.806882863243421\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:43:17 INFO 140512987567936] Epoch[191] Batch [5]#011Speed: 309.61 samples/sec#011loss=0.806883\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:43:18 INFO 140512987567936] processed a total of 605 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742060592.5076957, \"EndTime\": 1742060598.6984231, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6190.420866012573, \"count\": 1, \"min\": 6190.420866012573, \"max\": 6190.420866012573}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:43:18 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=97.72985100741256 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:43:18 INFO 140512987567936] #progress_metric: host=algo-1, completed 96.0 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:43:18 INFO 140512987567936] #quality_metric: host=algo-1, epoch=191, train loss <loss>=0.7962202578783035\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:43:18 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:43:22 INFO 140512987567936] Epoch[192] Batch[0] avg_epoch_loss=0.735436\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:43:22 INFO 140512987567936] #quality_metric: host=algo-1, epoch=192, batch=0 train loss <loss>=0.7354356646537781\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:43:23 INFO 140512987567936] Epoch[192] Batch[5] avg_epoch_loss=0.700296\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:43:23 INFO 140512987567936] #quality_metric: host=algo-1, epoch=192, batch=5 train loss <loss>=0.7002962132294973\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:43:23 INFO 140512987567936] Epoch[192] Batch [5]#011Speed: 314.13 samples/sec#011loss=0.700296\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:43:24 INFO 140512987567936] processed a total of 604 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742060598.6985042, \"EndTime\": 1742060604.7692049, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6070.316553115845, \"count\": 1, \"min\": 6070.316553115845, \"max\": 6070.316553115845}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:43:24 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=99.49878723364729 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:43:24 INFO 140512987567936] #progress_metric: host=algo-1, completed 96.5 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:43:24 INFO 140512987567936] #quality_metric: host=algo-1, epoch=192, train loss <loss>=0.7793801009654999\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:43:24 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:43:28 INFO 140512987567936] Epoch[193] Batch[0] avg_epoch_loss=1.169473\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:43:28 INFO 140512987567936] #quality_metric: host=algo-1, epoch=193, batch=0 train loss <loss>=1.1694729328155518\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:43:29 INFO 140512987567936] Epoch[193] Batch[5] avg_epoch_loss=0.748379\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:43:29 INFO 140512987567936] #quality_metric: host=algo-1, epoch=193, batch=5 train loss <loss>=0.7483793248732885\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:43:30 INFO 140512987567936] Epoch[193] Batch [5]#011Speed: 291.95 samples/sec#011loss=0.748379\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:43:31 INFO 140512987567936] processed a total of 629 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742060604.7692807, \"EndTime\": 1742060611.011965, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6242.252826690674, \"count\": 1, \"min\": 6242.252826690674, \"max\": 6242.252826690674}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:43:31 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=100.76323262168316 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:43:31 INFO 140512987567936] #progress_metric: host=algo-1, completed 97.0 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:43:31 INFO 140512987567936] #quality_metric: host=algo-1, epoch=193, train loss <loss>=0.6342078492045402\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:43:31 INFO 140512987567936] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:43:31 INFO 140512987567936] Saved checkpoint to \"/opt/ml/model/state_b64e5bb3-9fa1-4dfd-8665-ffe96328cd56-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742060611.012034, \"EndTime\": 1742060611.025815, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 13.44609260559082, \"count\": 1, \"min\": 13.44609260559082, \"max\": 13.44609260559082}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:43:35 INFO 140512987567936] Epoch[194] Batch[0] avg_epoch_loss=0.470010\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:43:35 INFO 140512987567936] #quality_metric: host=algo-1, epoch=194, batch=0 train loss <loss>=0.47000962495803833\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:43:36 INFO 140512987567936] Epoch[194] Batch[5] avg_epoch_loss=0.704750\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:43:36 INFO 140512987567936] #quality_metric: host=algo-1, epoch=194, batch=5 train loss <loss>=0.7047503187010685\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:43:36 INFO 140512987567936] Epoch[194] Batch [5]#011Speed: 287.54 samples/sec#011loss=0.704750\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:43:37 INFO 140512987567936] Epoch[194] Batch[10] avg_epoch_loss=0.532612\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:43:37 INFO 140512987567936] #quality_metric: host=algo-1, epoch=194, batch=10 train loss <loss>=0.3260455667972565\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:43:37 INFO 140512987567936] Epoch[194] Batch [10]#011Speed: 251.16 samples/sec#011loss=0.326046\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:43:37 INFO 140512987567936] processed a total of 653 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742060611.0258784, \"EndTime\": 1742060617.5552723, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6529.33931350708, \"count\": 1, \"min\": 6529.33931350708, \"max\": 6529.33931350708}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:43:37 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=100.00851559089044 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:43:37 INFO 140512987567936] #progress_metric: host=algo-1, completed 97.5 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:43:37 INFO 140512987567936] #quality_metric: host=algo-1, epoch=194, train loss <loss>=0.5326117951084267\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:43:37 INFO 140512987567936] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:43:37 INFO 140512987567936] Saved checkpoint to \"/opt/ml/model/state_265457c7-c155-43f0-9517-d7627e2ab714-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742060617.5553446, \"EndTime\": 1742060617.5691857, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 13.430118560791016, \"count\": 1, \"min\": 13.430118560791016, \"max\": 13.430118560791016}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:43:41 INFO 140512987567936] Epoch[195] Batch[0] avg_epoch_loss=0.766382\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:43:41 INFO 140512987567936] #quality_metric: host=algo-1, epoch=195, batch=0 train loss <loss>=0.7663821578025818\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:43:42 INFO 140512987567936] Epoch[195] Batch[5] avg_epoch_loss=0.826120\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:43:42 INFO 140512987567936] #quality_metric: host=algo-1, epoch=195, batch=5 train loss <loss>=0.8261201530694962\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:43:42 INFO 140512987567936] Epoch[195] Batch [5]#011Speed: 289.08 samples/sec#011loss=0.826120\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:43:44 INFO 140512987567936] Epoch[195] Batch[10] avg_epoch_loss=0.775942\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:43:44 INFO 140512987567936] #quality_metric: host=algo-1, epoch=195, batch=10 train loss <loss>=0.7157292366027832\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:43:44 INFO 140512987567936] Epoch[195] Batch [10]#011Speed: 239.86 samples/sec#011loss=0.715729\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:43:44 INFO 140512987567936] processed a total of 656 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742060617.5692465, \"EndTime\": 1742060624.1186082, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6549.308776855469, \"count\": 1, \"min\": 6549.308776855469, \"max\": 6549.308776855469}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:43:44 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=100.15608482239304 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:43:44 INFO 140512987567936] #progress_metric: host=algo-1, completed 98.0 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:43:44 INFO 140512987567936] #quality_metric: host=algo-1, epoch=195, train loss <loss>=0.7759424637664448\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:43:44 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:43:48 INFO 140512987567936] Epoch[196] Batch[0] avg_epoch_loss=0.651235\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:43:48 INFO 140512987567936] #quality_metric: host=algo-1, epoch=196, batch=0 train loss <loss>=0.6512352228164673\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:43:49 INFO 140512987567936] Epoch[196] Batch[5] avg_epoch_loss=0.636661\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:43:49 INFO 140512987567936] #quality_metric: host=algo-1, epoch=196, batch=5 train loss <loss>=0.6366607348124186\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:43:49 INFO 140512987567936] Epoch[196] Batch [5]#011Speed: 302.66 samples/sec#011loss=0.636661\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:43:50 INFO 140512987567936] processed a total of 590 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742060624.119047, \"EndTime\": 1742060630.3097181, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6190.319061279297, \"count\": 1, \"min\": 6190.319061279297, \"max\": 6190.319061279297}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:43:50 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=95.30785766240881 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:43:50 INFO 140512987567936] #progress_metric: host=algo-1, completed 98.5 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:43:50 INFO 140512987567936] #quality_metric: host=algo-1, epoch=196, train loss <loss>=0.7460338950157166\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:43:50 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:43:54 INFO 140512987567936] Epoch[197] Batch[0] avg_epoch_loss=0.697341\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:43:54 INFO 140512987567936] #quality_metric: host=algo-1, epoch=197, batch=0 train loss <loss>=0.697340726852417\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:43:55 INFO 140512987567936] Epoch[197] Batch[5] avg_epoch_loss=0.840647\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:43:55 INFO 140512987567936] #quality_metric: host=algo-1, epoch=197, batch=5 train loss <loss>=0.8406468530495962\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:43:55 INFO 140512987567936] Epoch[197] Batch [5]#011Speed: 284.18 samples/sec#011loss=0.840647\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:43:57 INFO 140512987567936] Epoch[197] Batch[10] avg_epoch_loss=0.850103\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:43:57 INFO 140512987567936] #quality_metric: host=algo-1, epoch=197, batch=10 train loss <loss>=0.8614504754543304\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:43:57 INFO 140512987567936] Epoch[197] Batch [10]#011Speed: 226.24 samples/sec#011loss=0.861450\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:43:57 INFO 140512987567936] processed a total of 641 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742060630.309827, \"EndTime\": 1742060637.0108738, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6700.514793395996, \"count\": 1, \"min\": 6700.514793395996, \"max\": 6700.514793395996}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:43:57 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=95.66284819256188 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:43:57 INFO 140512987567936] #progress_metric: host=algo-1, completed 99.0 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:43:57 INFO 140512987567936] #quality_metric: host=algo-1, epoch=197, train loss <loss>=0.850103045051748\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:43:57 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:44:01 INFO 140512987567936] Epoch[198] Batch[0] avg_epoch_loss=0.825208\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:44:01 INFO 140512987567936] #quality_metric: host=algo-1, epoch=198, batch=0 train loss <loss>=0.8252081871032715\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:44:02 INFO 140512987567936] Epoch[198] Batch[5] avg_epoch_loss=0.829999\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:44:02 INFO 140512987567936] #quality_metric: host=algo-1, epoch=198, batch=5 train loss <loss>=0.8299987514813741\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:44:02 INFO 140512987567936] Epoch[198] Batch [5]#011Speed: 281.82 samples/sec#011loss=0.829999\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:44:03 INFO 140512987567936] processed a total of 632 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742060637.0109432, \"EndTime\": 1742060643.3881922, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6376.8720626831055, \"count\": 1, \"min\": 6376.8720626831055, \"max\": 6376.8720626831055}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:44:03 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=99.10652073617283 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:44:03 INFO 140512987567936] #progress_metric: host=algo-1, completed 99.5 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:44:03 INFO 140512987567936] #quality_metric: host=algo-1, epoch=198, train loss <loss>=0.7879843071103096\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:44:03 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:44:07 INFO 140512987567936] Epoch[199] Batch[0] avg_epoch_loss=0.971283\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:44:07 INFO 140512987567936] #quality_metric: host=algo-1, epoch=199, batch=0 train loss <loss>=0.971283495426178\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:44:08 INFO 140512987567936] Epoch[199] Batch[5] avg_epoch_loss=0.991016\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:44:08 INFO 140512987567936] #quality_metric: host=algo-1, epoch=199, batch=5 train loss <loss>=0.9910162885983785\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:44:08 INFO 140512987567936] Epoch[199] Batch [5]#011Speed: 297.94 samples/sec#011loss=0.991016\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:44:09 INFO 140512987567936] processed a total of 617 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742060643.3882613, \"EndTime\": 1742060649.610065, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6221.323013305664, \"count\": 1, \"min\": 6221.323013305664, \"max\": 6221.323013305664}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:44:09 INFO 140512987567936] #throughput_metric: host=algo-1, train throughput=99.17231403497067 records/second\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:44:09 INFO 140512987567936] #progress_metric: host=algo-1, completed 100.0 % of epochs\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:44:09 INFO 140512987567936] #quality_metric: host=algo-1, epoch=199, train loss <loss>=1.0456446290016175\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:44:09 INFO 140512987567936] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:44:09 INFO 140512987567936] Final loss: 0.5326117951084267 (occurred at epoch 194)\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:44:09 INFO 140512987567936] #quality_metric: host=algo-1, train final_loss <loss>=0.5326117951084267\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:44:09 INFO 140512987567936] Worker algo-1 finished training.\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:44:09 WARNING 140512987567936] wait_for_all_workers will not sync workers since the kv store is not running distributed\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:44:09 INFO 140512987567936] All workers finished. Serializing model for prediction.\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742060649.610169, \"EndTime\": 1742060650.8922956, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"get_graph.time\": {\"sum\": 1281.1057567596436, \"count\": 1, \"min\": 1281.1057567596436, \"max\": 1281.1057567596436}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:44:11 INFO 140512987567936] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742060650.8923764, \"EndTime\": 1742060651.0815706, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"finalize.time\": {\"sum\": 1470.4272747039795, \"count\": 1, \"min\": 1470.4272747039795, \"max\": 1470.4272747039795}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:44:11 INFO 140512987567936] Serializing to /opt/ml/model/model_algo-1\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:44:11 INFO 140512987567936] Saved checkpoint to \"/opt/ml/model/model_algo-1-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742060651.0816443, \"EndTime\": 1742060651.0883522, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"model.serialize.time\": {\"sum\": 6.667852401733398, \"count\": 1, \"min\": 6.667852401733398, \"max\": 6.667852401733398}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:44:11 INFO 140512987567936] Successfully serialized the model for prediction.\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:44:11 INFO 140512987567936] #memory_usage::<batchbuffer> = 576.5283203125 mb\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:44:11 INFO 140512987567936] Evaluating model accuracy on testset using 100 samples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742060651.088401, \"EndTime\": 1742060651.0945926, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"model.bind.time\": {\"sum\": 0.029802322387695312, \"count\": 1, \"min\": 0.029802322387695312, \"max\": 0.029802322387695312}}}\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742060651.0946903, \"EndTime\": 1742060651.9155972, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"model.score.time\": {\"sum\": 821.0325241088867, \"count\": 1, \"min\": 821.0325241088867, \"max\": 821.0325241088867}}}\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:44:11 INFO 140512987567936] #test_score (algo-1, RMSE): 1.0638049211678469\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:44:11 INFO 140512987567936] #test_score (algo-1, mean_absolute_QuantileLoss): 9.677662596385925\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:44:11 INFO 140512987567936] #test_score (algo-1, mean_wQuantileLoss): 0.6779982162827016\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:44:11 INFO 140512987567936] #test_score (algo-1, wQuantileLoss[0.1]): 0.37214025257422145\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:44:11 INFO 140512987567936] #test_score (algo-1, wQuantileLoss[0.2]): 0.40012541360946285\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:44:11 INFO 140512987567936] #test_score (algo-1, wQuantileLoss[0.3]): 0.2948991908153524\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:44:11 INFO 140512987567936] #test_score (algo-1, wQuantileLoss[0.4]): 0.6415807249390318\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:44:11 INFO 140512987567936] #test_score (algo-1, wQuantileLoss[0.5]): 0.8112668549590797\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:44:11 INFO 140512987567936] #test_score (algo-1, wQuantileLoss[0.6]): 0.9288918901001414\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:44:11 INFO 140512987567936] #test_score (algo-1, wQuantileLoss[0.7]): 0.9926726685412159\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:44:11 INFO 140512987567936] #test_score (algo-1, wQuantileLoss[0.8]): 0.9235460519443324\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:44:11 INFO 140512987567936] #test_score (algo-1, wQuantileLoss[0.9]): 0.736860899061477\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:44:11 INFO 140512987567936] #quality_metric: host=algo-1, test RMSE <loss>=1.0638049211678469\u001b[0m\n",
      "\u001b[34m[03/15/2025 17:44:11 INFO 140512987567936] #quality_metric: host=algo-1, test mean_wQuantileLoss <loss>=0.6779982162827016\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1742060651.915666, \"EndTime\": 1742060652.1291316, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"setuptime\": {\"sum\": 4.356145858764648, \"count\": 1, \"min\": 4.356145858764648, \"max\": 4.356145858764648}, \"totaltime\": {\"sum\": 1293534.7909927368, \"count\": 1, \"min\": 1293534.7909927368, \"max\": 1293534.7909927368}}}\u001b[0m\n",
      "\n",
      "2025-03-15 17:44:30 Uploading - Uploading generated training model\n",
      "2025-03-15 17:44:30 Completed - Training job completed\n",
      "Training seconds: 1489\n",
      "Billable seconds: 1489\n",
      "CPU times: user 2.73 s, sys: 147 ms, total: 2.88 s\n",
      "Wall time: 25min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data_channels = {\"train\": \"{}/train/\".format(s3_data_path), \"test\": \"{}/test/\".format(s3_data_path)}\n",
    "\n",
    "estimator.fit(inputs=data_channels, wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "03726ef3-a0ac-48af-a66e-a261da679add",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-west-2-827154710549/train-test-hack/output\n"
     ]
    }
   ],
   "source": [
    "print(estimator.output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "c1928adf-7d8c-4ac5-a973-3ce1002a20f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model downloaded as model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "model_artifact = estimator.latest_training_job.describe()[\"ModelArtifacts\"][\"S3ModelArtifacts\"]\n",
    "s3 = boto3.client(\"s3\")\n",
    "\n",
    "# Parse the S3 path\n",
    "s3_bucket = model_artifact.split(\"/\")[2]\n",
    "s3_key = \"/\".join(model_artifact.split(\"/\")[3:])\n",
    "\n",
    "# Download the model locally\n",
    "s3.download_file(s3_bucket, s3_key, \"model.tar.gz\")\n",
    "print(\"Model downloaded as model.tar.gz\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "8d97d309-3b75-4d95-bbd5-a1801f54e17e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[03/15/25 18:19:01] </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Creating model with name: deepar-demo-notebook-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-03-15-18-19-01-326 <a href=\"file:///home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/session.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">session.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/session.py#4094\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">4094</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[03/15/25 18:19:01]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Creating model with name: deepar-demo-notebook-\u001b[1;36m2025\u001b[0m-03-15-18-19-01-326 \u001b]8;id=792495;file:///home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/session.py\u001b\\\u001b[2msession.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=49405;file:///home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/session.py#4094\u001b\\\u001b[2m4094\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Creating endpoint-config with name                                     <a href=\"file:///home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/session.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">session.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/session.py#5937\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">5937</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         deepar-demo-notebook-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-03-15-18-19-01-326                           <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Creating endpoint-config with name                                     \u001b]8;id=830555;file:///home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/session.py\u001b\\\u001b[2msession.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=713536;file:///home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/session.py#5937\u001b\\\u001b[2m5937\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         deepar-demo-notebook-\u001b[1;36m2025\u001b[0m-03-15-18-19-01-326                           \u001b[2m               \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[03/15/25 18:19:02] </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Creating endpoint with name                                            <a href=\"file:///home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/session.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">session.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/session.py#4759\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">4759</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         deepar-demo-notebook-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-03-15-18-19-01-326                           <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[03/15/25 18:19:02]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Creating endpoint with name                                            \u001b]8;id=624834;file:///home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/session.py\u001b\\\u001b[2msession.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=490785;file:///home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/session.py#4759\u001b\\\u001b[2m4759\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         deepar-demo-notebook-\u001b[1;36m2025\u001b[0m-03-15-18-19-01-326                           \u001b[2m               \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------!"
     ]
    }
   ],
   "source": [
    "predictor = estimator.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.m5.large\",  # Change instance type based on workload\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3220fdcd-ad8a-4ac1-b832-1068c20954f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name = predictor.endpoint_name\n",
    "print(\"Endpoint Name:\", endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "dbe5e9d8-7351-4faf-b074-f6841fdd69f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEST DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "5d76b935-a4c4-4e2b-99bd-14402df91bf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[03/15/25 18:13:17] </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Creating model with name: deepar-demo-notebook-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-03-15-18-13-17-656 <a href=\"file:///home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/session.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">session.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/session.py#4094\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">4094</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[03/15/25 18:13:17]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Creating model with name: deepar-demo-notebook-\u001b[1;36m2025\u001b[0m-03-15-18-13-17-656 \u001b]8;id=918938;file:///home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/session.py\u001b\\\u001b[2msession.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=752787;file:///home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/session.py#4094\u001b\\\u001b[2m4094\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_transformer = estimator.transformer(\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.large\",  # Use a compute instance\n",
    "    assemble_with=\"Line\",\n",
    "    output_path=\"s3://sagemaker-us-west-2-827154710549/train-test-hack/batch_output\"  # Change to your output bucket\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "ad126c4b-101b-4ac6-8e40-1a946dd50344",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[03/15/25 18:14:38] </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Creating transform job with name:                                      <a href=\"file:///home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/session.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">session.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/session.py#3951\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3951</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         deepar-demo-notebook-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-03-15-18-14-38-936                           <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[03/15/25 18:14:38]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Creating transform job with name:                                      \u001b]8;id=379580;file:///home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/session.py\u001b\\\u001b[2msession.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=229974;file:///home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/session.py#3951\u001b\\\u001b[2m3951\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         deepar-demo-notebook-\u001b[1;36m2025\u001b[0m-03-15-18-14-38-936                           \u001b[2m               \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "........................"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #ff0000; text-decoration-color: #ff0000\">╭─────────────────────────────── </span><span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">Traceback </span><span style=\"color: #ff7f7f; text-decoration-color: #ff7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #ff0000; text-decoration-color: #ff0000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>                                                                                    <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1 batch_transformer.transform(                                                                 <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>data=<span style=\"color: #808000; text-decoration-color: #808000\">\"s3://sagemaker-us-west-2-827154710549/train-test-hack/data/test/test.json\"</span>,  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">#</span>     <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>content_type=<span style=\"color: #808000; text-decoration-color: #808000\">\"json\"</span>,                                                                     <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">4 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>split_type=<span style=\"color: #808000; text-decoration-color: #808000\">\"Line\"</span>                                                                        <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/workflow/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">pipel</span> <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">ine_context.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">346</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">wrapper</span>                                                                    <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">343 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>                                                                               <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">344 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> _StepArguments(retrieve_caller_name(self_instance), run_func, *args,    <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">345 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                   <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>346 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> run_func(*args, **kwargs)                                                   <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">347 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>                                                                                       <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">348 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> wrapper                                                                         <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">349 </span>                                                                                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">transformer.py</span> <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> :<span style=\"color: #0000ff; text-decoration-color: #0000ff\">318</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">transform</span>                                                                                <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">315 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>)                                                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">316 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                   <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">317 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> wait:                                                                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>318 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.latest_transform_job.wait(logs=logs)                                      <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">319 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>                                                                                       <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">320 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span><span style=\"color: #808080; text-decoration-color: #808080\"> </span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">transform_with_monitoring</span>(                                                         <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">321 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>,                                                                              <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">transformer.py</span> <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> :<span style=\"color: #0000ff; text-decoration-color: #0000ff\">686</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">wait</span>                                                                                     <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">683 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>                                                                                       <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">684 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span><span style=\"color: #808080; text-decoration-color: #808080\"> </span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">wait</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, logs=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>):                                                             <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">685 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> logs:                                                                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>686 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.sagemaker_session.logs_for_transform_job(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.job_name, wait=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>)        <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">687 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                              <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">688 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.sagemaker_session.wait_for_transform_job(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.job_name)                   <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">689 </span>                                                                                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">session.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">620</span> <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">5</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">logs_for_transform_job</span>                                                                      <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">6202 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> state == LogState.COMPLETE:                                                <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">6203 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">break</span>                                                                     <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">6204 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>                                                                              <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>6205 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>time.sleep(poll)                                                              <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">6206 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>                                                                              <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">6207 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> state == LogState.JOB_COMPLETE:                                            <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">6208 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>state = LogState.COMPLETE                                                 <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">KeyboardInterrupt</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;255;0;0m╭─\u001b[0m\u001b[38;2;255;0;0m──────────────────────────────\u001b[0m\u001b[38;2;255;0;0m \u001b[0m\u001b[1;38;2;255;0;0mTraceback \u001b[0m\u001b[1;2;38;2;255;0;0m(most recent call last)\u001b[0m\u001b[38;2;255;0;0m \u001b[0m\u001b[38;2;255;0;0m───────────────────────────────\u001b[0m\u001b[38;2;255;0;0m─╮\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m1\u001b[0m                                                                                    \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m                                                                                                  \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m \u001b[31m❱ \u001b[0m1 batch_transformer.transform(                                                                 \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m2 \u001b[0m\u001b[2m│   \u001b[0mdata=\u001b[33m\"\u001b[0m\u001b[33ms3://sagemaker-us-west-2-827154710549/train-test-hack/data/test/test.json\u001b[0m\u001b[33m\"\u001b[0m,  \u001b[2m#\u001b[0m     \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m3 \u001b[0m\u001b[2m│   \u001b[0mcontent_type=\u001b[33m\"\u001b[0m\u001b[33mjson\u001b[0m\u001b[33m\"\u001b[0m,                                                                     \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m4 \u001b[0m\u001b[2m│   \u001b[0msplit_type=\u001b[33m\"\u001b[0m\u001b[33mLine\u001b[0m\u001b[33m\"\u001b[0m                                                                        \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m                                                                                                  \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m \u001b[2;33m/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/workflow/\u001b[0m\u001b[1;33mpipel\u001b[0m \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m \u001b[1;33mine_context.py\u001b[0m:\u001b[94m346\u001b[0m in \u001b[92mwrapper\u001b[0m                                                                    \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m                                                                                                  \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m343 \u001b[0m\u001b[2m│   │   │   \u001b[0m                                                                               \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m344 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m _StepArguments(retrieve_caller_name(self_instance), run_func, *args,    \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m345 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                   \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m \u001b[31m❱ \u001b[0m346 \u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m run_func(*args, **kwargs)                                                   \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m347 \u001b[0m\u001b[2m│   \u001b[0m                                                                                       \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m348 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mreturn\u001b[0m wrapper                                                                         \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m349 \u001b[0m                                                                                           \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m                                                                                                  \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m \u001b[2;33m/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/\u001b[0m\u001b[1;33mtransformer.py\u001b[0m \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m :\u001b[94m318\u001b[0m in \u001b[92mtransform\u001b[0m                                                                                \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m                                                                                                  \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m315 \u001b[0m\u001b[2m│   │   \u001b[0m)                                                                                  \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m316 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                   \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m317 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m wait:                                                                           \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m \u001b[31m❱ \u001b[0m318 \u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m.latest_transform_job.wait(logs=logs)                                      \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m319 \u001b[0m\u001b[2m│   \u001b[0m                                                                                       \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m320 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m\u001b[90m \u001b[0m\u001b[92mtransform_with_monitoring\u001b[0m(                                                         \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m321 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m,                                                                              \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m                                                                                                  \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m \u001b[2;33m/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/\u001b[0m\u001b[1;33mtransformer.py\u001b[0m \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m :\u001b[94m686\u001b[0m in \u001b[92mwait\u001b[0m                                                                                     \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m                                                                                                  \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m683 \u001b[0m\u001b[2m│   \u001b[0m                                                                                       \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m684 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m\u001b[90m \u001b[0m\u001b[92mwait\u001b[0m(\u001b[96mself\u001b[0m, logs=\u001b[94mTrue\u001b[0m):                                                             \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m685 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m logs:                                                                           \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m \u001b[31m❱ \u001b[0m686 \u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m.sagemaker_session.logs_for_transform_job(\u001b[96mself\u001b[0m.job_name, wait=\u001b[94mTrue\u001b[0m)        \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m687 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                              \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m688 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m.sagemaker_session.wait_for_transform_job(\u001b[96mself\u001b[0m.job_name)                   \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m689 \u001b[0m                                                                                           \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m                                                                                                  \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m \u001b[2;33m/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/\u001b[0m\u001b[1;33msession.py\u001b[0m:\u001b[94m620\u001b[0m \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m \u001b[94m5\u001b[0m in \u001b[92mlogs_for_transform_job\u001b[0m                                                                      \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m                                                                                                  \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m6202 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m state == LogState.COMPLETE:                                                \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m6203 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mbreak\u001b[0m                                                                     \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m6204 \u001b[0m\u001b[2m│   │   │   \u001b[0m                                                                              \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m \u001b[31m❱ \u001b[0m6205 \u001b[2m│   │   │   \u001b[0mtime.sleep(poll)                                                              \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m6206 \u001b[0m\u001b[2m│   │   │   \u001b[0m                                                                              \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m6207 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m state == LogState.JOB_COMPLETE:                                            \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m6208 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mstate = LogState.COMPLETE                                                 \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mKeyboardInterrupt\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_transformer.transform(\n",
    "    data=\"s3://sagemaker-us-west-2-827154710549/train-test-hack/data/test/test.json\",  # Your test dataset in S3\n",
    "    content_type=\"json\",\n",
    "    split_type=\"Line\"\n",
    ")\n",
    "\n",
    "batch_transformer.wait()  # Wait for completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7db53a1-e928-43b1-8fe1-ee1fe22fa1e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df952a69-9314-468c-b44c-f157de381709",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
